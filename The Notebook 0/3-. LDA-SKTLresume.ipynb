{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd3b9a6",
   "metadata": {},
   "source": [
    "## Modelo de tópicos: Latent Dirichlet Allocation (LDA) ##\n",
    "\n",
    "*Topic modeling*, en términos muy generales, consiste en una técnica para identificar tópicos o temas en textos a través de la detección de patrones en una colección de documentos llamado \"corpus\" y agrupando estas palabras en tópicos. A su vez, podemos definir *Topic modeling generativo* como un modelo que clasifica documentos aún no procesados una vez que el modelo ya ha sido entrenado por el corpus incompleto, característica crucial a la hora de trabajar con un corpus \"infinito\".\n",
    "\n",
    "Uno de las técnicas de \"Topic modeling generativo\" que ha resonado este último tiempo ha sido LDA. La esencia de este modelo consiste en decir que, para un grupo de texto (corpus), cada uno de ellos puede ser modelado por una distribución de tópicos y cada uno de estos tópicos puede ser modelado por una distribución de palabras. Tanto los tópicos como sus respectivas distribuciones (distribución de Dirichlet) son variables latentes del total de datos (Nicolai, F.,2019).\n",
    "\n",
    "En otras palabras, este modelo permite que un conjunto de observaciones puedan ser explicados por grupos inadvertidos que describen por qué algunas partes de los datos son similares, por ejemplo, si las observaciones son palabras en documentos, cada uno de estos documentos es una mezcla de categorías (tópicos) y la aparición de cada palabra en un documento se debe a una de las categorías a las que el documento pertenece. Así, la intuición detrás de LDA es que cada documento siempre exhibe múltiples temas en su cuerpo. Por ende, LDA es un modelo estadístico para colección de documentos que intenta capturar esta intuición (Chandía, B., 2016).\n",
    "\n",
    "Veamos la siguiente figura. Se muestra un documento donde palabras que coocurren son agrupadas y categorizadas en un tópico, la etiqueta de éste la ponemos nosotros, por ejemplo \"gene\", \"genomes, \"dna\" la podemos categorizar dentro de palabras que hablan de genética, luego \"organism\", \"evolve\", \"survive\" dentro de palabras que hablan de biología evolutiva y por último \"data\", \"computer\" y \"computational\" dentro de análisis de datos. Si continuamos con este procedimiento daremos con que todo el texto se desenvuelve dentro de estos tres tópicos . En LDA se define un tema para pasar a ser una distribución sobre un diccionario de palabras ya fijado, por ejemplo, el tema \"genética\" tiene un vocabulario de palabras que poseen una alta probabilidad de pertenecer al tema \"genética\"(Chandía, B., 2016).\n",
    "\n",
    "![Figura 1: Intución detrás de LDA (Blei, David., 2012)](LDAintuicion.png)\n",
    "\n",
    "#### La matemática tras LDA ####\n",
    "\n",
    "Entendamos un poco la matemática tras el método sin entrar tanto en tecnisismo ni operaciones matemáticas innecesarias que solo complicarán el entendimiento del método. Primero, definamos un vocabulario (creado por los autores del método):\n",
    "\n",
    "- Documento (D): Observación o muestra de carácter textual.\n",
    "- Corpus (C): Colección de todos los documentos a trabajar.\n",
    "- Vocabulario (V): Todas las palabras únicas encontradas en el corpus posterior al procesamiento.\n",
    "- Matriz término documento: Matriz cuyas filas son documentos y cuyas columnas son cada palabra del documento.\n",
    "\n",
    "Con *k-tópicos*,  $B_{(1:k)}$ son distribuciones de Dirichlet($\\eta$) de probabilidad sobre un vocabulario fijo. El modelo asume que cada documento D perteneciente al corpus C es generado por el siguiente proceso generativo:\n",
    "\n",
    "- Escoger mezcla de tópicos $\\theta^{d} $ de una distribución sobre un (K) simplex tal como una Dirichlet ($\\alpha$)\n",
    "- Cada una de las palabras del documento se genera escogiendo una asignación de tópico $z$ desde una distribución Multinomial( $\\theta^{d} $) y posteriormente una palabra $w$ desde una Multinomial($\\beta_{z}$)\n",
    "\n",
    "Graficamente este modelo lo podemos representar a través de la siguiente estructura donde cada término significa:\n",
    "- K: número de tópicos\n",
    "- D: número de documentos\n",
    "- N: cantidad de palabras en el documento d $\\in$ D\n",
    "- $\\alpha$: vector positivo de parámetros $\\alpha$ de dimensión K\n",
    "- $B_{(1:K)}$ representa los tópicos K de la estructura de tópicos ocultos\n",
    "- $\\theta_{(1:D)}$ representa la proporción de tópicos por documentos $d \\in D$\n",
    "- $W_{(d,n)}$ representa las variables observadas dada la asignación del documento $d \\in D$\n",
    "- $Z_{(1:D,1:N)}$ representa la asignación del tópico oculto $\\beta_{(k')}$ a la palabra $W_{(d,n)}$\n",
    "\n",
    "![Figura 1: Esquema funcionamiento LDA](esquemaLDA.png)\n",
    "\n",
    "Así, la distribución conjunta de una mezcla de tópicos  𝜃  junto a un conjunto de  𝑁  tópicos  𝑧  y un set de  𝑁  palabras  𝑤  se define por:\n",
    "\n",
    "$$ p( \\theta, z, w \\vert \\alpha, \\beta)= p(\\theta, \\alpha)\\prod_{n=1}^{N}p(z_{n} \\vert \\alpha)p(w_{n} \\vert z_{n}, \\beta)$$\n",
    "\n",
    "Ahora bien, a través de operaciones matemáticas que no incluiremos llegamos al computo de la distribución posterior de las variables ocultas dado un documento (así funciona la inferencia dentro de LDA):\n",
    "\n",
    "$$ p(\\theta, z \\vert w, \\alpha, \\beta)= \\dfrac{p(\\theta, z, w \\vert \\alpha , \\beta)}{p(w \\vert \\alpha, \\beta)} $$\n",
    "\n",
    "Como comentario, ésta ecuación en la forma que está expresada no se puede computar por lo que se proponen métodos alternativos de resolución como la inferencia varacional.\n",
    "\n",
    "#### LDA como modelo: ####\n",
    "\n",
    "La aplicación del algoritmo LDA la podemos describir a través de las siguientes etapas:\n",
    "- Colección de la DATA (futuro corpus)\n",
    "- Preprocesamiento de los datos\n",
    "- Implementación del modelo: entrenamiento y testeo.\n",
    "- Visualización\n",
    "\n",
    "Veamos un ejemplo para entender mejor la metodología tras LDA:\n",
    "\n",
    "1) **Colección de la data:**\n",
    "\n",
    "Trabajaremos con un Dataset en inglés de más de un millon de titulares publicados por ABC (Australian Broadcasting Corporation) en un periodo de 15 años. Para ello utilizaremos la librería **pandas** para importar la data y crear el dataframe sobre el cual vamos a trabajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c99fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Importamos la DATAset.\n",
    "data = pd.read_csv('abcnews-date-text.csv',error_bad_lines=None);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02e32f",
   "metadata": {},
   "source": [
    "Número de documentos pertenecientes a nuestro corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eeeeda71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226258"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d49ad",
   "metadata": {},
   "source": [
    "Visualizamos la información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5833367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ambitious olsson wins triple jump</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>antic delighted with record breaking barca</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aussie qualifier stosur wastes four memphis match</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aust addresses un security council over iraq</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>australia is locked into war timetable opp</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>australia to contribute 10 million in aid to iraq</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>barca take record as robson celebrates birthda...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bathhouse plans move ahead</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        headline_text  index\n",
       "0   aba decides against community broadcasting lic...      0\n",
       "1      act fire witnesses must be aware of defamation      1\n",
       "2      a g calls for infrastructure protection summit      2\n",
       "3            air nz staff in aust strike for pay rise      3\n",
       "4       air nz strike to affect australian travellers      4\n",
       "5                   ambitious olsson wins triple jump      5\n",
       "6          antic delighted with record breaking barca      6\n",
       "7   aussie qualifier stosur wastes four memphis match      7\n",
       "8        aust addresses un security council over iraq      8\n",
       "9          australia is locked into war timetable opp      9\n",
       "10  australia to contribute 10 million in aid to iraq     10\n",
       "11  barca take record as robson celebrates birthda...     11\n",
       "12                         bathhouse plans move ahead     12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f3cdb",
   "metadata": {},
   "source": [
    "Perfecto, ya tenemos lista la información con la cual vamos a trabajar. Ahora es de vital importancia eliminar y reducir al máximo los datos que no nos aportarán nada para el análisis, para ello vamos el siguiente paso:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2841f3",
   "metadata": {},
   "source": [
    "2) **Limpieza de información**\n",
    "\n",
    "Para efectos del preprocesamiento la idea es limpiar todo lo que no sirva en el corpus. Esto lo haremos a través de:\n",
    "- **Creación de tokens:** Reducimos el texto a oraciones y las oraciones a palabras, dejamos todo en minúscula, removemos puntuación, signos y números. \n",
    "- **Eliminación de las stopwords:** importamos un listado de palabras que no aportan en el análisis y las eliminamos de nuestro corpus\n",
    "- **Lemmatización:** palabras en tercera persona pasan a primera persona y dejamos los verbos en tiempo presente\n",
    "- **Stemmización:** las palabras se reducen a su raiz.\n",
    "\n",
    "Tanto gensim como nltk nos entregan las herramientas necesarias para realizar este proceso:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e6a03",
   "metadata": {},
   "source": [
    "**What is Gensim?**\n",
    "\n",
    "\n",
    "Gensim = “Generate Similar” is a popular open source natural language processing (NLP) library used for unsupervised topic modeling. It uses top academic models and modern statistical machine learning to perform various complex tasks such as:\n",
    "\n",
    "- Building document or word vectors\n",
    "- Corpora\n",
    "- Performing topic identification\n",
    "- Performing document comparison (retrieving semantically similar documents)\n",
    "- Analysing plain-text documents for semantic structure\n",
    "\n",
    "Más info: [Gensim](https://radimrehurek.com/gensim/intro.html)\n",
    "\n",
    "**What is Natural Language Toolkit (NLTK)?**\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "Info provided by (website official): [NLTK](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b71dd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos las librerías correspondientes para la limpieza\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "175205c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Importamos listado de palabras en inglés para eliminar en el corpus:\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc849696",
   "metadata": {},
   "source": [
    "Visualizamos un ejemplo de cómo quedarían las palabras post-stemmizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5bef1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978642ca",
   "metadata": {},
   "source": [
    "Creamos la función para lemmatizar y stemmizar nuestros textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1d136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lemmatizamos/stemmizamos\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "##Stopwords\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a460665",
   "metadata": {},
   "source": [
    "Visualizamos un documento aleatorio original y lo comparamos con uno procesado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a33ac4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f57fd1",
   "metadata": {},
   "source": [
    "Podemos notar el cambio para efectos de ratepayers -> ratepay, voting -> vote, etc. Corresponde ahora implementarlos para todo el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f69940d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos:  153.07344889640808\n"
     ]
    }
   ],
   "source": [
    "##Agregamos un contador\n",
    "import time \n",
    "t = time.time()\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "t = time.time()-t\n",
    "print(\"segundos: \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc6bc1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [decid, communiti, broadcast, licenc]\n",
       "1                                 [wit, awar, defam]\n",
       "2             [call, infrastructur, protect, summit]\n",
       "3                        [staff, aust, strike, rise]\n",
       "4               [strike, affect, australian, travel]\n",
       "5                 [ambiti, olsson, win, tripl, jump]\n",
       "6             [antic, delight, record, break, barca]\n",
       "7      [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8              [aust, address, secur, council, iraq]\n",
       "9                           [australia, lock, timet]\n",
       "10             [australia, contribut, million, iraq]\n",
       "11         [barca, record, robson, celebr, birthday]\n",
       "12                           [bathhous, plan, ahead]\n",
       "13             [hop, launceston, cycl, championship]\n",
       "14               [plan, boost, paroo, water, suppli]\n",
       "15               [blizzard, buri, unit, state, bill]\n",
       "16         [brigadi, dismiss, report, troop, harass]\n",
       "17    [british, combat, troop, arriv, daili, kuwait]\n",
       "18             [bryant, lead, laker, doubl, overtim]\n",
       "19                [bushfir, victim, urg, centrelink]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Visualizamos los textos procesados\n",
    "processed_docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b8aeb",
   "metadata": {},
   "source": [
    "Antes de seguir hablemos un poco [Scikit-learn](https://scikit-learn.org/stable/index.html). Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.([Source](https://www.tutorialspoint.com/scikit_learn/scikit_learn_introduction.htm))\n",
    "\n",
    "Una de tantas herramientas útiles de Sklearn es el [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). El CountVectorizer proporciona una manera simple de tokenizar una colección de documentos de texto y construir un vocabulario de palabras conocidas. Los pasos a seguir son:\n",
    "\n",
    "- Cree una instancia de la clase CountVectorizer.\n",
    "- Llame a la función fit() para aprender un vocabulario de uno o más documentos.\n",
    "- Llame a la función transform() en uno o más documentos según sea necesario para codificar cada uno como un vector.\n",
    "\n",
    "Se devuelve un vector codificado con una longitud de todo el vocabulario y un número entero para el número de veces que cada palabra apareció en el documento. Veamos un ejemplo:\n",
    "\n",
    "NOTA: También hubiesemos podido hacer limpieza de información a través del CountVectorizer() sin ningún problema a través de sus diversos parámetros (ver website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97cac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Información que queremos tokenizar\n",
    "corpus = [\n",
    "    'Carlos is a nice guy.',\n",
    "    'but Pedro is no a nice guy',\n",
    "    'Carlos and Pedro are not friends',\n",
    "    'Is this the last sentence?',]\n",
    "#Creación de la clase CountVectorizer()\n",
    "v = CountVectorizer()\n",
    "#Creamos el diccionario y codificamos como vector\n",
    "X = v.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7191da0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'but', 'carlos', 'friends', 'guy', 'is', 'last',\n",
       "       'nice', 'no', 'not', 'pedro', 'sentence', 'the', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos el vocabulario\n",
    "v.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64b9d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 1 0 1 1 0 1 0 0 0]\n",
      " [1 1 0 1 1 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#veamos el vector\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafa568",
   "metadata": {},
   "source": [
    "Ahora que ya sabemos cómo funciona CountVectorizer(), lo implementaremos en nuestro corpus para convertir nuestros documents  a una matriz de \"token counts\". Para ello utilizaremos [\"train_test_split\"](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de SKlearn que nos permitirá separar la información en entrenamiento y testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a89bde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "info_train, info_test = train_test_split(processed_docs,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e59ffeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905827     ['central', 'west', 'council', 'pilot', 'organ']\n",
       "487792                     ['seiz', 'dog', 'take', 'rspca']\n",
       "888351        ['woman', 'court', 'cannabi', 'plant', 'gun']\n",
       "972375    ['drug', 'arrest', 'adelaid', 'night', 'club',...\n",
       "488550      ['treasur', 'pitch', 'pulp', 'propos', 'europ']\n",
       "                                ...                        \n",
       "110268    ['broadford', 'footbal', 'club', 'warn', 'stop...\n",
       "259178       ['south', 'promot', 'taylor', 'head', 'coach']\n",
       "131932      ['jackson', 'give', 'sampl', 'polic', 'report']\n",
       "671155              ['capello', 'quit', 'england', 'manag']\n",
       "121958              ['worker', 'admit', 'skim', 'thousand']\n",
       "Name: headline_text, Length: 981006, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostramos la data para entrenamiento:\n",
    "info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8f19a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Clase del CountVectorizer()\n",
    "vectorizer = CountVectorizer(min_df= 15 ,lowercase=False)\n",
    "\n",
    "#Dejamos todo como str (la info contiene números)\n",
    "info_train=info_train.apply(str)\n",
    "\n",
    "#Aplicamos el CV\n",
    "info_train_vec = vectorizer.fit_transform(info_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "70fff61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'central': 2118,\n",
       " 'west': 13422,\n",
       " 'council': 2825,\n",
       " 'pilot': 9275,\n",
       " 'organ': 8764,\n",
       " 'seiz': 10914,\n",
       " 'dog': 3573,\n",
       " 'take': 12110,\n",
       " 'rspca': 10539,\n",
       " 'woman': 13598,\n",
       " 'court': 2842,\n",
       " 'cannabi': 1919,\n",
       " 'plant': 9326,\n",
       " 'gun': 5271,\n",
       " 'drug': 3698,\n",
       " 'arrest': 563,\n",
       " 'adelaid': 109,\n",
       " 'night': 8454,\n",
       " 'club': 2429,\n",
       " 'strip': 11826,\n",
       " 'treasur': 12612,\n",
       " 'pitch': 9308,\n",
       " 'pulp': 9708,\n",
       " 'propos': 9657,\n",
       " 'europ': 4090,\n",
       " 'plan': 9321,\n",
       " 'law': 6879,\n",
       " 'blame': 1269,\n",
       " 'stifl': 11729,\n",
       " 'develop': 3368,\n",
       " 'daryl': 3111,\n",
       " 'hannah': 5390,\n",
       " 'make': 7353,\n",
       " 'splash': 11551,\n",
       " 'week': 13387,\n",
       " 'mother': 8117,\n",
       " 'heartach': 5518,\n",
       " 'homeless': 5746,\n",
       " 'women': 13600,\n",
       " 'fear': 4302,\n",
       " 'lose': 7187,\n",
       " 'kid': 6601,\n",
       " 'firefight': 4430,\n",
       " 'cancer': 1906,\n",
       " 'rate': 9918,\n",
       " 'averag': 737,\n",
       " 'kennett': 6566,\n",
       " 'swim': 12045,\n",
       " 'champ': 2154,\n",
       " 'swimmer': 12046,\n",
       " 'break': 1527,\n",
       " 'world': 13644,\n",
       " 'record': 10009,\n",
       " 'hobart': 5694,\n",
       " 'assault': 621,\n",
       " 'derryn': 3322,\n",
       " 'hinch': 5659,\n",
       " 'sentenc': 10942,\n",
       " 'breach': 1525,\n",
       " 'order': 8759,\n",
       " 'back': 771,\n",
       " 'waltz': 13278,\n",
       " 'matilda': 7546,\n",
       " 'dastyari': 3114,\n",
       " 'group': 5226,\n",
       " 'harass': 5402,\n",
       " 'face': 4209,\n",
       " 'legal': 6936,\n",
       " 'action': 89,\n",
       " 'queensland': 9783,\n",
       " 'mick': 7816,\n",
       " 'deni': 3280,\n",
       " 'pork': 9458,\n",
       " 'barrel': 936,\n",
       " 'sport': 11565,\n",
       " 'rort': 10483,\n",
       " 'urgent': 12986,\n",
       " 'upgrad': 12966,\n",
       " 'canberra': 1902,\n",
       " 'convent': 2724,\n",
       " 'facil': 4213,\n",
       " 'defend': 3209,\n",
       " 'timet': 12417,\n",
       " 'beat': 1035,\n",
       " 'econom': 3822,\n",
       " 'turnaround': 12747,\n",
       " 'govt': 5089,\n",
       " 'hick': 5624,\n",
       " 'plea': 9350,\n",
       " 'bargain': 915,\n",
       " 'strike': 11823,\n",
       " 'forestri': 4595,\n",
       " 'bookkeep': 1398,\n",
       " 'accus': 69,\n",
       " 'steal': 11679,\n",
       " 'destroy': 3348,\n",
       " 'nambour': 8286,\n",
       " 'home': 5740,\n",
       " 'slipper': 11291,\n",
       " 'name': 8288,\n",
       " 'red': 10017,\n",
       " 'best': 1149,\n",
       " 'croft': 2936,\n",
       " 'claim': 2346,\n",
       " 'lade': 6772,\n",
       " 'report': 10186,\n",
       " 'dead': 3141,\n",
       " 'cystic': 3043,\n",
       " 'fibrosi': 4367,\n",
       " 'suffer': 11901,\n",
       " 'carri': 2005,\n",
       " 'differ': 3411,\n",
       " 'lyon': 7260,\n",
       " 'famili': 4251,\n",
       " 'boswel': 1434,\n",
       " 'silver': 11183,\n",
       " 'chemo': 2225,\n",
       " 'patient': 9056,\n",
       " 'say': 10734,\n",
       " 'tell': 12230,\n",
       " 'speak': 11501,\n",
       " 'media': 7685,\n",
       " 'bali': 837,\n",
       " 'deadlock': 3144,\n",
       " 'climat': 2399,\n",
       " 'draft': 3650,\n",
       " 'meatwork': 7676,\n",
       " 'warn': 13302,\n",
       " 'pond': 9435,\n",
       " 'pong': 9437,\n",
       " 'jayasuriya': 6309,\n",
       " 'year': 13730,\n",
       " 'cricket': 2912,\n",
       " 'anti': 435,\n",
       " 'corrupt': 2804,\n",
       " 'code': 2469,\n",
       " 'polic': 9407,\n",
       " 'prais': 9520,\n",
       " 'hero': 5602,\n",
       " 'help': 5566,\n",
       " 'heart': 5517,\n",
       " 'attack': 669,\n",
       " 'earli': 3788,\n",
       " 'topless': 12496,\n",
       " 'trade': 12556,\n",
       " 'hockey': 5700,\n",
       " 'argu': 533,\n",
       " 'case': 2025,\n",
       " 'incom': 6019,\n",
       " 'cut': 3023,\n",
       " 'buddi': 1677,\n",
       " 'blow': 1312,\n",
       " 'hawk': 5474,\n",
       " 'french': 4686,\n",
       " 'journalist': 6419,\n",
       " 'hand': 5369,\n",
       " 'jail': 6276,\n",
       " 'term': 12262,\n",
       " 'papua': 8965,\n",
       " 'communiti': 2570,\n",
       " 'farewel': 4266,\n",
       " 'teach': 12199,\n",
       " 'pioneer': 9291,\n",
       " 'mcmeniman': 7652,\n",
       " 'sign': 11167,\n",
       " 'forc': 4582,\n",
       " 'bodi': 1344,\n",
       " 'ukrainian': 12791,\n",
       " 'shipwreck': 11084,\n",
       " 'power': 9509,\n",
       " 'bill': 1194,\n",
       " 'rise': 10354,\n",
       " 'indonesian': 6048,\n",
       " 'rescu': 10200,\n",
       " 'effort': 3849,\n",
       " 'slow': 11303,\n",
       " 'urg': 12984,\n",
       " 'appli': 479,\n",
       " 'pipelin': 9294,\n",
       " 'fund': 4738,\n",
       " 'xenophon': 13693,\n",
       " 'support': 11971,\n",
       " 'youth': 13761,\n",
       " 'allow': 300,\n",
       " 'roll': 10447,\n",
       " 'googl': 5062,\n",
       " 'languag': 6822,\n",
       " 'superior': 11958,\n",
       " 'complex': 2588,\n",
       " 'rudd': 10549,\n",
       " 'meet': 7704,\n",
       " 'opposit': 8737,\n",
       " 'leader': 6900,\n",
       " 'disabl': 3449,\n",
       " 'advoc': 136,\n",
       " 'call': 1854,\n",
       " 'reform': 10057,\n",
       " 'photo': 9235,\n",
       " 'exhibit': 4145,\n",
       " 'celebr': 2100,\n",
       " 'karen': 6500,\n",
       " 'refuge': 10062,\n",
       " 'contribut': 2719,\n",
       " 'tasmanian': 12174,\n",
       " 'fossil': 4626,\n",
       " 'answer': 428,\n",
       " 'evolutionari': 4118,\n",
       " 'teenag': 12217,\n",
       " 'killer': 6614,\n",
       " 'bulli': 1710,\n",
       " 'school': 10781,\n",
       " 'indi': 6034,\n",
       " 'troubl': 12675,\n",
       " 'posit': 9477,\n",
       " 'start': 11657,\n",
       " 'mobil': 7963,\n",
       " 'speed': 11517,\n",
       " 'camera': 1874,\n",
       " 'patrol': 9061,\n",
       " 'zone': 13798,\n",
       " 'grammar': 5111,\n",
       " 'teacher': 12200,\n",
       " 'abus': 43,\n",
       " 'alleg': 288,\n",
       " 'molong': 8001,\n",
       " 'flood': 4512,\n",
       " 'demolit': 3271,\n",
       " 'expect': 4157,\n",
       " 'strong': 11833,\n",
       " 'earthquak': 3795,\n",
       " 'eastern': 3803,\n",
       " 'indonesia': 6047,\n",
       " 'critic': 2927,\n",
       " 'netbal': 8389,\n",
       " 'pathway': 9054,\n",
       " 'block': 1298,\n",
       " 'money': 8018,\n",
       " 'coronavirus': 2794,\n",
       " 'maker': 7355,\n",
       " 'botul': 1446,\n",
       " 'link': 7080,\n",
       " 'courtney': 2844,\n",
       " 'barnett': 926,\n",
       " 'tame': 12131,\n",
       " 'australian': 712,\n",
       " 'grammi': 5112,\n",
       " 'nomin': 8505,\n",
       " 'shoot': 11099,\n",
       " 'appeal': 471,\n",
       " 'tale': 12115,\n",
       " 'wed': 13382,\n",
       " 'ring': 10339,\n",
       " 'symbol': 12062,\n",
       " 'jewelleri': 6354,\n",
       " 'nadal': 8267,\n",
       " 'storm': 11773,\n",
       " 'barcelona': 909,\n",
       " 'scienc': 10798,\n",
       " 'music': 8230,\n",
       " 'romanc': 10455,\n",
       " 'zimbabw': 13791,\n",
       " 'withhold': 13577,\n",
       " 'food': 4560,\n",
       " 'tabl': 12084,\n",
       " 'obama': 8609,\n",
       " 'arsenal': 569,\n",
       " 'ahead': 187,\n",
       " 'leed': 6931,\n",
       " 'militari': 7860,\n",
       " 'syria': 12078,\n",
       " 'rule': 10558,\n",
       " 'noroc': 8526,\n",
       " 'referendum': 10050,\n",
       " 'wait': 13243,\n",
       " 'time': 12412,\n",
       " 'psychiatrist': 9689,\n",
       " 'criticis': 2928,\n",
       " 'releas': 10118,\n",
       " 'bronco': 1628,\n",
       " 'berrigan': 1142,\n",
       " 'meyer': 7808,\n",
       " 'eel': 3845,\n",
       " 'clash': 2361,\n",
       " 'schapell': 10765,\n",
       " 'corbi': 2774,\n",
       " 'shorten': 11112,\n",
       " 'australia': 711,\n",
       " 'parol': 9008,\n",
       " 'investig': 6192,\n",
       " 'shepparton': 11066,\n",
       " 'busi': 1784,\n",
       " 'death': 3153,\n",
       " 'identifi': 5940,\n",
       " 'involv': 6199,\n",
       " 'fatal': 4288,\n",
       " 'melbourn': 7715,\n",
       " 'clean': 2374,\n",
       " 'energi': 3967,\n",
       " 'target': 12162,\n",
       " 'emiss': 3930,\n",
       " 'save': 10725,\n",
       " 'burka': 1750,\n",
       " 'segreg': 10909,\n",
       " 'hewitt': 5613,\n",
       " 'glad': 4981,\n",
       " 'rebound': 9977,\n",
       " 'malaysia': 7362,\n",
       " 'deport': 3303,\n",
       " 'karoonda': 6506,\n",
       " 'east': 3800,\n",
       " 'murray': 8219,\n",
       " 'reveal': 10266,\n",
       " 'johnson': 6390,\n",
       " 'miss': 7937,\n",
       " 'second': 10888,\n",
       " 'demand': 3259,\n",
       " 'level': 6997,\n",
       " 'uni': 12887,\n",
       " 'rural': 10571,\n",
       " 'health': 5509,\n",
       " 'confer': 2634,\n",
       " 'afghan': 149,\n",
       " 'capit': 1941,\n",
       " 'kill': 6613,\n",
       " 'volunt': 13212,\n",
       " 'career': 1970,\n",
       " 'program': 9632,\n",
       " 'launch': 6863,\n",
       " 'combat': 2534,\n",
       " 'unfair': 12873,\n",
       " 'retail': 10240,\n",
       " 'practic': 9516,\n",
       " 'reunit': 10261,\n",
       " 'think': 12328,\n",
       " 'rat': 9917,\n",
       " 'import': 5992,\n",
       " 'poll': 9417,\n",
       " 'nelson': 8369,\n",
       " 'scalper': 10742,\n",
       " 'olymp': 8689,\n",
       " 'ticket': 12384,\n",
       " 'light': 7044,\n",
       " 'debut': 3165,\n",
       " 'outback': 8798,\n",
       " 'documentari': 3566,\n",
       " 'qaeda': 9752,\n",
       " 'yemen': 13736,\n",
       " 'interview': 6176,\n",
       " 'jam': 6280,\n",
       " 'hird': 5676,\n",
       " 'liber': 7015,\n",
       " 'threat': 12350,\n",
       " 'bail': 815,\n",
       " 'renew': 10160,\n",
       " 'kalgoorli': 6478,\n",
       " 'kidnapp': 6605,\n",
       " 'tear': 12204,\n",
       " 'apart': 451,\n",
       " 'guilt': 5257,\n",
       " 'diagnosi': 3382,\n",
       " 'regul': 10086,\n",
       " 'sale': 10641,\n",
       " 'hemp': 5570,\n",
       " 'seed': 10904,\n",
       " 'consumpt': 2696,\n",
       " 'tourism': 12530,\n",
       " 'board': 1329,\n",
       " 'alburi': 246,\n",
       " 'today': 12450,\n",
       " 'die': 3405,\n",
       " 'fall': 4242,\n",
       " 'citi': 2332,\n",
       " 'balconi': 830,\n",
       " 'photograph': 9236,\n",
       " 'ban': 859,\n",
       " 'parliament': 9001,\n",
       " 'elect': 3872,\n",
       " 'result': 10234,\n",
       " 'delay': 3234,\n",
       " 'contempt': 2704,\n",
       " 'smith': 11332,\n",
       " 'student': 11845,\n",
       " 'uncov': 12833,\n",
       " 'soft': 11397,\n",
       " 'coral': 2770,\n",
       " 'speci': 11505,\n",
       " 'tamar': 12127,\n",
       " 'river': 10365,\n",
       " 'geoff': 4899,\n",
       " 'toovey': 12491,\n",
       " 'london': 7159,\n",
       " 'bridg': 1569,\n",
       " 'stab': 11600,\n",
       " 'suspect': 12007,\n",
       " 'wwii': 13686,\n",
       " 'pen': 9124,\n",
       " 'stori': 11771,\n",
       " 'futur': 4758,\n",
       " 'generat': 4884,\n",
       " 'masri': 7519,\n",
       " 'charg': 2182,\n",
       " 'domest': 3587,\n",
       " 'art': 573,\n",
       " 'quarter': 9776,\n",
       " 'downer': 3635,\n",
       " 'push': 9735,\n",
       " 'pacif': 8894,\n",
       " 'island': 6236,\n",
       " 'work': 13634,\n",
       " 'scheme': 10768,\n",
       " 'convict': 2729,\n",
       " 'babi': 766,\n",
       " 'keli': 6548,\n",
       " 'lane': 6816,\n",
       " 'refus': 10065,\n",
       " 'water': 13334,\n",
       " 'price': 9578,\n",
       " 'hard': 5407,\n",
       " 'swallow': 12022,\n",
       " 'cowra': 2863,\n",
       " 'minimum': 7898,\n",
       " 'size': 11224,\n",
       " 'entertain': 3989,\n",
       " 'abc': 12,\n",
       " 'david': 3125,\n",
       " 'split': 11553,\n",
       " 'schooli': 10785,\n",
       " 'dairi': 3051,\n",
       " 'farmer': 4270,\n",
       " 'halt': 5350,\n",
       " 'milk': 7862,\n",
       " 'altern': 314,\n",
       " 'wine': 13542,\n",
       " 'award': 752,\n",
       " 'state': 11664,\n",
       " 'origin': 8768,\n",
       " 'live': 7105,\n",
       " 'blog': 1302,\n",
       " 'cook': 2735,\n",
       " 'stand': 11633,\n",
       " 'aussi': 701,\n",
       " 'flash': 4475,\n",
       " 'north': 8530,\n",
       " 'brawl': 1519,\n",
       " 'street': 11810,\n",
       " 'perman': 9164,\n",
       " 'resid': 10210,\n",
       " 'promis': 9643,\n",
       " 'check': 2211,\n",
       " 'peter': 9192,\n",
       " 'ryan': 10590,\n",
       " 'extend': 4182,\n",
       " 'bell': 1087,\n",
       " 'hit': 5683,\n",
       " 'maiden': 7339,\n",
       " 'centuri': 2123,\n",
       " 'england': 3971,\n",
       " 'declar': 3179,\n",
       " 'grain': 5109,\n",
       " 'season': 10879,\n",
       " 'edg': 3832,\n",
       " 'tribun': 12642,\n",
       " 'controversi': 2721,\n",
       " 'cull': 2984,\n",
       " 'surviv': 12002,\n",
       " 'birthday': 1235,\n",
       " 'parti': 9017,\n",
       " 'give': 4978,\n",
       " 'touch': 12521,\n",
       " 'judg': 6434,\n",
       " 'dump': 3729,\n",
       " 'pass': 9029,\n",
       " 'gaff': 4771,\n",
       " 'surat': 11976,\n",
       " 'prepar': 9552,\n",
       " 'possibl': 9479,\n",
       " 'grog': 5210,\n",
       " 'restrict': 10232,\n",
       " 'hamper': 5363,\n",
       " 'recruit': 10015,\n",
       " 'drought': 3692,\n",
       " 'levi': 6999,\n",
       " 'slug': 11309,\n",
       " 'attract': 676,\n",
       " 'coal': 2438,\n",
       " 'worker': 13637,\n",
       " 'hunter': 5879,\n",
       " 'chamber': 2152,\n",
       " 'crime': 2913,\n",
       " 'concern': 2611,\n",
       " 'outlook': 8814,\n",
       " 'rabbit': 9820,\n",
       " 'plagu': 9319,\n",
       " 'leagu': 6905,\n",
       " 'januari': 6293,\n",
       " 'hurt': 5890,\n",
       " 'carol': 1994,\n",
       " 'candlelight': 1911,\n",
       " 'firework': 4435,\n",
       " 'labor': 6761,\n",
       " 'accommod': 61,\n",
       " 'cape': 1937,\n",
       " 'alumina': 317,\n",
       " 'drop': 3691,\n",
       " 'hill': 5649,\n",
       " 'project': 9636,\n",
       " 'invad': 6185,\n",
       " 'chief': 2248,\n",
       " 'apologis': 465,\n",
       " 'consult': 2694,\n",
       " 'slay': 11274,\n",
       " 'teen': 12216,\n",
       " 'rememb': 10143,\n",
       " 'peac': 9093,\n",
       " 'justic': 6464,\n",
       " 'luczak': 7224,\n",
       " 'open': 8726,\n",
       " 'mar': 7440,\n",
       " 'rover': 10524,\n",
       " 'travel': 12603,\n",
       " 'rocki': 10418,\n",
       " 'road': 10378,\n",
       " 'seek': 10905,\n",
       " 'beckham': 1046,\n",
       " 'notch': 8548,\n",
       " 'real': 9956,\n",
       " 'european': 4092,\n",
       " 'goal': 5026,\n",
       " 'wind': 13532,\n",
       " 'caus': 2076,\n",
       " 'chao': 2168,\n",
       " 'snowi': 11375,\n",
       " 'mountain': 8138,\n",
       " 'dribbl': 3676,\n",
       " 'nat': 8322,\n",
       " 'faster': 4284,\n",
       " 'premier': 9547,\n",
       " 'biotech': 1224,\n",
       " 'allianc': 296,\n",
       " 'pfizer': 9208,\n",
       " 'vaccin': 13013,\n",
       " 'move': 8146,\n",
       " 'closer': 2420,\n",
       " 'get': 4925,\n",
       " 'approv': 488,\n",
       " 'mccain': 7586,\n",
       " 'potato': 9493,\n",
       " 'cost': 2809,\n",
       " 'father': 4290,\n",
       " 'twin': 12766,\n",
       " 'toddler': 12452,\n",
       " 'grant': 5125,\n",
       " 'return': 10259,\n",
       " 'christchurch': 2295,\n",
       " 'quak': 9767,\n",
       " 'koala': 6682,\n",
       " 'sterilis': 11712,\n",
       " 'kangaroo': 6490,\n",
       " 'environ': 4004,\n",
       " 'band': 862,\n",
       " 'adel': 108,\n",
       " 'domin': 3588,\n",
       " 'chart': 2195,\n",
       " 'metro': 7803,\n",
       " 'confus': 2643,\n",
       " 'commut': 2571,\n",
       " 'commonwealth': 2566,\n",
       " 'ombudsman': 8697,\n",
       " 'condit': 2623,\n",
       " 'nauru': 8337,\n",
       " 'countri': 2835,\n",
       " 'wide': 13485,\n",
       " 'phil': 9219,\n",
       " 'saudi': 10720,\n",
       " 'arabia': 501,\n",
       " 'grace': 5095,\n",
       " 'period': 9159,\n",
       " 'vote': 13216,\n",
       " 'cervic': 2133,\n",
       " 'begin': 1069,\n",
       " 'boyfriend': 1481,\n",
       " 'sperm': 11528,\n",
       " 'brisban': 1593,\n",
       " 'doubl': 3624,\n",
       " 'shooter': 11100,\n",
       " 'agent': 170,\n",
       " 'highlight': 5638,\n",
       " 'rental': 10167,\n",
       " 'shortag': 11109,\n",
       " 'boom': 1402,\n",
       " 'sugar': 11904,\n",
       " 'product': 9624,\n",
       " 'challeng': 2150,\n",
       " 'talk': 12119,\n",
       " 'continu': 2712,\n",
       " 'courthous': 2843,\n",
       " 'act': 87,\n",
       " 'nurs': 8590,\n",
       " 'rape': 9908,\n",
       " 'grow': 5228,\n",
       " 'toxic': 12542,\n",
       " 'illeg': 5957,\n",
       " 'store': 11769,\n",
       " 'station': 11669,\n",
       " 'gympi': 5307,\n",
       " 'prison': 9602,\n",
       " 'exchang': 4134,\n",
       " 'enhanc': 3974,\n",
       " 'egypt': 3854,\n",
       " 'israel': 6242,\n",
       " 'tie': 12390,\n",
       " 'winton': 13559,\n",
       " 'reflect': 10054,\n",
       " 'latest': 6855,\n",
       " 'novel': 8558,\n",
       " 'minist': 7899,\n",
       " 'cyril': 3041,\n",
       " 'rioli': 10345,\n",
       " 'voic': 13202,\n",
       " 'insid': 6122,\n",
       " 'dale': 3059,\n",
       " 'fidel': 4370,\n",
       " 'castro': 2044,\n",
       " 'entir': 3993,\n",
       " 'bash': 961,\n",
       " 'ocean': 8638,\n",
       " 'pool': 9445,\n",
       " 'campaign': 1881,\n",
       " 'ramp': 9885,\n",
       " 'half': 5344,\n",
       " 'offer': 8658,\n",
       " 'sue': 11899,\n",
       " 'colleg': 2504,\n",
       " 'haze': 5485,\n",
       " 'see': 10902,\n",
       " 'tini': 12427,\n",
       " 'local': 7125,\n",
       " 'market': 7477,\n",
       " 'close': 2419,\n",
       " 'rain': 9867,\n",
       " 'aid': 193,\n",
       " 'bulldog': 1703,\n",
       " 'creek': 2902,\n",
       " 'cadel': 1826,\n",
       " 'evan': 4103,\n",
       " 'triumph': 12663,\n",
       " 'itali': 6245,\n",
       " 'kite': 6652,\n",
       " 'runner': 10567,\n",
       " 'censor': 2108,\n",
       " 'activ': 90,\n",
       " 'pick': 9249,\n",
       " 'sydney': 12059,\n",
       " 'search': 10876,\n",
       " 'sister': 11217,\n",
       " 'public': 9698,\n",
       " 'sector': 10896,\n",
       " 'union': 12894,\n",
       " 'influenc': 6076,\n",
       " 'secur': 10898,\n",
       " 'passag': 9030,\n",
       " 'hamilton': 5358,\n",
       " 'wast': 13329,\n",
       " 'boost': 1408,\n",
       " 'region': 10077,\n",
       " 'deep': 3195,\n",
       " 'purpl': 9729,\n",
       " 'member': 7726,\n",
       " 'aust': 702,\n",
       " 'staff': 11608,\n",
       " 'stop': 11764,\n",
       " 'debacl': 3154,\n",
       " 'undercov': 12839,\n",
       " 'oper': 8727,\n",
       " 'plane': 9322,\n",
       " 'crash': 2879,\n",
       " 'channel': 2166,\n",
       " 'ten': 12242,\n",
       " 'canadian': 1898,\n",
       " 'owner': 8878,\n",
       " 'mutant': 8238,\n",
       " 'loss': 7189,\n",
       " 'shark': 11027,\n",
       " 'bite': 1242,\n",
       " 'sink': 11209,\n",
       " 'cheetah': 2217,\n",
       " 'wildcat': 13504,\n",
       " 'final': 4400,\n",
       " 'aliv': 281,\n",
       " 'track': 12551,\n",
       " 'hottest': 5826,\n",
       " 'children': 2256,\n",
       " 'india': 6035,\n",
       " 'sweat': 12034,\n",
       " 'tendulkar': 12247,\n",
       " 'fit': 4451,\n",
       " 'swamp': 12023,\n",
       " 'cheap': 2203,\n",
       " 'knight': 6669,\n",
       " 'consid': 2675,\n",
       " 'reject': 10108,\n",
       " 'booz': 1414,\n",
       " 'cultur': 2992,\n",
       " 'serv': 10965,\n",
       " 'timer': 12416,\n",
       " 'villag': 13147,\n",
       " 'fete': 4358,\n",
       " 'pie': 9257,\n",
       " 'coast': 2441,\n",
       " 'wild': 13502,\n",
       " 'offic': 8659,\n",
       " 'breed': 1541,\n",
       " 'cattl': 2071,\n",
       " 'milit': 7859,\n",
       " 'philippin': 9224,\n",
       " 'bomb': 1376,\n",
       " 'iran': 6210,\n",
       " 'coach': 2436,\n",
       " 'blue': 1315,\n",
       " 'bring': 1589,\n",
       " 'idri': 5946,\n",
       " 'cover': 2850,\n",
       " 'chang': 2164,\n",
       " 'finalist': 4402,\n",
       " 'alcohol': 249,\n",
       " 'alic': 273,\n",
       " 'spring': 11575,\n",
       " 'casino': 2030,\n",
       " 'fin': 4398,\n",
       " 'eject': 3865,\n",
       " 'drunk': 3703,\n",
       " 'aim': 197,\n",
       " 'self': 10918,\n",
       " 'suffici': 11902,\n",
       " 'gile': 4947,\n",
       " 'launceston': 6862,\n",
       " 'respit': 10221,\n",
       " 'care': 1969,\n",
       " 'servic': 10967,\n",
       " 'hepburn': 5589,\n",
       " 'virginia': 13173,\n",
       " 'trioli': 12655,\n",
       " 'lockdown': 7130,\n",
       " 'melburnian': 7716,\n",
       " 'mayor': 7576,\n",
       " 'vietnam': 13133,\n",
       " 'trip': 12656,\n",
       " 'lend': 6963,\n",
       " 'slide': 11284,\n",
       " 'midland': 7836,\n",
       " 'emerg': 3924,\n",
       " 'violent': 13166,\n",
       " 'coff': 2475,\n",
       " 'robberi': 10391,\n",
       " 'super': 11947,\n",
       " 'iceland': 5932,\n",
       " 'iraqi': 6213,\n",
       " 'invas': 6187,\n",
       " 'passion': 9033,\n",
       " 'agricultur': 183,\n",
       " 'julia': 6446,\n",
       " 'hailstorm': 5328,\n",
       " 'toll': 12461,\n",
       " 'fruit': 4720,\n",
       " 'tree': 12619,\n",
       " 'hour': 5831,\n",
       " 'wastewat': 13330,\n",
       " 'spend': 11527,\n",
       " 'free': 4673,\n",
       " 'whoop': 13477,\n",
       " 'cough': 2821,\n",
       " 'nuttal': 8596,\n",
       " 'attempt': 670,\n",
       " 'overthrow': 8868,\n",
       " 'beatti': 1038,\n",
       " 'thwart': 12373,\n",
       " 'go': 5025,\n",
       " 'raid': 9861,\n",
       " 'wednesday': 13385,\n",
       " 'august': 686,\n",
       " 'issu': 6243,\n",
       " 'south': 11464,\n",
       " 'territori': 12272,\n",
       " 'primari': 9586,\n",
       " 'produc': 9623,\n",
       " 'farm': 4269,\n",
       " 'financ': 4403,\n",
       " 'loan': 7121,\n",
       " 'meander': 7671,\n",
       " 'option': 8743,\n",
       " 'manag': 7395,\n",
       " 'bonus': 1390,\n",
       " 'pilbara': 9268,\n",
       " 'look': 7171,\n",
       " 'perform': 9154,\n",
       " 'semi': 10927,\n",
       " 'secret': 10891,\n",
       " 'penguin': 9132,\n",
       " 'chase': 2197,\n",
       " 'danger': 3083,\n",
       " 'freeway': 4678,\n",
       " 'drive': 3685,\n",
       " 'kewel': 6589,\n",
       " 'liverpool': 7108,\n",
       " 'histor': 5680,\n",
       " 'juve': 6468,\n",
       " 'rematch': 10141,\n",
       " 'human': 5861,\n",
       " 'right': 10332,\n",
       " 'watch': 13331,\n",
       " 'palestinian': 8930,\n",
       " 'leigh': 6953,\n",
       " 'complet': 2587,\n",
       " 'hawaiian': 5473,\n",
       " 'ironman': 6221,\n",
       " 'kevin': 6588,\n",
       " 'mcdonald': 7606,\n",
       " 'ceas': 2093,\n",
       " 'effect': 3846,\n",
       " 'gaza': 4866,\n",
       " 'protea': 9668,\n",
       " 'lodg': 7142,\n",
       " 'hall': 5346,\n",
       " 'marijuana': 7465,\n",
       " 'possess': 9478,\n",
       " 'boss': 1432,\n",
       " 'andrew': 383,\n",
       " 'demetriou': 3264,\n",
       " 'wayn': 13360,\n",
       " 'bennett': 1120,\n",
       " 'golden': 5043,\n",
       " 'point': 9392,\n",
       " 'cowboy': 2859,\n",
       " 'wollongong': 13594,\n",
       " 'basketbal': 972,\n",
       " 'groom': 5213,\n",
       " 'offenc': 8655,\n",
       " 'activist': 91,\n",
       " 'joshua': 6416,\n",
       " 'wong': 13603,\n",
       " 'china': 2265,\n",
       " 'peta': 9189,\n",
       " 'robot': 10404,\n",
       " 'bolt': 1372,\n",
       " 'approach': 486,\n",
       " 'ultim': 12795,\n",
       " 'sprint': 11583,\n",
       " 'sach': 10600,\n",
       " 'shellfish': 11058,\n",
       " 'reef': 10043,\n",
       " 'timor': 12419,\n",
       " 'civilian': 2342,\n",
       " 'gosnel': 5076,\n",
       " 'deliber': 3241,\n",
       " 'scipion': 10803,\n",
       " 'honour': 5772,\n",
       " 'whistleblow': 13466,\n",
       " 'door': 3608,\n",
       " 'restaur': 10226,\n",
       " 'belinda': 1086,\n",
       " 'neal': 8348,\n",
       " 'marin': 7467,\n",
       " 'debt': 3162,\n",
       " 'ski': 11235,\n",
       " 'serbia': 10955,\n",
       " 'quit': 9810,\n",
       " 'fail': 4225,\n",
       " 'greater': 5151,\n",
       " 'lofti': 7145,\n",
       " 'rang': 9895,\n",
       " 'heritag': 5599,\n",
       " 'waff': 13233,\n",
       " 'agenc': 168,\n",
       " 'need': 8356,\n",
       " 'unhealthi': 12884,\n",
       " 'alert': 258,\n",
       " 'canteen': 1929,\n",
       " 'cemeteri': 2106,\n",
       " 'hell': 5562,\n",
       " 'ignor': 5950,\n",
       " 'hater': 5461,\n",
       " 'enni': 3977,\n",
       " 'near': 8350,\n",
       " 'baghdad': 807,\n",
       " 'better': 1157,\n",
       " 'white': 13468,\n",
       " 'kingston': 6637,\n",
       " 'larri': 6845,\n",
       " 'lobster': 7124,\n",
       " 'stay': 11675,\n",
       " 'driver': 3686,\n",
       " 'fieri': 4376,\n",
       " 'highway': 5640,\n",
       " 'coupl': 2838,\n",
       " 'young': 13753,\n",
       " 'marri': 7491,\n",
       " 'man': 7393,\n",
       " 'rooster': 10477,\n",
       " 'wooli': 13623,\n",
       " 'threaten': 12351,\n",
       " 'walk': 13257,\n",
       " 'away': 754,\n",
       " 'morrison': 8089,\n",
       " 'asset': 625,\n",
       " 'test': 12280,\n",
       " 'pension': 9142,\n",
       " 'whitlam': 13472,\n",
       " 'engin': 3970,\n",
       " 'skill': 11238,\n",
       " 'industri': 6052,\n",
       " 'matur': 7554,\n",
       " 'howard': 5841,\n",
       " 'armi': 550,\n",
       " 'murder': 8214,\n",
       " 'villawood': 13150,\n",
       " 'cheaper': 2204,\n",
       " 'anderson': 378,\n",
       " 'label': 6760,\n",
       " 'green': 5158,\n",
       " 'brown': 1644,\n",
       " 'accord': 64,\n",
       " 'ipart': 6205,\n",
       " 'madrid': 7309,\n",
       " 'thump': 12364,\n",
       " 'avoid': 744,\n",
       " 'upset': 12973,\n",
       " 'toni': 12479,\n",
       " 'abbott': 11,\n",
       " 'turkish': 12744,\n",
       " 'prevent': 9574,\n",
       " 'foreign': 4588,\n",
       " 'fighter': 4385,\n",
       " 'cloth': 2424,\n",
       " 'nappi': 8303,\n",
       " 'salin': 10644,\n",
       " 'unseason': 12936,\n",
       " 'summer': 11923,\n",
       " 'govern': 5087,\n",
       " 'announc': 419,\n",
       " 'buyout': 1805,\n",
       " 'lawyer': 6890,\n",
       " 'want': 13285,\n",
       " 'forest': 4593,\n",
       " 'protest': 9671,\n",
       " 'pygmi': 9744,\n",
       " 'peopl': 9144,\n",
       " 'flore': 4519,\n",
       " 'ancestor': 372,\n",
       " 'hobbit': 5697,\n",
       " 'suspici': 12010,\n",
       " 'agre': 179,\n",
       " 'ramsi': 9890,\n",
       " 'review': 10275,\n",
       " 'rundl': 10566,\n",
       " 'mall': 7377,\n",
       " 'garden': 4820,\n",
       " 'feel': 4318,\n",
       " 'economist': 3824,\n",
       " 'haigh': 5326,\n",
       " 'chocol': 2277,\n",
       " 'recal': 9982,\n",
       " 'hazelnut': 5487,\n",
       " 'luke': 7233,\n",
       " 'odonnel': 8647,\n",
       " 'million': 7872,\n",
       " 'patel': 9045,\n",
       " 'medic': 7689,\n",
       " 'suspens': 12009,\n",
       " 'nitschk': 8484,\n",
       " 'licenc': 7026,\n",
       " 'fight': 4383,\n",
       " 'knowl': 6678,\n",
       " 'focus': 4545,\n",
       " 'samaritan': 10661,\n",
       " 'xmas': 13697,\n",
       " 'keen': 6541,\n",
       " 'victorian': 13124,\n",
       " 'team': 12202,\n",
       " 'head': 5491,\n",
       " 'stricken': 11816,\n",
       " 'sell': 10922,\n",
       " 'tassi': 12178,\n",
       " 'devil': 3370,\n",
       " 'diseas': 3483,\n",
       " 'dual': 3706,\n",
       " 'citizen': 2335,\n",
       " 'child': 2251,\n",
       " 'porn': 9459,\n",
       " 'webber': 13377,\n",
       " 'pole': 9406,\n",
       " 'solomon': 11419,\n",
       " 'warlord': 13296,\n",
       " 'hostag': 5816,\n",
       " 'bundaberg': 1724,\n",
       " 'revamp': 10265,\n",
       " 'singer': 11205,\n",
       " 'toad': 12443,\n",
       " 'awar': 751,\n",
       " 'reach': 9947,\n",
       " 'deal': 3148,\n",
       " ...}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostramos el vocabulario:\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "108845c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981006, 13808)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "731d4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numéro de traces:  981006\n",
      "Número de tokens:  13808\n"
     ]
    }
   ],
   "source": [
    "print('Numéro de traces: ',info_train_vec.shape[0])\n",
    "print('Número de tokens: ',info_train_vec.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efbf8e",
   "metadata": {},
   "source": [
    "**3) Implementación del modelo:** Ya contamos con el elemento principal para implementar nuestro modelo [LDA-SKTL](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Acerca de los parámetros que vamos a utilizar:\n",
    "- n_components: número de tópicos:\n",
    "- max_iter: the maximum number of passes over the training data.\n",
    "- learning_method: method used to update_component. Only used in fit method. In general, if the data size is large, the online update will be much faster than the batch update.\n",
    "- learning_offset: A (positive) parameter that downweights early iterations in online learning\n",
    "- random_state: Pass an int for reproducible results across multiple function calls.\n",
    "\n",
    "Existen más parámetros para configurar el modelo pero no aondaremos en ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e56f9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 10\n",
    "#Implementamos el modelo\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, max_iter=5, learning_method='online',learning_offset=50., random_state=0)\n",
    "\n",
    "lda.fit(info_train_vec)\n",
    "\n",
    "# making LDA TOP MATRIX USING CORPUS TF\n",
    "lda_topic_modelling = lda.fit_transform(info_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc0a9c",
   "metadata": {},
   "source": [
    "Nota: Las siguientes funciones no tienen otro objetivo más que trabajar con la información que arrojó el modelo, no son fundamentales para efectos del desarrollo del modelo como tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f645769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return an integer list of predicted topic catergories for a given topic matrix\n",
    "def get_keys(topic_matrix):\n",
    "  # print(topic_matrix.argmax(axis = 1)) # axis = 1, will return maximum index in that array \n",
    "  keys = topic_matrix.argmax(axis = 1).tolist()\n",
    "  print(\"length of the keys is: \",len(keys))\n",
    "  return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f38d4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a tuple of topic categories and their accompanying magnitude for a given list of keys\n",
    "from collections import Counter\n",
    "def key_to_count(keys):\n",
    "  count_pairs = Counter(keys).items()\n",
    "  # print(\"Count_pairs\",count_pairs)\n",
    "  categories = [pair[0] for pair in count_pairs]\n",
    "  # print(\"categories\",categories)\n",
    "  counts = [pair[1] for pair in count_pairs]\n",
    "  # print(\"Counts: \",counts)\n",
    "  return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e4650f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the keys is:  981006\n"
     ]
    }
   ],
   "source": [
    "lda_keys = get_keys(lda_topic_modelling)\n",
    "# print(\"keys: \",lda_keys)\n",
    "# key_to_count(lda_keys)\n",
    "\n",
    "lda_categories, lda_count = key_to_count(lda_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9083d4",
   "metadata": {},
   "source": [
    "**4) Visualización de la información:** Con el fin de mostrar de una manera ordenada los resultados arrojados por el modelo mostramos lo siguiente:\n",
    "- 4.1) El tópico predominante para cada documento perteneciente a la información de entrenamiento\n",
    "- 4.2) la contribución de cada tópico para cada documento\n",
    "- 4.3) Número de documentos asociados a cada tópico\n",
    "- 4.4) Tabla que contenga la contribución de cada palabra a su respectivo tópico \n",
    "- 4.5) Las palabras más importantes asociadas a cada tópico\n",
    "- 4.6) [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/index.html): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb782d8",
   "metadata": {},
   "source": [
    "**4.1) Tópico predominante para cada documento:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5ef8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #0 - topic: 0\n",
      "\n",
      "Document #1 - topic: 5\n",
      "\n",
      "Document #2 - topic: 3\n",
      "\n",
      "Document #3 - topic: 6\n",
      "\n",
      "Document #4 - topic: 5\n",
      "\n",
      "Document #5 - topic: 9\n",
      "\n",
      "Document #6 - topic: 5\n",
      "\n",
      "Document #7 - topic: 6\n",
      "\n",
      "Document #8 - topic: 1\n",
      "\n",
      "Document #9 - topic: 8\n",
      "\n",
      "Document #10 - topic: 6\n",
      "\n",
      "Document #11 - topic: 5\n",
      "\n",
      "Document #12 - topic: 3\n",
      "\n",
      "Document #13 - topic: 2\n",
      "\n",
      "Document #14 - topic: 4\n",
      "\n",
      "Document #15 - topic: 3\n",
      "\n",
      "Document #16 - topic: 0\n",
      "\n",
      "Document #17 - topic: 1\n",
      "\n",
      "Document #18 - topic: 3\n",
      "\n",
      "Document #19 - topic: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostramos el tópico predominante para cada trace_traindata\n",
    "doc_topic = lda.transform(info_train_vec)\n",
    "\n",
    "# check for 20 documents\n",
    "for n in range(20):\n",
    "  # print(doc_topic[n])\n",
    "  topic_most_pr = doc_topic[n].argmax()\n",
    "  # print(topic_most_pr)\n",
    "  print(\"Document #{} - topic: {}\\n\".format(n,topic_most_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df3fc6",
   "metadata": {},
   "source": [
    "**4.2) Tabla tópico-documento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e585f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226253</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226254</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226255</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.516666</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226256</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226257</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226258 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.020000  0.020000  0.020000  0.020000  0.020000  0.820000  0.020000   \n",
       "1        0.025000  0.025000  0.025000  0.025000  0.025000  0.025000  0.025000   \n",
       "2        0.220000  0.420000  0.020000  0.220000  0.020000  0.020000  0.020000   \n",
       "3        0.220000  0.020000  0.020000  0.420000  0.020000  0.020000  0.020000   \n",
       "4        0.020000  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1226253  0.020000  0.020000  0.020000  0.220000  0.020000  0.220000  0.020000   \n",
       "1226254  0.020000  0.020000  0.020000  0.020000  0.420000  0.020000  0.020000   \n",
       "1226255  0.016667  0.016667  0.016667  0.016667  0.016667  0.183333  0.016667   \n",
       "1226256  0.014286  0.300000  0.157143  0.014286  0.157143  0.014286  0.014286   \n",
       "1226257  0.014286  0.014286  0.014286  0.157143  0.014286  0.157143  0.300000   \n",
       "\n",
       "                7         8         9  \n",
       "0        0.020000  0.020000  0.020000  \n",
       "1        0.025000  0.775000  0.025000  \n",
       "2        0.020000  0.020000  0.020000  \n",
       "3        0.220000  0.020000  0.020000  \n",
       "4        0.820000  0.020000  0.020000  \n",
       "...           ...       ...       ...  \n",
       "1226253  0.220000  0.020000  0.220000  \n",
       "1226254  0.420000  0.020000  0.020000  \n",
       "1226255  0.516666  0.016667  0.183333  \n",
       "1226256  0.157143  0.014286  0.157143  \n",
       "1226257  0.157143  0.014286  0.157143  \n",
       "\n",
       "[1226258 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a dataframe from the document-topic matrix\n",
    "doc_topic_df = pd.DataFrame(data=doc_topic)\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe91aae",
   "metadata": {},
   "source": [
    "**4.3) Número de documentos asociados a cada tópico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5edd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = lda.transform(processed_docs_vec)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(processed_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be9c39d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc0          5\n",
       "Doc1          8\n",
       "Doc2          1\n",
       "Doc3          3\n",
       "Doc4          7\n",
       "             ..\n",
       "Doc1226253    3\n",
       "Doc1226254    4\n",
       "Doc1226255    7\n",
       "Doc1226256    1\n",
       "Doc1226257    6\n",
       "Name: dominant_topic, Length: 1226258, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic['dominant_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d4c20ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Num</th>\n",
       "      <th>Num Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>212590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>201546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>155160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>124670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>101651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>97856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>91251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>88591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>78178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>74765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Num  Num Documents\n",
       "0          0         212590\n",
       "1          1         201546\n",
       "2          2         155160\n",
       "3          3         124670\n",
       "4          6         101651\n",
       "5          4          97856\n",
       "6          5          91251\n",
       "7          8          88591\n",
       "8          9          78178\n",
       "9          7          74765"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88066b0b",
   "metadata": {},
   "source": [
    "**4.4) Topic-Keyword Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "370231a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaco</th>\n",
       "      <th>aacta</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abalon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abar</th>\n",
       "      <th>abat</th>\n",
       "      <th>abattoir</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zygier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic0</th>\n",
       "      <td>132.121386</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>629.153581</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>78.339516</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>398.294495</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100023</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>15.614606</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>709.410616</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>183.855465</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100023</td>\n",
       "      <td>0.100040</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>23.598954</td>\n",
       "      <td>0.100024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>0.100021</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>45.519442</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>44.207086</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>32.720552</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13808 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              aaco     aacta     aaron      abalon     abandon      abar  \\\n",
       "Topic0  132.121386  0.100002  0.100005    0.100011    0.100016  0.100009   \n",
       "Topic1    0.100008  0.100007  0.100004  398.294495    0.100010  0.100013   \n",
       "Topic2    0.100008  0.100004  0.100018    0.100006    0.100012  0.100006   \n",
       "Topic3    0.100006  0.100020  0.100007    0.100014  709.410616  0.100014   \n",
       "Topic4    0.100021  0.100019  0.100004    0.100008    0.100010  0.100002   \n",
       "\n",
       "             abat    abattoir        abba     abbey  ...      zone        zoo  \\\n",
       "Topic0   0.100014  629.153581    0.100014  0.100001  ...  0.100012   0.100012   \n",
       "Topic1   0.100016    0.100009    0.100008  0.100012  ...  0.100011   0.100010   \n",
       "Topic2  15.614606    0.100006    0.100008  0.100011  ...  0.100005   0.100010   \n",
       "Topic3   0.100006    0.100008  183.855465  0.100002  ...  0.100009   0.100012   \n",
       "Topic4   0.100016    0.100009    0.100011  0.100005  ...  0.100011  45.519442   \n",
       "\n",
       "         zookeep      zoom  zuckerberg       zuma     zurich    zverev  \\\n",
       "Topic0  0.100004  0.100024    0.100011  78.339516   0.100007  0.100003   \n",
       "Topic1  0.100002  0.100029    0.100009   0.100023   0.100002  0.100008   \n",
       "Topic2  0.100003  0.100011    0.100005   0.100017   0.100004  0.100020   \n",
       "Topic3  0.100027  0.100007    0.100007   0.100023   0.100040  0.100010   \n",
       "Topic4  0.100013  0.100011   44.207086   0.100010  32.720552  0.100002   \n",
       "\n",
       "        zvonareva    zygier  \n",
       "Topic0   0.100002  0.100002  \n",
       "Topic1   0.100002  0.100009  \n",
       "Topic2   0.100006  0.100001  \n",
       "Topic3  23.598954  0.100024  \n",
       "Topic4   0.100004  0.100004  \n",
       "\n",
       "[5 rows x 13808 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(lda.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd571f",
   "metadata": {},
   "source": [
    "**4.5) Tabla Word-Topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "012f2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c29ec1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>plan</td>\n",
       "      <td>council</td>\n",
       "      <td>water</td>\n",
       "      <td>call</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>warn</td>\n",
       "      <td>year</td>\n",
       "      <td>health</td>\n",
       "      <td>rise</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>polic</td>\n",
       "      <td>interview</td>\n",
       "      <td>crash</td>\n",
       "      <td>investig</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>charg</td>\n",
       "      <td>court</td>\n",
       "      <td>face</td>\n",
       "      <td>murder</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>hous</td>\n",
       "      <td>elect</td>\n",
       "      <td>govern</td>\n",
       "      <td>help</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>chang</td>\n",
       "      <td>claim</td>\n",
       "      <td>world</td>\n",
       "      <td>trial</td>\n",
       "      <td>record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>death</td>\n",
       "      <td>sydney</td>\n",
       "      <td>say</td>\n",
       "      <td>open</td>\n",
       "      <td>polic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>australia</td>\n",
       "      <td>farmer</td>\n",
       "      <td>servic</td>\n",
       "      <td>talk</td>\n",
       "      <td>protest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>kill</td>\n",
       "      <td>report</td>\n",
       "      <td>australian</td>\n",
       "      <td>urg</td>\n",
       "      <td>fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>govt</td>\n",
       "      <td>market</td>\n",
       "      <td>coast</td>\n",
       "      <td>case</td>\n",
       "      <td>boost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word 0     Word 1      Word 2    Word 3   Word 4\n",
       "index                                                       \n",
       "Topic 0       plan    council       water      call     fear\n",
       "Topic 1       warn       year      health      rise     test\n",
       "Topic 2      polic  interview       crash  investig    fight\n",
       "Topic 3      charg      court        face    murder    woman\n",
       "Topic 4       hous      elect      govern      help      win\n",
       "Topic 5      chang      claim       world     trial   record\n",
       "Topic 6      death     sydney         say      open    polic\n",
       "Topic 7  australia     farmer      servic      talk  protest\n",
       "Topic 8       kill     report  australian       urg     fund\n",
       "Topic 9       govt     market       coast      case    boost"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic - Keywords Dataframe\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda, n_words=5) \n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords.index.name='index'\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b459e2",
   "metadata": {},
   "source": [
    "**4.6) pyLDAvis:** The pyLDAvis offers the best visualization to view the topics-keywords distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7bb9c9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2156423697090985925207548594\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2156423697090985925207548594_data = {\"mdsDat\": {\"x\": [32.42095184326172, 0.648955225944519, 71.01968383789062, 3.7859814167022705, -42.666500091552734, -3.6216893196105957, -3.632401704788208, -36.58440017700195, 44.391380310058594, 40.235809326171875], \"y\": [8.115229606628418, 71.70709991455078, 9.971006393432617, 33.18513488769531, -9.22206974029541, -43.52651596069336, -4.171707630157471, 35.804412841796875, 50.61538314819336, -31.591665267944336], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.70025115512123, 10.44723316270002, 10.405568307082472, 10.243107131087761, 10.110136577664957, 10.016527358145904, 9.859816325406628, 9.539459055447317, 9.415395548142767, 9.262505379200956]}, \"tinfo\": {\"Term\": [\"polic\", \"charg\", \"australia\", \"plan\", \"govt\", \"interview\", \"kill\", \"report\", \"australian\", \"death\", \"crash\", \"council\", \"face\", \"court\", \"warn\", \"urg\", \"water\", \"fund\", \"sydney\", \"say\", \"murder\", \"year\", \"chang\", \"hous\", \"open\", \"health\", \"elect\", \"claim\", \"world\", \"jail\", \"govt\", \"market\", \"coast\", \"case\", \"boost\", \"worker\", \"minist\", \"road\", \"concern\", \"gold\", \"famili\", \"close\", \"countri\", \"region\", \"farm\", \"strike\", \"reject\", \"offer\", \"offic\", \"storm\", \"local\", \"vote\", \"cut\", \"bomb\", \"develop\", \"law\", \"give\", \"hour\", \"parti\", \"award\", \"plan\", \"warn\", \"year\", \"health\", \"rise\", \"test\", \"south\", \"melbourn\", \"north\", \"continu\", \"power\", \"rural\", \"support\", \"high\", \"probe\", \"public\", \"union\", \"secur\", \"canberra\", \"brisban\", \"hear\", \"perth\", \"question\", \"women\", \"trade\", \"begin\", \"station\", \"issu\", \"remain\", \"wont\", \"airport\", \"fatal\", \"nation\", \"seek\", \"say\", \"death\", \"sydney\", \"open\", \"jail\", \"miss\", \"hospit\", \"drug\", \"die\", \"price\", \"arrest\", \"busi\", \"child\", \"tell\", \"adelaid\", \"search\", \"fall\", \"iraq\", \"guilti\", \"bushfir\", \"look\", \"speak\", \"second\", \"children\", \"tasmanian\", \"media\", \"citi\", \"risk\", \"club\", \"week\", \"name\", \"say\", \"polic\", \"kill\", \"report\", \"australian\", \"urg\", \"fund\", \"green\", \"time\", \"forc\", \"budget\", \"land\", \"cost\", \"bodi\", \"rain\", \"target\", \"campaign\", \"near\", \"hit\", \"sign\", \"game\", \"save\", \"premier\", \"blaze\", \"england\", \"million\", \"battl\", \"rate\", \"boat\", \"despit\", \"cancer\", \"right\", \"attack\", \"leader\", \"council\", \"charg\", \"face\", \"murder\", \"woman\", \"group\", \"final\", \"work\", \"want\", \"push\", \"alleg\", \"assault\", \"defend\", \"appeal\", \"mayor\", \"teen\", \"aussi\", \"island\", \"action\", \"doctor\", \"play\", \"drive\", \"launch\", \"ahead\", \"beat\", \"compani\", \"reveal\", \"anti\", \"east\", \"stop\", \"stand\", \"court\", \"water\", \"call\", \"fear\", \"coronavirus\", \"indigen\", \"get\", \"student\", \"trump\", \"victoria\", \"job\", \"threat\", \"weather\", \"find\", \"river\", \"liber\", \"rail\", \"link\", \"critic\", \"debat\", \"demand\", \"chines\", \"rais\", \"sport\", \"girl\", \"move\", \"approv\", \"truck\", \"good\", \"ralli\", \"bush\", \"plan\", \"council\", \"school\", \"home\", \"queensland\", \"west\", \"mine\", \"say\", \"studi\", \"hous\", \"elect\", \"govern\", \"help\", \"win\", \"china\", \"industri\", \"lead\", \"centr\", \"train\", \"communiti\", \"park\", \"share\", \"need\", \"back\", \"delay\", \"futur\", \"welcom\", \"announc\", \"prison\", \"start\", \"injur\", \"decis\", \"expect\", \"darwin\", \"threaten\", \"peopl\", \"join\", \"team\", \"increas\", \"flood\", \"polic\", \"chang\", \"claim\", \"world\", \"trial\", \"record\", \"labor\", \"take\", \"break\", \"state\", \"news\", \"live\", \"head\", \"inquiri\", \"port\", \"bank\", \"sale\", \"tasmania\", \"life\", \"make\", \"hill\", \"star\", \"plead\", \"challeng\", \"tourism\", \"food\", \"senat\", \"turn\", \"titl\", \"stay\", \"covid\", \"accus\", \"meet\", \"deni\", \"interview\", \"crash\", \"investig\", \"fight\", \"lose\", \"victim\", \"feder\", \"resid\", \"releas\", \"review\", \"sentenc\", \"race\", \"research\", \"rescu\", \"rat\", \"unit\", \"presid\", \"black\", \"clash\", \"go\", \"light\", \"promis\", \"show\", \"john\", \"number\", \"victori\", \"plane\", \"indonesia\", \"order\", \"christma\", \"polic\", \"leav\", \"driver\", \"court\", \"australia\", \"farmer\", \"servic\", \"talk\", \"protest\", \"dead\", \"shoot\", \"deal\", \"return\", \"consid\", \"opposit\", \"abus\", \"chief\", \"drought\", \"season\", \"free\", \"spark\", \"stab\", \"build\", \"india\", \"project\", \"fail\", \"care\", \"suspect\", \"drop\", \"violenc\", \"teacher\", \"export\", \"confid\", \"cattl\"], \"Freq\": [31887.0, 16666.0, 15942.0, 18642.0, 14154.0, 12988.0, 12789.0, 12371.0, 12285.0, 12258.0, 11715.0, 15788.0, 11458.0, 14553.0, 11309.0, 11175.0, 10663.0, 10553.0, 10504.0, 18832.0, 10147.0, 10247.0, 9671.0, 9336.0, 9389.0, 9181.0, 8909.0, 8773.0, 8624.0, 8767.0, 14153.915569821454, 8714.442261546732, 7593.649262581598, 6990.485770328782, 6922.588898026754, 6826.618509326829, 6743.92484960184, 6704.6817728372, 6356.43058140871, 5630.61758278248, 5627.125123072479, 5148.529761900321, 4727.8404179831905, 4720.152679715207, 4711.123258292816, 4594.1579070617645, 4463.775979717268, 4452.208621727015, 4384.635345499472, 4282.813146025861, 4191.712966282362, 4041.323945388614, 4012.4729325322096, 3910.432313825382, 3826.6421795971382, 3719.611958786286, 3695.876459993423, 3670.17248502882, 3591.530397103846, 3353.470009356809, 4697.2422159564085, 11308.36661474395, 10246.211878830987, 9181.043018635452, 7934.700313819051, 7730.26927323437, 7585.775219542234, 6801.762223028719, 6763.424516424601, 6614.597073584908, 6496.410484904188, 6440.517814450049, 5750.88443095563, 5662.7546251731155, 5573.031474317118, 5381.132062895774, 5319.776133564551, 4933.200249089519, 4907.285897210111, 4638.4901302276285, 4588.175081818763, 4567.967443941982, 4391.300741991752, 4287.62228203697, 4154.018205588551, 4065.3989617028155, 3541.9327085402583, 3128.617274552319, 2847.111494813473, 2792.4886574084776, 2765.151431715426, 3903.7058028145207, 6199.158557382559, 5263.724587718437, 4184.924281550313, 12258.07865090431, 10503.669084536796, 9388.631437139431, 8766.102799943028, 8602.776799366402, 8344.679725372784, 7591.488160957902, 6985.9896195267975, 6631.115173464298, 6499.122432043116, 6491.056779862105, 5935.451377846886, 5249.981218564819, 5228.878484780454, 5175.052501624061, 5165.3051922923805, 4669.212275754116, 4599.2115994502465, 4470.14262087203, 4330.026904440535, 3848.0293451594207, 3840.775946479342, 3492.3869720751977, 3306.4272481844414, 3194.0696606310926, 3122.496516208165, 3061.1252329014437, 3034.3682051426536, 2815.586473029691, 2747.4726126978785, 9565.422053311344, 9316.929537230422, 12788.62557339703, 12370.711379744793, 12284.527356968996, 11174.862908621042, 10552.9531539738, 6128.198240788746, 5528.917048482375, 5327.249102751331, 4989.3970220761885, 4419.160099734757, 4158.363729848802, 4113.588890776753, 4090.6511179291124, 4063.9023505256687, 3839.546702453239, 3835.643597494953, 3723.3898790830162, 3593.937440689403, 3489.280655132766, 3395.714086128725, 3348.33566898217, 3298.800312949354, 3138.6700791416547, 3116.5917392147608, 3004.04422395883, 2864.9844885508924, 2830.7074692612873, 2774.9310619320336, 2716.516108172091, 2693.3893253992087, 7939.335396705808, 3266.840747470193, 4319.842780135133, 16665.827883955448, 11457.294725643247, 10146.42878209415, 8097.471812753048, 6830.406965112772, 6821.281446830002, 6736.559835578152, 5848.604112157976, 5739.103760560558, 5524.156672651465, 5191.933898784039, 4809.387795840578, 4603.580550841031, 4478.223836068907, 4437.980739304507, 4372.7249717617515, 4061.441013789795, 4051.914466185975, 3878.4415438680408, 3836.326942952457, 3653.5411926436295, 3534.110610460536, 3532.044282087064, 3530.021154510518, 3384.359423421619, 3238.427256231314, 3226.721390283326, 3210.4542630858186, 3202.8063400925917, 3079.509022961046, 11523.395076264742, 10662.54536076783, 8538.288348021471, 7053.539459294798, 6638.178689201341, 5811.472027728794, 5275.743578552164, 4531.46638280258, 4435.752626581479, 3904.318906662728, 3897.5041618409273, 3616.3285283777327, 3609.53676431196, 3536.431049664432, 3527.5619783013117, 3351.123921569478, 3153.871103773439, 3129.049731204426, 2876.065556362792, 2873.604600890352, 2866.7049684620215, 2825.9336010185134, 2778.7882806482567, 2609.9484100196796, 2573.673660131445, 2558.785001373715, 2502.789042009642, 2459.906861767922, 2415.1601586912243, 2379.1880678210628, 2368.2919985897747, 13944.064656139873, 11467.99310093168, 6333.890121668092, 6073.970814876792, 4486.50544562192, 3716.0039303440426, 3283.984212089571, 5081.698317105624, 2822.6496925838633, 9335.307363438187, 8908.60238932309, 8488.302084748717, 8317.42292620246, 7307.486398167261, 7155.91658714523, 6399.606267534849, 6291.8881998060715, 5929.277811121434, 5218.538052769869, 5208.541707215338, 5064.79941018935, 4799.897948961161, 4735.644674967074, 4653.80262244856, 4253.766677349077, 4054.102836994151, 4045.670930210479, 3954.3854309560434, 3931.8282964045848, 3830.207033909956, 3804.2594192842403, 3583.638201655222, 3537.5315589320394, 3466.2391060273912, 3274.9734912390936, 3242.0166192160855, 3144.951383991301, 3046.061315627986, 3006.1923003114384, 4382.655302250746, 4959.524590825783, 9671.074383455034, 8772.894721196713, 8623.828102080815, 6595.0765590437895, 6460.691252033944, 6268.171745660027, 5919.987948866705, 5496.255647519841, 5380.7070636968265, 5176.932399734039, 4763.473973404325, 4716.307566685377, 4663.80068968712, 4383.683908433525, 4143.203165063992, 4065.0776172671017, 3838.3527502818024, 3602.2841376423135, 3598.344116522069, 3377.6917664839452, 3143.4560587352917, 3075.3287468787476, 3062.05807023901, 2982.386870570788, 2974.1245033517957, 2969.6619349266457, 2888.544571149777, 2839.311246284314, 2786.301077302124, 2661.583738874801, 6106.968723286079, 4277.238543817293, 3323.4337670273735, 12987.915458452164, 11714.222746013378, 6954.293052248755, 6478.817133290385, 5565.960781396179, 5353.379693082583, 5216.003391505065, 5191.324328064076, 4679.640619752056, 4603.699986577971, 4129.490867750625, 3987.6943151338137, 3900.011465901311, 3688.845222501198, 3503.0146583510827, 3267.1586231080837, 3093.1356389252333, 2977.116348245758, 2911.372364931764, 2901.437808879441, 2900.2336317278505, 2873.230137259527, 2751.341833130554, 2632.5331731063116, 2621.9114210797075, 2611.4884027917565, 2502.33230983349, 2412.4731164756404, 2391.270968491662, 2388.86455491311, 17610.799321654944, 3821.868501567802, 3718.2590648645837, 3029.3436625337267, 15942.065641632926, 7377.107080265316, 7125.274983215448, 7037.585997011014, 7002.238399010305, 6713.740839229963, 6334.94806809841, 6254.359576034788, 6204.489625785637, 5034.0566845555495, 4408.448484939447, 4149.430496581216, 3941.6416867541843, 3891.5310474828957, 3726.890107127519, 3700.0160727605826, 3626.446200786192, 3547.083318957657, 3504.8964292246064, 3361.997129199009, 3266.6169146335874, 3264.462971174047, 3200.1616621030694, 3174.9006649842404, 3143.4294862025063, 3099.814724358203, 2875.398339693083, 2859.0577101940407, 2846.3742113089033, 2767.3851573387215], \"Total\": [31887.0, 16666.0, 15942.0, 18642.0, 14154.0, 12988.0, 12789.0, 12371.0, 12285.0, 12258.0, 11715.0, 15788.0, 11458.0, 14553.0, 11309.0, 11175.0, 10663.0, 10553.0, 10504.0, 18832.0, 10147.0, 10247.0, 9671.0, 9336.0, 9389.0, 9181.0, 8909.0, 8773.0, 8624.0, 8767.0, 14154.81475664772, 8715.34143001926, 7594.548422972642, 6991.384972033013, 6923.4880822036375, 6827.517692448681, 6744.824034176667, 6705.580966245468, 6357.329766708882, 5631.516741270864, 5628.0243151028735, 5149.42894102655, 4728.7395585393315, 4721.051863794303, 4712.022427555587, 4595.057080454809, 4464.675165952868, 4453.107803104626, 4385.534549530135, 4283.712326417461, 4192.6121412751845, 4042.2231355129675, 4013.3721241523153, 3911.3315142482625, 3827.5413595094506, 3720.511151250726, 3696.775659961506, 3671.071615871787, 3592.429598764122, 3354.369200759752, 18642.106106495663, 11309.265193889192, 10247.110459509788, 9181.94158744136, 7935.598895842968, 7731.167859308837, 7586.6738128375955, 6802.660815493608, 6764.323105839518, 6615.495651035359, 6497.309059373371, 6441.416360343984, 5751.783013824905, 5663.653216814798, 5573.930065570824, 5382.030635860259, 5320.674717856559, 4934.098833774804, 4908.184481572364, 4639.388717968543, 4589.073668541727, 4568.866030561281, 4392.19932826002, 4288.520870379298, 4154.916802081637, 4066.2975373891527, 3542.831286445149, 3129.515843291594, 2848.010072438676, 2793.3872482492743, 2766.050005789259, 3976.0534738816614, 8364.513060348496, 7343.841133974853, 18832.744759808058, 12258.977623940631, 10504.568058849169, 9389.53040889876, 8767.001762572221, 8603.675752338908, 8345.57869919756, 7592.387128405435, 6986.888579639053, 6632.014150341808, 6500.021386867668, 6491.955757229542, 5936.35035696328, 5250.880213883563, 5229.777454949595, 5175.951441237558, 5166.204163257376, 4670.111245250006, 4600.1105777532, 4471.041604789534, 4330.925883919877, 3848.9283139234826, 3841.6749295382447, 3493.2859529955504, 3307.3262406819463, 3194.9686224155776, 3123.395503268178, 3062.0242151755915, 3035.2671817890564, 2816.4854388771064, 2748.3715879220317, 18832.744759808058, 31887.95208466731, 12789.523913812201, 12371.609737927027, 12285.425720578221, 11175.761267245489, 10553.851506527864, 6129.096596277678, 5529.815402817272, 5328.147460444527, 4990.295362192562, 4420.058454190202, 4159.262083015329, 4114.487250145613, 4091.549473333867, 4064.8007015335656, 3840.4450560922046, 3836.541951518241, 3724.288234724938, 3594.8357927614616, 3490.1790075580716, 3396.6124409671, 3349.2340307461855, 3299.6986498216784, 3139.568430194226, 3117.4900811220705, 3004.9425636218016, 2865.882844740217, 2831.605821699792, 2775.8294389303837, 2717.4144489741393, 2694.2876668684517, 10426.786702873964, 4576.84339362458, 15788.634290780448, 16666.725747324595, 11458.192604951993, 10147.32665695455, 8098.3697024857, 6831.304874435966, 6822.179341202692, 6737.457733377833, 5849.502014230885, 5740.001661142343, 5525.054552693884, 5192.831764928171, 4810.285694684087, 4604.478431407638, 4479.121735113767, 4438.878628672518, 4373.622862479715, 4062.3389161529317, 4052.8123549728352, 3879.3394416838205, 3837.2248360059557, 3654.4390779078685, 3535.008502631506, 3532.942188274428, 3530.919049465634, 3385.2573261421953, 3239.325158963414, 3227.619303319295, 3211.3521812429967, 3203.704238444533, 3080.4069106697066, 14553.536332412963, 10663.44293269843, 8539.185936855356, 7054.437042161914, 6639.076269389538, 5812.369607383665, 5276.641165981551, 4532.363958229047, 4436.650183370805, 3905.2164843373994, 3898.401732415304, 3617.226113829801, 3610.4343296774355, 3537.328641558905, 3528.459556834007, 3352.0215157859707, 3154.768673652537, 3129.9473208064833, 2876.9631376090374, 2874.5021745547506, 2867.6025563649255, 2826.831194677945, 2779.6858653771837, 2610.8459745914847, 2574.571254451623, 2559.682573526104, 2503.6866095826063, 2460.804456874183, 2416.0577383251, 2380.085649801559, 2369.189580873866, 18642.106106495663, 15788.634290780448, 8278.143322503925, 8544.779180126172, 6437.361647328921, 5164.694970724031, 4238.709941068391, 18832.744759808058, 3768.3758680299475, 9336.204835655484, 8909.499863883462, 8489.199570818215, 8318.320411825951, 7308.383870967865, 7156.81406527754, 6400.50375119667, 6292.785673218372, 5930.175279813457, 5219.435526830749, 5209.439183391683, 5065.696878472977, 4800.79543383199, 4736.542158985414, 4654.700109228875, 4254.664154057074, 4055.0003039649896, 4046.568404255165, 3955.2828977752424, 3932.7257777868845, 3831.1045079721694, 3805.156880976184, 3584.5356769802156, 3538.4290361813587, 3467.1365874078274, 3275.870969806525, 3242.9140962516108, 3145.8488650938475, 3046.9587953254654, 3007.089781498661, 6856.911363616533, 31887.95208466731, 9671.971151220956, 8773.791487164019, 8624.724854419503, 6595.973328754682, 6461.588004628844, 6269.0685115500355, 5920.884707468818, 5497.152390830047, 5381.603826884463, 5177.8291385967605, 4764.370728190505, 4717.20433048268, 4664.69745490723, 4384.580659865394, 4144.099930008216, 4065.9743737069543, 3839.24951418917, 3603.180905141191, 3599.2408821861072, 3378.5885088957775, 3144.352820134328, 3076.2255263648062, 3062.954829877491, 2983.283635519334, 2975.021261966181, 2970.558689270036, 2889.441330778875, 2840.2080071315686, 2787.197844611738, 2662.480513017147, 8155.251011968378, 5750.293148597325, 4677.878458334667, 12988.812652423287, 11715.119968800567, 6955.190272905839, 6479.714372555791, 5566.858027536842, 5354.276937065238, 5216.90064919691, 5192.2215765728315, 4680.537863072161, 4604.597233141996, 4130.3881064452, 3988.591559614966, 3900.9087109467, 3689.7424569838595, 3503.911901837378, 3268.0558675108696, 3094.032879288225, 2978.013574927791, 2912.2695981282395, 2902.335046298426, 2901.130872902171, 2874.1273842738146, 2752.23907084867, 2633.4304018527364, 2622.8086658505285, 2612.38563888692, 2503.229518021276, 2413.3703630897126, 2392.168208604585, 2389.7618005621835, 31887.95208466731, 5337.700353688755, 5915.640047553728, 14553.536332412963, 15942.96156270709, 7378.002990522395, 7126.170904075117, 7038.481904176988, 7003.134308909817, 6714.63674421261, 6335.843976679697, 6255.2554915433375, 6205.385532173038, 5034.95259957038, 4409.344397738357, 4150.326411648914, 3942.5376064919005, 3892.4269499755032, 3727.786020425709, 3700.911989550418, 3627.342117554706, 3547.9792310770595, 3505.7923442899787, 3362.893031508754, 3267.5128374026945, 3265.358888156858, 3201.057561270421, 3175.796583100949, 3144.3254066181034, 3100.710646121895, 2876.2942444087344, 2859.953619211418, 2847.2701133314895, 2768.2810657895284], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5138, -3.9988, -4.1365, -4.2192, -4.229, -4.2429, -4.2551, -4.261, -4.3143, -4.4356, -4.4362, -4.5251, -4.6103, -4.6119, -4.6138, -4.639, -4.6678, -4.6704, -4.6857, -4.7092, -4.7307, -4.7672, -4.7744, -4.8001, -4.8218, -4.8502, -4.8566, -4.8635, -4.8852, -4.9538, -4.6168, -3.7143, -3.8129, -3.9227, -4.0686, -4.0947, -4.1136, -4.2227, -4.2283, -4.2506, -4.2686, -4.2772, -4.3905, -4.4059, -4.4219, -4.4569, -4.4684, -4.5439, -4.5491, -4.6055, -4.6164, -4.6208, -4.6602, -4.6841, -4.7158, -4.7373, -4.8752, -4.9993, -5.0935, -5.1129, -5.1228, -4.7779, -4.3154, -4.479, -4.7084, -3.6297, -3.7841, -3.8964, -3.965, -3.9838, -4.0142, -4.1088, -4.1919, -4.2441, -4.2642, -4.2654, -4.3549, -4.4776, -4.4817, -4.492, -4.4939, -4.5949, -4.61, -4.6384, -4.6703, -4.7883, -4.7902, -4.8853, -4.94, -4.9746, -4.9972, -5.0171, -5.0258, -5.1007, -5.1252, -3.8777, -3.904, -3.5716, -3.6048, -3.6118, -3.7064, -3.7637, -4.3072, -4.4101, -4.4473, -4.5128, -4.6342, -4.695, -4.7058, -4.7114, -4.718, -4.7748, -4.7758, -4.8055, -4.8409, -4.8704, -4.8976, -4.9117, -4.9266, -4.9763, -4.9834, -5.0202, -5.0676, -5.0796, -5.0995, -5.1208, -5.1293, -4.0483, -4.9363, -4.6569, -3.2937, -3.6684, -3.7899, -4.0155, -4.1857, -4.187, -4.1995, -4.3408, -4.3597, -4.3979, -4.4599, -4.5365, -4.5802, -4.6078, -4.6168, -4.6317, -4.7055, -4.7079, -4.7516, -4.7625, -4.8114, -4.8446, -4.8452, -4.8457, -4.8879, -4.932, -4.9356, -4.9406, -4.943, -4.9823, -3.6627, -3.731, -3.9532, -4.1442, -4.2049, -4.3379, -4.4346, -4.5867, -4.608, -4.7357, -4.7374, -4.8123, -4.8142, -4.8346, -4.8371, -4.8884, -4.9491, -4.957, -5.0413, -5.0422, -5.0446, -5.0589, -5.0757, -5.1384, -5.1524, -5.1582, -5.1803, -5.1976, -5.216, -5.231, -5.2356, -3.4627, -3.6582, -4.2518, -4.2937, -4.5967, -4.7851, -4.9087, -4.4721, -5.0601, -3.8482, -3.895, -3.9433, -3.9636, -4.0931, -4.114, -4.2257, -4.2427, -4.3021, -4.4298, -4.4317, -4.4597, -4.5134, -4.5269, -4.5443, -4.6342, -4.6822, -4.6843, -4.7072, -4.7129, -4.7391, -4.7459, -4.8056, -4.8185, -4.8389, -4.8957, -4.9058, -4.9362, -4.9681, -4.9813, -4.6043, -4.4807, -3.7798, -3.8773, -3.8944, -4.1626, -4.1832, -4.2135, -4.2706, -4.3449, -4.3661, -4.4047, -4.488, -4.4979, -4.5091, -4.5711, -4.6275, -4.6465, -4.7039, -4.7674, -4.7685, -4.8318, -4.9036, -4.9255, -4.9299, -4.9562, -4.959, -4.9605, -4.9882, -5.0054, -5.0242, -5.07, -4.2395, -4.5956, -4.8479, -3.4718, -3.5751, -4.0965, -4.1673, -4.3192, -4.3581, -4.3841, -4.3889, -4.4926, -4.509, -4.6177, -4.6526, -4.6749, -4.7305, -4.7822, -4.8519, -4.9067, -4.9449, -4.9672, -4.9706, -4.9711, -4.9804, -5.0238, -5.0679, -5.072, -5.0759, -5.1186, -5.1552, -5.164, -5.165, -3.1673, -4.6951, -4.7226, -4.9275, -3.2505, -4.0211, -4.0558, -4.0682, -4.0733, -4.1153, -4.1734, -4.1862, -4.1942, -4.4033, -4.536, -4.5965, -4.6479, -4.6607, -4.7039, -4.7111, -4.7312, -4.7534, -4.7653, -4.8069, -4.8357, -4.8364, -4.8563, -4.8642, -4.8742, -4.8881, -4.9633, -4.969, -4.9734, -5.0016], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2346, 0.8565, 2.2588, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2585, 2.2585, 2.2585, 2.2585, 2.2405, 1.9592, 1.9258, 0.7547, 2.2628, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2625, 2.2625, 2.2625, 2.2625, 2.2625, 2.2625, 1.5854, 1.0324, 2.2785, 2.2785, 2.2785, 2.2785, 2.2785, 2.2784, 2.2784, 2.2784, 2.2784, 2.2784, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2782, 2.2782, 2.2782, 2.2782, 2.006, 1.9414, 0.9825, 2.2916, 2.2916, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2913, 2.0582, 2.3008, 2.3008, 2.3008, 2.3008, 2.3008, 2.3008, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.0106, 1.9812, 2.0332, 1.9596, 1.9399, 1.9717, 2.0457, 0.991, 2.012, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3164, 2.3164, 2.3164, 2.3164, 2.3164, 2.3164, 2.3164, 1.8691, 0.4558, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.0605, 2.0538, 2.0079, 2.3628, 2.3627, 2.3627, 2.3627, 2.3627, 2.3627, 2.3627, 2.3627, 2.3626, 2.3626, 2.3626, 2.3626, 2.3626, 2.3626, 2.3626, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3624, 2.3624, 1.7691, 2.0288, 1.8985, 0.7933, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.379, 2.379, 2.379, 2.379, 2.379, 2.379, 2.379, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789]}, \"token.table\": {\"Topic\": [10, 8, 10, 5, 3, 5, 2, 5, 7, 5, 5, 6, 3, 5, 4, 7, 5, 10, 4, 1, 7, 8, 4, 5, 2, 9, 4, 4, 4, 1, 1, 8, 2, 4, 10, 6, 3, 3, 6, 4, 2, 4, 10, 1, 10, 7, 8, 8, 5, 10, 3, 3, 7, 6, 9, 3, 8, 9, 1, 3, 1, 7, 5, 1, 10, 10, 2, 6, 4, 4, 6, 1, 5, 9, 8, 9, 6, 1, 7, 10, 10, 3, 6, 7, 5, 7, 6, 1, 8, 4, 1, 3, 5, 5, 3, 9, 10, 10, 3, 5, 7, 4, 7, 10, 5, 10, 3, 1, 1, 10, 2, 9, 6, 9, 9, 5, 6, 2, 7, 8, 4, 10, 4, 7, 4, 6, 6, 1, 9, 1, 6, 7, 1, 4, 5, 3, 8, 2, 2, 7, 2, 8, 4, 1, 6, 3, 1, 7, 7, 10, 6, 9, 7, 7, 8, 9, 9, 3, 5, 2, 3, 6, 9, 7, 4, 8, 4, 5, 1, 7, 4, 10, 9, 10, 6, 8, 9, 6, 8, 1, 3, 9, 8, 1, 5, 3, 8, 10, 2, 4, 6, 8, 1, 3, 6, 5, 3, 2, 7, 4, 7, 8, 2, 9, 1, 1, 3, 10, 9, 7, 1, 7, 2, 1, 6, 9, 5, 8, 3, 7, 9, 8, 2, 4, 9, 3, 7, 2, 10, 9, 10, 2, 5, 2, 6, 2, 9, 6, 4, 6, 6, 9, 4, 8, 1, 1, 9, 2, 4, 9, 9, 9, 10, 5, 9, 4, 2, 3, 6, 1, 2, 8, 4, 2, 3, 6, 6, 10, 3, 10, 3, 2, 1, 2, 8, 9, 10, 7, 10, 9, 4, 2, 10, 3, 6, 10, 5, 8, 7, 8, 2, 8, 5, 1, 1, 6, 2, 6, 2, 10, 3, 8, 10, 4, 8, 3, 10, 7, 5, 3, 2, 6, 7, 4, 8, 8, 2, 7, 8, 6, 6, 8, 2, 9, 4, 9, 9, 6, 10, 1, 5, 2, 6, 6, 3, 7, 6, 7, 7, 5, 2, 2, 5, 1, 8, 2], \"Freq\": [0.999680407872212, 0.7488426770724246, 0.25100392336126626, 0.9997995577140801, 0.999851340720271, 0.9997333134186132, 0.9996203952252991, 0.9998091326187232, 0.9996756495531675, 0.9998081238023772, 0.9998960943319064, 0.9997257605724381, 0.9998428640758426, 0.9998398244029802, 0.7614042778693988, 0.23852027195631625, 0.9998575866051327, 0.9999396873220004, 0.9999653475111157, 0.9995918157251618, 0.9998495909054406, 0.9997345792749226, 0.9996863289058459, 0.9997397138102689, 0.9996809044647564, 0.9996596473111055, 0.9997882685978866, 0.9997860501291707, 0.999881576946047, 0.9996595752000534, 0.9999295034240194, 0.9997903658570628, 0.999700667899811, 0.9997404237427757, 0.9997739899537207, 0.99949789544768, 0.9997670330805202, 0.9998527782281207, 0.9998611182770669, 0.9998841134072473, 0.9997586721573299, 0.9998474840765288, 0.9996696212891588, 0.9998019030508901, 0.9995372342045178, 0.9998018136466459, 0.9996882651130936, 0.9998995911789053, 0.9999564553148832, 0.9998636394765099, 0.9997725274146435, 0.9996318786916234, 0.9998862531190394, 0.9997059623936831, 0.9996812232240032, 0.9995532095545641, 0.9999097896086114, 0.9995640519926261, 0.9999167012436792, 0.9995825139227745, 0.9999277872834437, 0.9999156946887712, 0.9996285877199096, 0.9997908293642647, 0.9995539189184958, 0.9998108026736019, 0.9999250772638206, 0.9998378886842285, 0.9996965608345569, 0.2736145457826332, 0.726345280332231, 0.9998436034528491, 0.7917663265344319, 0.20812810926605868, 0.9998195243064512, 0.9999043997156196, 0.9996652242093592, 0.9996581119044362, 0.9996721826847101, 0.9999051707133436, 0.9997992901257136, 0.9999202524084291, 0.9998253003392393, 0.9998505588928419, 0.9997327196832597, 0.9998438997690475, 0.9997898745195397, 0.28944745188656024, 0.7103647582119939, 0.9997011924008187, 0.9998585620745533, 0.9998728218392314, 0.9996547242890302, 0.9998798508065101, 0.37138838440795885, 0.6285034197673149, 0.9995784766375281, 0.999890312655577, 0.9998173000952171, 0.9995789371060282, 0.9999438954047816, 0.999818946391243, 0.9998787495306612, 0.9996665613018995, 0.9998959168349572, 0.9995838472267822, 0.9997669152787378, 0.9998179973920645, 0.9997830172561133, 0.9998640566392175, 0.9818781426469805, 0.018108408368489395, 0.9999380471950771, 0.9998273593350778, 0.9998897524621122, 0.9998271313104349, 0.9996243940856115, 0.3606580089575007, 0.6392090793613933, 0.9996567211202027, 0.9997846417628181, 0.9997535770769493, 0.9999193179355103, 0.999753315933414, 0.9996621928114523, 0.9998784897510778, 0.9997781166667516, 0.999790179325755, 0.9995400095863748, 0.9999082411906766, 0.9995622048644278, 0.9998586944731116, 0.99994243961071, 0.9998210835380953, 0.9998089860634315, 0.9997585758571607, 0.9997446940182562, 0.999897452250987, 0.9997660380679685, 0.9998412646110537, 0.9998846651110527, 0.9998258122010929, 0.999654099080483, 0.2890653986407087, 0.7108434134994593, 0.999930657990486, 0.9997080918097174, 0.9998709501690791, 0.9996375959556093, 0.9997344454609806, 0.9997643633360952, 0.9994321787029994, 0.9999212950704738, 0.9996959702287262, 0.9998504822844413, 0.9999374344333826, 0.9998288655149412, 0.9997620516532372, 0.9996704075704743, 0.9998351683399527, 0.9998857348727249, 0.9998969494570137, 0.9998365622830079, 0.9997301634216232, 0.999959035706432, 0.9998295581635347, 0.9997605338931211, 0.9997147099842179, 0.9998626126276885, 0.9998751469922589, 0.7138107466274339, 0.2860049792884331, 0.7160386958325056, 0.28383009528682523, 0.9996952538099293, 0.9996722603798477, 0.9996101958333786, 0.999697336501421, 0.999712296068315, 0.9998539952529455, 0.9997862157089054, 0.9998458686151869, 0.9996552378052137, 0.9998460840541898, 0.9997495636019507, 0.9996968288174156, 0.7437881668768996, 0.2559869491799851, 0.9999028592617607, 0.9998427962529735, 0.7747640309570815, 0.22506847915135678, 0.9998778271794058, 0.9999214577166365, 0.9997333366515193, 0.9998692604466772, 0.9995009452404255, 0.7411070979595945, 0.2588315642978742, 0.9998587395823922, 0.999885536966163, 0.9998398675246775, 0.9998043993731797, 0.9996916794347, 0.9997512292193209, 0.9998781107470257, 0.9999435106042942, 0.9996951025782775, 0.999511652817564, 0.9998624318648164, 0.9998804155370866, 0.9997181250491133, 0.9998104495611191, 0.2519565103410379, 0.7479841558857637, 0.9995088272919346, 0.9996808016057692, 0.9996016136156785, 0.2921793150987546, 0.15554463914240882, 0.5522775483743875, 0.9998675677538084, 0.9997985228405469, 0.9996315483675202, 0.9996661705519876, 0.9998470826028386, 0.9998154517177414, 0.9998331400717478, 0.9998430496135091, 0.9996077472835814, 0.9998380283941757, 0.9998085042746891, 0.999825494625006, 0.30291913159937517, 0.6970246889673828, 0.9997269412951496, 0.9998516870915147, 0.9997563454782099, 0.9998657053183768, 0.9997532579541715, 0.9995438610364087, 0.9997397474985316, 0.9996919466747088, 0.9999089999813633, 0.9997771971533781, 0.9998487760189102, 0.9998850852000569, 0.9996453409879231, 0.9999507147461048, 0.9997987781010422, 0.999767051470815, 0.9997647294987673, 0.9997767210166307, 0.9995909151139868, 0.9998702963339122, 0.9995220752096051, 0.9999245304795733, 0.9996655104259087, 0.99986975709184, 0.9999133607888128, 0.999935361988623, 0.9997603591126262, 0.9998196906542196, 0.22221933411062983, 0.5078919786781778, 0.2698491412067433, 0.7651474193230238, 0.23471446727891304, 0.9998161804166132, 0.999789145508513, 0.9998243137301766, 0.9997772979804778, 0.2830943592150858, 0.7167911048139546, 0.9998119245137104, 0.9996639283260007, 0.9998356895882967, 0.9998343120753732, 0.9998667933296963, 0.9995497953423472, 0.9997675018249389, 0.9999111846832724, 0.9996299997322527, 0.9997588123633988, 0.9996759768290747, 0.9997240031541103, 0.999867903597964, 0.9995697619791057, 0.9997116998583905, 0.9998877979680617, 0.9997653609844958, 0.9995702333746943, 0.9997801799441777, 0.9998337128258898, 0.9997699527043298, 0.9996990625109506, 0.2507711632528929, 0.7491290940348325, 0.9998638658963624, 0.9997491706159053, 0.9999459226837328, 0.9998505785009287, 0.9999315329379902, 0.999803015795273, 0.9996745420727275, 0.9995989991354246, 0.9995500305953571, 0.9996853271114344, 0.9998020606675653, 0.9998323683177469, 0.9998489414109111, 0.9996610347843302, 0.9997341257288359, 0.9998525442970743, 0.9995746765277278, 0.9995697239430905, 0.9997793452612149, 0.9999165567179611, 0.9998524359171619, 0.999673091914339, 0.9998534517386017, 0.9998472608617542, 0.9998731894181964, 0.999676912649699, 0.9999318822917487, 0.9997615108295206, 0.9994695886907761, 0.9996884975923157, 0.999770811854765, 0.9996974101943503, 0.9999141782959192, 0.9998881276663425, 0.9999584625058505, 0.9998797015434223, 0.9998276437469175, 0.9998595342526355, 0.7195003811578555, 0.2803650570281418, 0.9998106461028461, 0.9998308668860499, 0.9998785431166033, 0.999503381333847, 0.9999320614101124, 0.9999241755976328, 0.9999159562268086, 0.9998916319372007], \"Term\": [\"abus\", \"accus\", \"accus\", \"action\", \"adelaid\", \"ahead\", \"airport\", \"alleg\", \"announc\", \"anti\", \"appeal\", \"approv\", \"arrest\", \"assault\", \"attack\", \"attack\", \"aussi\", \"australia\", \"australian\", \"award\", \"back\", \"bank\", \"battl\", \"beat\", \"begin\", \"black\", \"blaze\", \"boat\", \"bodi\", \"bomb\", \"boost\", \"break\", \"brisban\", \"budget\", \"build\", \"bush\", \"bushfir\", \"busi\", \"call\", \"campaign\", \"canberra\", \"cancer\", \"care\", \"case\", \"cattl\", \"centr\", \"challeng\", \"chang\", \"charg\", \"chief\", \"child\", \"children\", \"china\", \"chines\", \"christma\", \"citi\", \"claim\", \"clash\", \"close\", \"club\", \"coast\", \"communiti\", \"compani\", \"concern\", \"confid\", \"consid\", \"continu\", \"coronavirus\", \"cost\", \"council\", \"council\", \"countri\", \"court\", \"court\", \"covid\", \"crash\", \"critic\", \"cut\", \"darwin\", \"dead\", \"deal\", \"death\", \"debat\", \"decis\", \"defend\", \"delay\", \"demand\", \"deni\", \"deni\", \"despit\", \"develop\", \"die\", \"doctor\", \"drive\", \"driver\", \"driver\", \"drop\", \"drought\", \"drug\", \"east\", \"elect\", \"england\", \"expect\", \"export\", \"face\", \"fail\", \"fall\", \"famili\", \"farm\", \"farmer\", \"fatal\", \"fatal\", \"fear\", \"feder\", \"fight\", \"final\", \"find\", \"flood\", \"flood\", \"food\", \"forc\", \"free\", \"fund\", \"futur\", \"game\", \"get\", \"girl\", \"give\", \"go\", \"gold\", \"good\", \"govern\", \"govt\", \"green\", \"group\", \"guilti\", \"head\", \"health\", \"hear\", \"help\", \"high\", \"hill\", \"hit\", \"home\", \"home\", \"hospit\", \"hour\", \"hous\", \"increas\", \"india\", \"indigen\", \"indonesia\", \"industri\", \"injur\", \"inquiri\", \"interview\", \"investig\", \"iraq\", \"island\", \"issu\", \"jail\", \"job\", \"john\", \"join\", \"kill\", \"labor\", \"land\", \"launch\", \"law\", \"lead\", \"leader\", \"leader\", \"leav\", \"leav\", \"liber\", \"life\", \"light\", \"link\", \"live\", \"local\", \"look\", \"lose\", \"make\", \"market\", \"mayor\", \"media\", \"meet\", \"meet\", \"melbourn\", \"million\", \"mine\", \"mine\", \"minist\", \"miss\", \"move\", \"murder\", \"name\", \"nation\", \"nation\", \"near\", \"need\", \"news\", \"north\", \"number\", \"offer\", \"offic\", \"open\", \"opposit\", \"order\", \"park\", \"parti\", \"peopl\", \"perth\", \"plan\", \"plan\", \"plane\", \"play\", \"plead\", \"polic\", \"polic\", \"polic\", \"port\", \"power\", \"premier\", \"presid\", \"price\", \"prison\", \"probe\", \"project\", \"promis\", \"protest\", \"public\", \"push\", \"queensland\", \"queensland\", \"question\", \"race\", \"rail\", \"rain\", \"rais\", \"ralli\", \"rat\", \"rate\", \"record\", \"region\", \"reject\", \"releas\", \"remain\", \"report\", \"rescu\", \"research\", \"resid\", \"return\", \"reveal\", \"review\", \"right\", \"rise\", \"risk\", \"river\", \"road\", \"rural\", \"sale\", \"save\", \"say\", \"say\", \"say\", \"school\", \"school\", \"search\", \"season\", \"second\", \"secur\", \"seek\", \"seek\", \"senat\", \"sentenc\", \"servic\", \"share\", \"shoot\", \"show\", \"sign\", \"south\", \"spark\", \"speak\", \"sport\", \"stab\", \"stand\", \"star\", \"start\", \"state\", \"station\", \"stay\", \"stop\", \"storm\", \"strike\", \"student\", \"studi\", \"studi\", \"support\", \"suspect\", \"sydney\", \"take\", \"talk\", \"target\", \"tasmania\", \"tasmanian\", \"teacher\", \"team\", \"teen\", \"tell\", \"test\", \"threat\", \"threaten\", \"time\", \"titl\", \"tourism\", \"trade\", \"train\", \"trial\", \"truck\", \"trump\", \"turn\", \"union\", \"unit\", \"urg\", \"victim\", \"victori\", \"victoria\", \"violenc\", \"vote\", \"want\", \"warn\", \"water\", \"weather\", \"week\", \"welcom\", \"west\", \"west\", \"win\", \"woman\", \"women\", \"wont\", \"work\", \"worker\", \"world\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 2, 7, 9, 4, 1, 5, 6, 3, 8]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2156423697090985925207548594\", ldavis_el2156423697090985925207548594_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2156423697090985925207548594\", ldavis_el2156423697090985925207548594_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2156423697090985925207548594\", ldavis_el2156423697090985925207548594_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=               x          y  topics  cluster       Freq\n",
       "topic                                                  \n",
       "9      32.420952   8.115230       1        1  10.700251\n",
       "1       0.648955  71.707100       2        1  10.447233\n",
       "6      71.019684   9.971006       3        1  10.405568\n",
       "8       3.785981  33.185135       4        1  10.243107\n",
       "3     -42.666500  -9.222070       5        1  10.110137\n",
       "0      -3.621689 -43.526516       6        1  10.016527\n",
       "4      -3.632402  -4.171708       7        1   9.859816\n",
       "5     -36.584400  35.804413       8        1   9.539459\n",
       "2      44.391380  50.615383       9        1   9.415396\n",
       "7      40.235809 -31.591665      10        1   9.262505, topic_info=            Term          Freq         Total Category  logprob  loglift\n",
       "9407       polic  31887.000000  31887.000000  Default  30.0000  30.0000\n",
       "2182       charg  16666.000000  16666.000000  Default  29.0000  29.0000\n",
       "711    australia  15942.000000  15942.000000  Default  28.0000  28.0000\n",
       "9321        plan  18642.000000  18642.000000  Default  27.0000  27.0000\n",
       "5089        govt  14154.000000  14154.000000  Default  26.0000  26.0000\n",
       "...          ...           ...           ...      ...      ...      ...\n",
       "13165    violenc   3099.814724   3100.710646  Topic10  -4.8881   2.3789\n",
       "12200    teacher   2875.398340   2876.294244  Topic10  -4.9633   2.3789\n",
       "4176      export   2859.057710   2859.953619  Topic10  -4.9690   2.3789\n",
       "2636      confid   2846.374211   2847.270113  Topic10  -4.9734   2.3789\n",
       "2071       cattl   2767.385157   2768.281066  Topic10  -5.0016   2.3789\n",
       "\n",
       "[359 rows x 6 columns], token_table=       Topic      Freq     Term\n",
       "term                           \n",
       "43        10  0.999680     abus\n",
       "69         8  0.748843    accus\n",
       "69        10  0.251004    accus\n",
       "89         5  0.999800   action\n",
       "109        3  0.999851  adelaid\n",
       "...      ...       ...      ...\n",
       "13604      2  0.999503     wont\n",
       "13634      5  0.999932     work\n",
       "13637      1  0.999924   worker\n",
       "13644      8  0.999916    world\n",
       "13730      2  0.999892     year\n",
       "\n",
       "[346 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[10, 2, 7, 9, 4, 1, 5, 6, 3, 8])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, info_train_vec, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83be02",
   "metadata": {},
   "source": [
    "Finalmente correspondería testear nuestro modelo, para ello utilizaremos la información ya prepocesada guardada en info_test. Aplicamos los mismos pasos que antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "545478fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dejamos todo como str:\n",
    "info_test=info_test.apply(str)\n",
    "\n",
    "#Aplicamos el CV\n",
    "#la clave acá está en aplicar el .transform() ya que contamos con nuestro vocabulario guardado en CountVectorizer()\n",
    "#y queremos crear la matrix-tokens de la información externa asociada a ese vocabulario.\n",
    "info_test_vec = vectorizer.transform(info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb14a44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299999</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.014287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502660</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.197340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245247</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245248</th>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245249</th>\n",
       "      <td>0.366666</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245250</th>\n",
       "      <td>0.014292</td>\n",
       "      <td>0.299994</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245251</th>\n",
       "      <td>0.219997</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245252 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.299999  0.014286  0.157143  0.014286  0.014286  0.014286  0.157143   \n",
       "1       0.502660  0.016667  0.016667  0.016667  0.183333  0.016667  0.016667   \n",
       "2       0.016667  0.016667  0.016667  0.016667  0.350000  0.016667  0.350000   \n",
       "3       0.350000  0.016667  0.016667  0.016667  0.016667  0.016667  0.350000   \n",
       "4       0.025000  0.025000  0.275000  0.275000  0.025000  0.025000  0.025000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "245247  0.016667  0.016667  0.183333  0.350000  0.016667  0.016667  0.016667   \n",
       "245248  0.157143  0.157143  0.014286  0.014286  0.300000  0.014286  0.014286   \n",
       "245249  0.366666  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n",
       "245250  0.014292  0.299994  0.014286  0.014286  0.014286  0.300000  0.014286   \n",
       "245251  0.219997  0.220000  0.020000  0.020000  0.020000  0.220000  0.020000   \n",
       "\n",
       "               7         8         9  \n",
       "0       0.014286  0.300000  0.014287  \n",
       "1       0.016667  0.016667  0.197340  \n",
       "2       0.016667  0.016667  0.183333  \n",
       "3       0.183333  0.016667  0.016667  \n",
       "4       0.025000  0.025000  0.275000  \n",
       "...          ...       ...       ...  \n",
       "245247  0.016667  0.016667  0.350000  \n",
       "245248  0.157143  0.157143  0.014286  \n",
       "245249  0.366667  0.033333  0.033333  \n",
       "245250  0.157143  0.014286  0.157143  \n",
       "245251  0.020000  0.220000  0.020003  \n",
       "\n",
       "[245252 rows x 10 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se lo pasamos a nuestro LDA\n",
    "doc_topic_test = lda.transform(info_test_vec)\n",
    "# making a dataframe from the document-topic matrix for test info\n",
    "doc_topic_df_test = pd.DataFrame(data=doc_topic_test)\n",
    "doc_topic_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b44dab",
   "metadata": {},
   "source": [
    "Debido a que train_test_split() nos hace la partición de manera aleatoria, corresponde hacer coincidir los índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5cb02e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos in dataset</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>519246</td>\n",
       "      <td>['billion', 'dollar', 'hole', 'liber', 'hospit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>534672</td>\n",
       "      <td>['plan', 'bauxit', 'bring', 'river', 'worri']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>755151</td>\n",
       "      <td>['hong', 'kong', 'social', 'worker', 'shan', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>831855</td>\n",
       "      <td>['teenag', 'girl', 'shoot', 'western', 'sydney']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581991</td>\n",
       "      <td>['unit', 'overpow', 'arsenal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245247</th>\n",
       "      <td>453023</td>\n",
       "      <td>['east', 'timor', 'prepar', 'possibl', 'cyclon']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245248</th>\n",
       "      <td>748199</td>\n",
       "      <td>['respons', 'cattl', 'diseas', 'win', 'industr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245249</th>\n",
       "      <td>663514</td>\n",
       "      <td>['observatori', 'gingin']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245250</th>\n",
       "      <td>984877</td>\n",
       "      <td>['giant', 'hors', 'defi', 'drought', 'north', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245251</th>\n",
       "      <td>933593</td>\n",
       "      <td>['senat', 'leyonhjelm', 'secur', 'plan']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245252 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pos in dataset                                      headline_text\n",
       "0               519246  ['billion', 'dollar', 'hole', 'liber', 'hospit...\n",
       "1               534672      ['plan', 'bauxit', 'bring', 'river', 'worri']\n",
       "2               755151  ['hong', 'kong', 'social', 'worker', 'shan', '...\n",
       "3               831855   ['teenag', 'girl', 'shoot', 'western', 'sydney']\n",
       "4               581991                     ['unit', 'overpow', 'arsenal']\n",
       "...                ...                                                ...\n",
       "245247          453023   ['east', 'timor', 'prepar', 'possibl', 'cyclon']\n",
       "245248          748199  ['respons', 'cattl', 'diseas', 'win', 'industr...\n",
       "245249          663514                          ['observatori', 'gingin']\n",
       "245250          984877  ['giant', 'hors', 'defi', 'drought', 'north', ...\n",
       "245251          933593           ['senat', 'leyonhjelm', 'secur', 'plan']\n",
       "\n",
       "[245252 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_test_a=pd.DataFrame(info_test)\n",
    "info_test_a.index.name='Pos in dataset'\n",
    "info_test_a.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168a396",
   "metadata": {},
   "source": [
    "Y ahora podemos ver cómo, por ejemplo, para la noticia número 534672:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c3da1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'planned bauxite mine brings river worries'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[documents['index'] == 534672].values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f4e3e",
   "metadata": {},
   "source": [
    "nos dice que su tópico principal es el tópico 0 con un score de 0.502660:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c30e20f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word 0       plan\n",
       "Word 1    council\n",
       "Word 2      water\n",
       "Word 3       call\n",
       "Word 4       fear\n",
       "Name: Topic 0, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_keywords.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d9a4a",
   "metadata": {},
   "source": [
    "Tiene sentido. Ya para terminar agregamos a nuestro dataset original la contribución de cada uno de sus tópicos arrojados por nuestro modelo LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b5be7737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['decid', 'communiti', 'broadcast', 'licenc']</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['wit', 'awar', 'defam']</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['call', 'infrastructur', 'protect', 'summit']</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['staff', 'aust', 'strike', 'rise']</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['strike', 'affect', 'australian', 'travel']</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226253</th>\n",
       "      <td>['reader', 'learn', 'look', 'year']</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226254</th>\n",
       "      <td>['south', 'african', 'variant', 'covid']</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226255</th>\n",
       "      <td>['victoria', 'coronavirus', 'restrict', 'mean'...</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226256</th>\n",
       "      <td>['what', 'life', 'like', 'american', 'doctor',...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226257</th>\n",
       "      <td>['women', 'shed', 'canberra', 'reskil', 'unemp...</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226258 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline_text         0  \\\n",
       "0            ['decid', 'communiti', 'broadcast', 'licenc']  0.020000   \n",
       "1                                 ['wit', 'awar', 'defam']  0.025000   \n",
       "2           ['call', 'infrastructur', 'protect', 'summit']  0.220000   \n",
       "3                      ['staff', 'aust', 'strike', 'rise']  0.020000   \n",
       "4             ['strike', 'affect', 'australian', 'travel']  0.220000   \n",
       "...                                                    ...       ...   \n",
       "1226253                ['reader', 'learn', 'look', 'year']  0.020000   \n",
       "1226254           ['south', 'african', 'variant', 'covid']  0.025000   \n",
       "1226255  ['victoria', 'coronavirus', 'restrict', 'mean'...  0.516667   \n",
       "1226256  ['what', 'life', 'like', 'american', 'doctor',...  0.014286   \n",
       "1226257  ['women', 'shed', 'canberra', 'reskil', 'unemp...  0.016667   \n",
       "\n",
       "                1         2         3         4         5         6         7  \\\n",
       "0        0.020000  0.020000  0.220000  0.220000  0.220000  0.020000  0.020000   \n",
       "1        0.025000  0.275000  0.275000  0.275000  0.025000  0.025000  0.025000   \n",
       "2        0.020000  0.020000  0.220000  0.020000  0.020000  0.220000  0.020000   \n",
       "3        0.220000  0.020000  0.220000  0.020000  0.020000  0.220000  0.020000   \n",
       "4        0.020000  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1226253  0.220000  0.020000  0.020000  0.020000  0.020000  0.220000  0.020000   \n",
       "1226254  0.525000  0.025000  0.025000  0.025000  0.275000  0.025000  0.025000   \n",
       "1226255  0.183333  0.016667  0.016667  0.016667  0.183333  0.016667  0.016667   \n",
       "1226256  0.157143  0.014286  0.157143  0.014286  0.300000  0.157143  0.014286   \n",
       "1226257  0.350000  0.183333  0.016667  0.016667  0.183333  0.016667  0.016667   \n",
       "\n",
       "                8         9  \n",
       "0        0.220000  0.020000  \n",
       "1        0.025000  0.025000  \n",
       "2        0.220000  0.020000  \n",
       "3        0.020000  0.220000  \n",
       "4        0.220000  0.420000  \n",
       "...           ...       ...  \n",
       "1226253  0.420000  0.020000  \n",
       "1226254  0.025000  0.025000  \n",
       "1226255  0.016667  0.016667  \n",
       "1226256  0.014286  0.157143  \n",
       "1226257  0.016667  0.183333  \n",
       "\n",
       "[1226258 rows x 11 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dejamos todo como str:\n",
    "processed_docs=processed_docs.apply(str)\n",
    "\n",
    "#Aplicamos el CV\n",
    "processed_docs_vec = vectorizer.transform(processed_docs)\n",
    "\n",
    "#se lo pasamos a nuestro LDA\n",
    "doc_topic_dataset = lda.transform(processed_docs_vec)\n",
    "\n",
    "# making a dataframe from the document-topic matrix for test info\n",
    "doc_topic_df_dataset = pd.DataFrame(data=doc_topic_dataset)\n",
    "\n",
    "#concat\n",
    "final_data=pd.concat([processed_docs, doc_topic_df_dataset], axis=1,)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c0a99",
   "metadata": {},
   "source": [
    "Como comentario, el número óptimo de tópicos no tiene una forma explícita de calcularse, por lo que se recomienda probar con distintos k hasta obtener los mejores resultados. El siguiente esquema muestra potecialmente la construcción del modelo LDA más óptimo:\n",
    "\n",
    "![Figura 3: esquema solución LDA (Hammoe,L.,2018)](gráficoLDA.png) \n",
    "\n",
    "#### ¿Por qué funciona el método? ####\n",
    "\n",
    "Para entender un poco mejor el fundamento de cómo esta técnica logra agrupar grupos de palabras en un tópico contamos con \"El principio de la caja de Dirichlet\", éste establece que si contamos con $n$ elementos para distribuir en $m$ lugares distintos (con $n > m $) existirá al menos un lugar de $m$ con más de un elemento, con esto podemos entender intuitivamente porqué si tenemos un texto con cinco palabras y trabajamos con cuatro tópicos, el algoritmo devolverá al menos un grupo de palabras con dos palabras pertenecientes de un mismo tipo.\n",
    "\n",
    "Además, un hecho muy simple pero no por eso menos importante que revela el LDA es que las palabras agrupadas en un mismo tópico *coocurren* y no así las palabras pertencientes a tópicos distintos.\n",
    "\n",
    "Una de las mayores ventajas de este modelo es que asume intuitivamente que una palabra pertenece a un tópico y que cada documento pertenece al menos a un tópico (como lo comentamos al inicio del texto). Durante éste proceso intuitivo lo que algoritmo va a decidir es a dónde irá a parar cada palabra teniendo en cuenta lo siguiente (Chandía, B., 2016):\n",
    "- Una palabra pertenece a un tópico, por lo que, en estricto rigor, si en el corpus se tiene un diccionario de mil palabras; potencialmente, existirán mil tópicos\n",
    "- Un documento pertenece a un tópico, por lo que, en estricto rigor, las palabras de x documento, pertenecen al tópico de dicho documento. \n",
    "\n",
    "Por ejemplo, de acuerdo a estas reglas, si contamos con un corpus de 5 documentos con un vocabulario total de 500 palabras estaremos trabajando potencialmente con un corpus con 500 tópicos y a su vez se tiene que en el corpus hay 5 tópicos (equivalente a la cantidad de documentos) y en estricto rigor cada documento habla de un tópico.\n",
    "\n",
    "Pero esta intuición estricta intuitivamente (valga la redundancia) no siempre será cierta (basta con mirar el ejemplo anterior)\n",
    "y es por esto que el LDA contempla la posibilidad de que un documento pueda estar hablando de más de un tema a través de la proporción $\\theta_{d}$ fijada con el parámetro Dirichlet $\\alpha$. De esta manera, este factor va a establecer en cómo va a influir la proporción de tópicos $\\theta_{d}$ de cada documento en la asignación de tópicos de cada palabra (Chandía B., 2016).\n",
    "\n",
    "#### Biliografía ####\n",
    "- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n",
    "\n",
    "- Chandía Sepúlveda, B. (2016). Aplicación y evaluacion LDA para asignaciòn de tópicos en datos de Twitter.\n",
    "\n",
    "- Hammoe, L. (2018). Detección de tópicos: utilizando el modelo LDA.\n",
    "\n",
    "- Nicolai Manaut, F. I. (2019). Sistema de análisis de tópicos para interacciones cliente-call center.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
