{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd3b9a6",
   "metadata": {},
   "source": [
    "## Modelo de t√≥picos: Latent Dirichlet Allocation (LDA) ##\n",
    "\n",
    "*Topic modeling*, en t√©rminos muy generales, consiste en una t√©cnica para identificar t√≥picos o temas en textos a trav√©s de la detecci√≥n de patrones en una colecci√≥n de documentos llamado \"corpus\" y agrupando estas palabras en t√≥picos. A su vez, podemos definir *Topic modeling generativo* como un modelo que clasifica documentos a√∫n no procesados una vez que el modelo ya ha sido entrenado por el corpus incompleto, caracter√≠stica crucial a la hora de trabajar con un corpus \"infinito\".\n",
    "\n",
    "Uno de las t√©cnicas de \"Topic modeling generativo\" que ha resonado este √∫ltimo tiempo ha sido LDA. La esencia de este modelo consiste en decir que, para un grupo de texto (corpus), cada uno de ellos puede ser modelado por una distribuci√≥n de t√≥picos y cada uno de estos t√≥picos puede ser modelado por una distribuci√≥n de palabras. Tanto los t√≥picos como sus respectivas distribuciones (distribuci√≥n de Dirichlet) son variables latentes del total de datos (Nicolai, F.,2019).\n",
    "\n",
    "En otras palabras, este modelo permite que un conjunto de observaciones puedan ser explicados por grupos inadvertidos que describen por qu√© algunas partes de los datos son similares, por ejemplo, si las observaciones son palabras en documentos, cada uno de estos documentos es una mezcla de categor√≠as (t√≥picos) y la aparici√≥n de cada palabra en un documento se debe a una de las categor√≠as a las que el documento pertenece. As√≠, la intuici√≥n detr√°s de LDA es que cada documento siempre exhibe m√∫ltiples temas en su cuerpo. Por ende, LDA es un modelo estad√≠stico para colecci√≥n de documentos que intenta capturar esta intuici√≥n (Chand√≠a, B., 2016).\n",
    "\n",
    "Veamos la siguiente figura. Se muestra un documento donde palabras que coocurren son agrupadas y categorizadas en un t√≥pico, la etiqueta de √©ste la ponemos nosotros, por ejemplo \"gene\", \"genomes, \"dna\" la podemos categorizar dentro de palabras que hablan de gen√©tica, luego \"organism\", \"evolve\", \"survive\" dentro de palabras que hablan de biolog√≠a evolutiva y por √∫ltimo \"data\", \"computer\" y \"computational\" dentro de an√°lisis de datos. Si continuamos con este procedimiento daremos con que todo el texto se desenvuelve dentro de estos tres t√≥picos . En LDA se define un tema para pasar a ser una distribuci√≥n sobre un diccionario de palabras ya fijado, por ejemplo, el tema \"gen√©tica\" tiene un vocabulario de palabras que poseen una alta probabilidad de pertenecer al tema \"gen√©tica\"(Chand√≠a, B., 2016).\n",
    "\n",
    "![Figura 1: Intuci√≥n detr√°s de LDA (Blei, David., 2012)](LDAintuicion.png)\n",
    "\n",
    "#### La matem√°tica tras LDA ####\n",
    "\n",
    "Entendamos un poco la matem√°tica tras el m√©todo sin entrar tanto en tecnisismo ni operaciones matem√°ticas innecesarias que solo complicar√°n el entendimiento del m√©todo. Primero, definamos un vocabulario (creado por los autores del m√©todo):\n",
    "\n",
    "- Documento (D): Observaci√≥n o muestra de car√°cter textual.\n",
    "- Corpus (C): Colecci√≥n de todos los documentos a trabajar.\n",
    "- Vocabulario (V): Todas las palabras √∫nicas encontradas en el corpus posterior al procesamiento.\n",
    "- Matriz t√©rmino documento: Matriz cuyas filas son documentos y cuyas columnas son cada palabra del documento.\n",
    "\n",
    "Con *k-t√≥picos*,  $B_{(1:k)}$ son distribuciones de Dirichlet($\\eta$) de probabilidad sobre un vocabulario fijo. El modelo asume que cada documento D perteneciente al corpus C es generado por el siguiente proceso generativo:\n",
    "\n",
    "- Escoger mezcla de t√≥picos $\\theta^{d} $ de una distribuci√≥n sobre un (K) simplex tal como una Dirichlet ($\\alpha$)\n",
    "- Cada una de las palabras del documento se genera escogiendo una asignaci√≥n de t√≥pico $z$ desde una distribuci√≥n Multinomial( $\\theta^{d} $) y posteriormente una palabra $w$ desde una Multinomial($\\beta_{z}$)\n",
    "\n",
    "Graficamente este modelo lo podemos representar a trav√©s de la siguiente estructura donde cada t√©rmino significa:\n",
    "- K: n√∫mero de t√≥picos\n",
    "- D: n√∫mero de documentos\n",
    "- N: cantidad de palabras en el documento d $\\in$ D\n",
    "- $\\alpha$: vector positivo de par√°metros $\\alpha$ de dimensi√≥n K\n",
    "- $B_{(1:K)}$ representa los t√≥picos K de la estructura de t√≥picos ocultos\n",
    "- $\\theta_{(1:D)}$ representa la proporci√≥n de t√≥picos por documentos $d \\in D$\n",
    "- $W_{(d,n)}$ representa las variables observadas dada la asignaci√≥n del documento $d \\in D$\n",
    "- $Z_{(1:D,1:N)}$ representa la asignaci√≥n del t√≥pico oculto $\\beta_{(k')}$ a la palabra $W_{(d,n)}$\n",
    "\n",
    "![Figura 1: Esquema funcionamiento LDA](esquemaLDA.png)\n",
    "\n",
    "As√≠, la distribuci√≥n conjunta de una mezcla de t√≥picos  ùúÉ  junto a un conjunto de  ùëÅ  t√≥picos  ùëß  y un set de  ùëÅ  palabras  ùë§  se define por:\n",
    "\n",
    "$$ p( \\theta, z, w \\vert \\alpha, \\beta)= p(\\theta, \\alpha)\\prod_{n=1}^{N}p(z_{n} \\vert \\alpha)p(w_{n} \\vert z_{n}, \\beta)$$\n",
    "\n",
    "Ahora bien, a trav√©s de operaciones matem√°ticas que no incluiremos llegamos al computo de la distribuci√≥n posterior de las variables ocultas dado un documento (as√≠ funciona la inferencia dentro de LDA):\n",
    "\n",
    "$$ p(\\theta, z \\vert w, \\alpha, \\beta)= \\dfrac{p(\\theta, z, w \\vert \\alpha , \\beta)}{p(w \\vert \\alpha, \\beta)} $$\n",
    "\n",
    "Como comentario, √©sta ecuaci√≥n en la forma que est√° expresada no se puede computar por lo que se proponen m√©todos alternativos de resoluci√≥n como la inferencia varacional.\n",
    "\n",
    "#### LDA como modelo: ####\n",
    "\n",
    "La aplicaci√≥n del algoritmo LDA la podemos describir a trav√©s de las siguientes etapas:\n",
    "- Colecci√≥n de la DATA (futuro corpus)\n",
    "- Preprocesamiento de los datos\n",
    "- Implementaci√≥n del modelo: entrenamiento y testeo.\n",
    "- Visualizaci√≥n\n",
    "\n",
    "Veamos un ejemplo para entender mejor la metodolog√≠a tras LDA:\n",
    "\n",
    "1) **Colecci√≥n de la data:**\n",
    "\n",
    "Trabajaremos con un Dataset en ingl√©s de m√°s de un millon de titulares publicados por ABC (Australian Broadcasting Corporation) en un periodo de 15 a√±os. Para ello utilizaremos la librer√≠a **pandas** para importar la data y crear el dataframe sobre el cual vamos a trabajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c99fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Importamos la DATAset.\n",
    "data = pd.read_csv('abcnews-date-text.csv',error_bad_lines=None);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02e32f",
   "metadata": {},
   "source": [
    "N√∫mero de documentos pertenecientes a nuestro corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eeeeda71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226258"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d49ad",
   "metadata": {},
   "source": [
    "Visualizamos la informaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5833367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ambitious olsson wins triple jump</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>antic delighted with record breaking barca</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aussie qualifier stosur wastes four memphis match</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aust addresses un security council over iraq</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>australia is locked into war timetable opp</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>australia to contribute 10 million in aid to iraq</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>barca take record as robson celebrates birthda...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bathhouse plans move ahead</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        headline_text  index\n",
       "0   aba decides against community broadcasting lic...      0\n",
       "1      act fire witnesses must be aware of defamation      1\n",
       "2      a g calls for infrastructure protection summit      2\n",
       "3            air nz staff in aust strike for pay rise      3\n",
       "4       air nz strike to affect australian travellers      4\n",
       "5                   ambitious olsson wins triple jump      5\n",
       "6          antic delighted with record breaking barca      6\n",
       "7   aussie qualifier stosur wastes four memphis match      7\n",
       "8        aust addresses un security council over iraq      8\n",
       "9          australia is locked into war timetable opp      9\n",
       "10  australia to contribute 10 million in aid to iraq     10\n",
       "11  barca take record as robson celebrates birthda...     11\n",
       "12                         bathhouse plans move ahead     12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f3cdb",
   "metadata": {},
   "source": [
    "Perfecto, ya tenemos lista la informaci√≥n con la cual vamos a trabajar. Ahora es de vital importancia eliminar y reducir al m√°ximo los datos que no nos aportar√°n nada para el an√°lisis, para ello vamos el siguiente paso:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2841f3",
   "metadata": {},
   "source": [
    "2) **Limpieza de informaci√≥n**\n",
    "\n",
    "Para efectos del preprocesamiento la idea es limpiar todo lo que no sirva en el corpus. Esto lo haremos a trav√©s de:\n",
    "- **Creaci√≥n de tokens:** Reducimos el texto a oraciones y las oraciones a palabras, dejamos todo en min√∫scula, removemos puntuaci√≥n, signos y n√∫meros. \n",
    "- **Eliminaci√≥n de las stopwords:** importamos un listado de palabras que no aportan en el an√°lisis y las eliminamos de nuestro corpus\n",
    "- **Lemmatizaci√≥n:** palabras en tercera persona pasan a primera persona y dejamos los verbos en tiempo presente\n",
    "- **Stemmizaci√≥n:** las palabras se reducen a su raiz.\n",
    "\n",
    "Tanto gensim como nltk nos entregan las herramientas necesarias para realizar este proceso:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e6a03",
   "metadata": {},
   "source": [
    "**What is Gensim?**\n",
    "\n",
    "\n",
    "Gensim = ‚ÄúGenerate Similar‚Äù is a popular open source natural language processing (NLP) library used for unsupervised topic modeling. It uses top academic models and modern statistical machine learning to perform various complex tasks such as:\n",
    "\n",
    "- Building document or word vectors\n",
    "- Corpora\n",
    "- Performing topic identification\n",
    "- Performing document comparison (retrieving semantically similar documents)\n",
    "- Analysing plain-text documents for semantic structure\n",
    "\n",
    "M√°s info: [Gensim](https://radimrehurek.com/gensim/intro.html)\n",
    "\n",
    "**What is Natural Language Toolkit (NLTK)?**\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "Info provided by (website official): [NLTK](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b71dd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importamos las librer√≠as correspondientes para la limpieza\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "175205c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Importamos listado de palabras en ingl√©s para eliminar en el corpus:\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc849696",
   "metadata": {},
   "source": [
    "Visualizamos un ejemplo de c√≥mo quedar√≠an las palabras post-stemmizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5bef1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978642ca",
   "metadata": {},
   "source": [
    "Creamos la funci√≥n para lemmatizar y stemmizar nuestros textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1d136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lemmatizamos/stemmizamos\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "##Stopwords\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a460665",
   "metadata": {},
   "source": [
    "Visualizamos un documento aleatorio original y lo comparamos con uno procesado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a33ac4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f57fd1",
   "metadata": {},
   "source": [
    "Podemos notar el cambio para efectos de ratepayers -> ratepay, voting -> vote, etc. Corresponde ahora implementarlos para todo el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f69940d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundos:  153.07344889640808\n"
     ]
    }
   ],
   "source": [
    "##Agregamos un contador\n",
    "import time \n",
    "t = time.time()\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "t = time.time()-t\n",
    "print(\"segundos: \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc6bc1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [decid, communiti, broadcast, licenc]\n",
       "1                                 [wit, awar, defam]\n",
       "2             [call, infrastructur, protect, summit]\n",
       "3                        [staff, aust, strike, rise]\n",
       "4               [strike, affect, australian, travel]\n",
       "5                 [ambiti, olsson, win, tripl, jump]\n",
       "6             [antic, delight, record, break, barca]\n",
       "7      [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8              [aust, address, secur, council, iraq]\n",
       "9                           [australia, lock, timet]\n",
       "10             [australia, contribut, million, iraq]\n",
       "11         [barca, record, robson, celebr, birthday]\n",
       "12                           [bathhous, plan, ahead]\n",
       "13             [hop, launceston, cycl, championship]\n",
       "14               [plan, boost, paroo, water, suppli]\n",
       "15               [blizzard, buri, unit, state, bill]\n",
       "16         [brigadi, dismiss, report, troop, harass]\n",
       "17    [british, combat, troop, arriv, daili, kuwait]\n",
       "18             [bryant, lead, laker, doubl, overtim]\n",
       "19                [bushfir, victim, urg, centrelink]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Visualizamos los textos procesados\n",
    "processed_docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b8aeb",
   "metadata": {},
   "source": [
    "Antes de seguir hablemos un poco [Scikit-learn](https://scikit-learn.org/stable/index.html). Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.([Source](https://www.tutorialspoint.com/scikit_learn/scikit_learn_introduction.htm))\n",
    "\n",
    "Una de tantas herramientas √∫tiles de Sklearn es el [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). El CountVectorizer proporciona una manera simple de tokenizar una colecci√≥n de documentos de texto y construir un vocabulario de palabras conocidas. Los pasos a seguir son:\n",
    "\n",
    "- Cree una instancia de la clase CountVectorizer.\n",
    "- Llame a la funci√≥n fit() para aprender un vocabulario de uno o m√°s documentos.\n",
    "- Llame a la funci√≥n transform() en uno o m√°s documentos seg√∫n sea necesario para codificar cada uno como un vector.\n",
    "\n",
    "Se devuelve un vector codificado con una longitud de todo el vocabulario y un n√∫mero entero para el n√∫mero de veces que cada palabra apareci√≥ en el documento. Veamos un ejemplo:\n",
    "\n",
    "NOTA: Tambi√©n hubiesemos podido hacer limpieza de informaci√≥n a trav√©s del CountVectorizer() sin ning√∫n problema a trav√©s de sus diversos par√°metros (ver website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97cac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Informaci√≥n que queremos tokenizar\n",
    "corpus = [\n",
    "    'Carlos is a nice guy.',\n",
    "    'but Pedro is no a nice guy',\n",
    "    'Carlos and Pedro are not friends',\n",
    "    'Is this the last sentence?',]\n",
    "#Creaci√≥n de la clase CountVectorizer()\n",
    "v = CountVectorizer()\n",
    "#Creamos el diccionario y codificamos como vector\n",
    "X = v.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7191da0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'but', 'carlos', 'friends', 'guy', 'is', 'last',\n",
       "       'nice', 'no', 'not', 'pedro', 'sentence', 'the', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos el vocabulario\n",
    "v.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64b9d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 1 0 1 1 0 1 0 0 0]\n",
      " [1 1 0 1 1 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#veamos el vector\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafa568",
   "metadata": {},
   "source": [
    "Ahora que ya sabemos c√≥mo funciona CountVectorizer(), lo implementaremos en nuestro corpus para convertir nuestros documents  a una matriz de \"token counts\". Para ello utilizaremos [\"train_test_split\"](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de SKlearn que nos permitir√° separar la informaci√≥n en entrenamiento y testeo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a89bde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "info_train, info_test = train_test_split(processed_docs,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e59ffeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905827     ['central', 'west', 'council', 'pilot', 'organ']\n",
       "487792                     ['seiz', 'dog', 'take', 'rspca']\n",
       "888351        ['woman', 'court', 'cannabi', 'plant', 'gun']\n",
       "972375    ['drug', 'arrest', 'adelaid', 'night', 'club',...\n",
       "488550      ['treasur', 'pitch', 'pulp', 'propos', 'europ']\n",
       "                                ...                        \n",
       "110268    ['broadford', 'footbal', 'club', 'warn', 'stop...\n",
       "259178       ['south', 'promot', 'taylor', 'head', 'coach']\n",
       "131932      ['jackson', 'give', 'sampl', 'polic', 'report']\n",
       "671155              ['capello', 'quit', 'england', 'manag']\n",
       "121958              ['worker', 'admit', 'skim', 'thousand']\n",
       "Name: headline_text, Length: 981006, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostramos la data para entrenamiento:\n",
    "info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8f19a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Clase del CountVectorizer()\n",
    "vectorizer = CountVectorizer(min_df= 15 ,lowercase=False)\n",
    "\n",
    "#Dejamos todo como str (la info contiene n√∫meros)\n",
    "info_train=info_train.apply(str)\n",
    "\n",
    "#Aplicamos el CV\n",
    "info_train_vec = vectorizer.fit_transform(info_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "70fff61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'central': 2118,\n",
       " 'west': 13422,\n",
       " 'council': 2825,\n",
       " 'pilot': 9275,\n",
       " 'organ': 8764,\n",
       " 'seiz': 10914,\n",
       " 'dog': 3573,\n",
       " 'take': 12110,\n",
       " 'rspca': 10539,\n",
       " 'woman': 13598,\n",
       " 'court': 2842,\n",
       " 'cannabi': 1919,\n",
       " 'plant': 9326,\n",
       " 'gun': 5271,\n",
       " 'drug': 3698,\n",
       " 'arrest': 563,\n",
       " 'adelaid': 109,\n",
       " 'night': 8454,\n",
       " 'club': 2429,\n",
       " 'strip': 11826,\n",
       " 'treasur': 12612,\n",
       " 'pitch': 9308,\n",
       " 'pulp': 9708,\n",
       " 'propos': 9657,\n",
       " 'europ': 4090,\n",
       " 'plan': 9321,\n",
       " 'law': 6879,\n",
       " 'blame': 1269,\n",
       " 'stifl': 11729,\n",
       " 'develop': 3368,\n",
       " 'daryl': 3111,\n",
       " 'hannah': 5390,\n",
       " 'make': 7353,\n",
       " 'splash': 11551,\n",
       " 'week': 13387,\n",
       " 'mother': 8117,\n",
       " 'heartach': 5518,\n",
       " 'homeless': 5746,\n",
       " 'women': 13600,\n",
       " 'fear': 4302,\n",
       " 'lose': 7187,\n",
       " 'kid': 6601,\n",
       " 'firefight': 4430,\n",
       " 'cancer': 1906,\n",
       " 'rate': 9918,\n",
       " 'averag': 737,\n",
       " 'kennett': 6566,\n",
       " 'swim': 12045,\n",
       " 'champ': 2154,\n",
       " 'swimmer': 12046,\n",
       " 'break': 1527,\n",
       " 'world': 13644,\n",
       " 'record': 10009,\n",
       " 'hobart': 5694,\n",
       " 'assault': 621,\n",
       " 'derryn': 3322,\n",
       " 'hinch': 5659,\n",
       " 'sentenc': 10942,\n",
       " 'breach': 1525,\n",
       " 'order': 8759,\n",
       " 'back': 771,\n",
       " 'waltz': 13278,\n",
       " 'matilda': 7546,\n",
       " 'dastyari': 3114,\n",
       " 'group': 5226,\n",
       " 'harass': 5402,\n",
       " 'face': 4209,\n",
       " 'legal': 6936,\n",
       " 'action': 89,\n",
       " 'queensland': 9783,\n",
       " 'mick': 7816,\n",
       " 'deni': 3280,\n",
       " 'pork': 9458,\n",
       " 'barrel': 936,\n",
       " 'sport': 11565,\n",
       " 'rort': 10483,\n",
       " 'urgent': 12986,\n",
       " 'upgrad': 12966,\n",
       " 'canberra': 1902,\n",
       " 'convent': 2724,\n",
       " 'facil': 4213,\n",
       " 'defend': 3209,\n",
       " 'timet': 12417,\n",
       " 'beat': 1035,\n",
       " 'econom': 3822,\n",
       " 'turnaround': 12747,\n",
       " 'govt': 5089,\n",
       " 'hick': 5624,\n",
       " 'plea': 9350,\n",
       " 'bargain': 915,\n",
       " 'strike': 11823,\n",
       " 'forestri': 4595,\n",
       " 'bookkeep': 1398,\n",
       " 'accus': 69,\n",
       " 'steal': 11679,\n",
       " 'destroy': 3348,\n",
       " 'nambour': 8286,\n",
       " 'home': 5740,\n",
       " 'slipper': 11291,\n",
       " 'name': 8288,\n",
       " 'red': 10017,\n",
       " 'best': 1149,\n",
       " 'croft': 2936,\n",
       " 'claim': 2346,\n",
       " 'lade': 6772,\n",
       " 'report': 10186,\n",
       " 'dead': 3141,\n",
       " 'cystic': 3043,\n",
       " 'fibrosi': 4367,\n",
       " 'suffer': 11901,\n",
       " 'carri': 2005,\n",
       " 'differ': 3411,\n",
       " 'lyon': 7260,\n",
       " 'famili': 4251,\n",
       " 'boswel': 1434,\n",
       " 'silver': 11183,\n",
       " 'chemo': 2225,\n",
       " 'patient': 9056,\n",
       " 'say': 10734,\n",
       " 'tell': 12230,\n",
       " 'speak': 11501,\n",
       " 'media': 7685,\n",
       " 'bali': 837,\n",
       " 'deadlock': 3144,\n",
       " 'climat': 2399,\n",
       " 'draft': 3650,\n",
       " 'meatwork': 7676,\n",
       " 'warn': 13302,\n",
       " 'pond': 9435,\n",
       " 'pong': 9437,\n",
       " 'jayasuriya': 6309,\n",
       " 'year': 13730,\n",
       " 'cricket': 2912,\n",
       " 'anti': 435,\n",
       " 'corrupt': 2804,\n",
       " 'code': 2469,\n",
       " 'polic': 9407,\n",
       " 'prais': 9520,\n",
       " 'hero': 5602,\n",
       " 'help': 5566,\n",
       " 'heart': 5517,\n",
       " 'attack': 669,\n",
       " 'earli': 3788,\n",
       " 'topless': 12496,\n",
       " 'trade': 12556,\n",
       " 'hockey': 5700,\n",
       " 'argu': 533,\n",
       " 'case': 2025,\n",
       " 'incom': 6019,\n",
       " 'cut': 3023,\n",
       " 'buddi': 1677,\n",
       " 'blow': 1312,\n",
       " 'hawk': 5474,\n",
       " 'french': 4686,\n",
       " 'journalist': 6419,\n",
       " 'hand': 5369,\n",
       " 'jail': 6276,\n",
       " 'term': 12262,\n",
       " 'papua': 8965,\n",
       " 'communiti': 2570,\n",
       " 'farewel': 4266,\n",
       " 'teach': 12199,\n",
       " 'pioneer': 9291,\n",
       " 'mcmeniman': 7652,\n",
       " 'sign': 11167,\n",
       " 'forc': 4582,\n",
       " 'bodi': 1344,\n",
       " 'ukrainian': 12791,\n",
       " 'shipwreck': 11084,\n",
       " 'power': 9509,\n",
       " 'bill': 1194,\n",
       " 'rise': 10354,\n",
       " 'indonesian': 6048,\n",
       " 'rescu': 10200,\n",
       " 'effort': 3849,\n",
       " 'slow': 11303,\n",
       " 'urg': 12984,\n",
       " 'appli': 479,\n",
       " 'pipelin': 9294,\n",
       " 'fund': 4738,\n",
       " 'xenophon': 13693,\n",
       " 'support': 11971,\n",
       " 'youth': 13761,\n",
       " 'allow': 300,\n",
       " 'roll': 10447,\n",
       " 'googl': 5062,\n",
       " 'languag': 6822,\n",
       " 'superior': 11958,\n",
       " 'complex': 2588,\n",
       " 'rudd': 10549,\n",
       " 'meet': 7704,\n",
       " 'opposit': 8737,\n",
       " 'leader': 6900,\n",
       " 'disabl': 3449,\n",
       " 'advoc': 136,\n",
       " 'call': 1854,\n",
       " 'reform': 10057,\n",
       " 'photo': 9235,\n",
       " 'exhibit': 4145,\n",
       " 'celebr': 2100,\n",
       " 'karen': 6500,\n",
       " 'refuge': 10062,\n",
       " 'contribut': 2719,\n",
       " 'tasmanian': 12174,\n",
       " 'fossil': 4626,\n",
       " 'answer': 428,\n",
       " 'evolutionari': 4118,\n",
       " 'teenag': 12217,\n",
       " 'killer': 6614,\n",
       " 'bulli': 1710,\n",
       " 'school': 10781,\n",
       " 'indi': 6034,\n",
       " 'troubl': 12675,\n",
       " 'posit': 9477,\n",
       " 'start': 11657,\n",
       " 'mobil': 7963,\n",
       " 'speed': 11517,\n",
       " 'camera': 1874,\n",
       " 'patrol': 9061,\n",
       " 'zone': 13798,\n",
       " 'grammar': 5111,\n",
       " 'teacher': 12200,\n",
       " 'abus': 43,\n",
       " 'alleg': 288,\n",
       " 'molong': 8001,\n",
       " 'flood': 4512,\n",
       " 'demolit': 3271,\n",
       " 'expect': 4157,\n",
       " 'strong': 11833,\n",
       " 'earthquak': 3795,\n",
       " 'eastern': 3803,\n",
       " 'indonesia': 6047,\n",
       " 'critic': 2927,\n",
       " 'netbal': 8389,\n",
       " 'pathway': 9054,\n",
       " 'block': 1298,\n",
       " 'money': 8018,\n",
       " 'coronavirus': 2794,\n",
       " 'maker': 7355,\n",
       " 'botul': 1446,\n",
       " 'link': 7080,\n",
       " 'courtney': 2844,\n",
       " 'barnett': 926,\n",
       " 'tame': 12131,\n",
       " 'australian': 712,\n",
       " 'grammi': 5112,\n",
       " 'nomin': 8505,\n",
       " 'shoot': 11099,\n",
       " 'appeal': 471,\n",
       " 'tale': 12115,\n",
       " 'wed': 13382,\n",
       " 'ring': 10339,\n",
       " 'symbol': 12062,\n",
       " 'jewelleri': 6354,\n",
       " 'nadal': 8267,\n",
       " 'storm': 11773,\n",
       " 'barcelona': 909,\n",
       " 'scienc': 10798,\n",
       " 'music': 8230,\n",
       " 'romanc': 10455,\n",
       " 'zimbabw': 13791,\n",
       " 'withhold': 13577,\n",
       " 'food': 4560,\n",
       " 'tabl': 12084,\n",
       " 'obama': 8609,\n",
       " 'arsenal': 569,\n",
       " 'ahead': 187,\n",
       " 'leed': 6931,\n",
       " 'militari': 7860,\n",
       " 'syria': 12078,\n",
       " 'rule': 10558,\n",
       " 'noroc': 8526,\n",
       " 'referendum': 10050,\n",
       " 'wait': 13243,\n",
       " 'time': 12412,\n",
       " 'psychiatrist': 9689,\n",
       " 'criticis': 2928,\n",
       " 'releas': 10118,\n",
       " 'bronco': 1628,\n",
       " 'berrigan': 1142,\n",
       " 'meyer': 7808,\n",
       " 'eel': 3845,\n",
       " 'clash': 2361,\n",
       " 'schapell': 10765,\n",
       " 'corbi': 2774,\n",
       " 'shorten': 11112,\n",
       " 'australia': 711,\n",
       " 'parol': 9008,\n",
       " 'investig': 6192,\n",
       " 'shepparton': 11066,\n",
       " 'busi': 1784,\n",
       " 'death': 3153,\n",
       " 'identifi': 5940,\n",
       " 'involv': 6199,\n",
       " 'fatal': 4288,\n",
       " 'melbourn': 7715,\n",
       " 'clean': 2374,\n",
       " 'energi': 3967,\n",
       " 'target': 12162,\n",
       " 'emiss': 3930,\n",
       " 'save': 10725,\n",
       " 'burka': 1750,\n",
       " 'segreg': 10909,\n",
       " 'hewitt': 5613,\n",
       " 'glad': 4981,\n",
       " 'rebound': 9977,\n",
       " 'malaysia': 7362,\n",
       " 'deport': 3303,\n",
       " 'karoonda': 6506,\n",
       " 'east': 3800,\n",
       " 'murray': 8219,\n",
       " 'reveal': 10266,\n",
       " 'johnson': 6390,\n",
       " 'miss': 7937,\n",
       " 'second': 10888,\n",
       " 'demand': 3259,\n",
       " 'level': 6997,\n",
       " 'uni': 12887,\n",
       " 'rural': 10571,\n",
       " 'health': 5509,\n",
       " 'confer': 2634,\n",
       " 'afghan': 149,\n",
       " 'capit': 1941,\n",
       " 'kill': 6613,\n",
       " 'volunt': 13212,\n",
       " 'career': 1970,\n",
       " 'program': 9632,\n",
       " 'launch': 6863,\n",
       " 'combat': 2534,\n",
       " 'unfair': 12873,\n",
       " 'retail': 10240,\n",
       " 'practic': 9516,\n",
       " 'reunit': 10261,\n",
       " 'think': 12328,\n",
       " 'rat': 9917,\n",
       " 'import': 5992,\n",
       " 'poll': 9417,\n",
       " 'nelson': 8369,\n",
       " 'scalper': 10742,\n",
       " 'olymp': 8689,\n",
       " 'ticket': 12384,\n",
       " 'light': 7044,\n",
       " 'debut': 3165,\n",
       " 'outback': 8798,\n",
       " 'documentari': 3566,\n",
       " 'qaeda': 9752,\n",
       " 'yemen': 13736,\n",
       " 'interview': 6176,\n",
       " 'jam': 6280,\n",
       " 'hird': 5676,\n",
       " 'liber': 7015,\n",
       " 'threat': 12350,\n",
       " 'bail': 815,\n",
       " 'renew': 10160,\n",
       " 'kalgoorli': 6478,\n",
       " 'kidnapp': 6605,\n",
       " 'tear': 12204,\n",
       " 'apart': 451,\n",
       " 'guilt': 5257,\n",
       " 'diagnosi': 3382,\n",
       " 'regul': 10086,\n",
       " 'sale': 10641,\n",
       " 'hemp': 5570,\n",
       " 'seed': 10904,\n",
       " 'consumpt': 2696,\n",
       " 'tourism': 12530,\n",
       " 'board': 1329,\n",
       " 'alburi': 246,\n",
       " 'today': 12450,\n",
       " 'die': 3405,\n",
       " 'fall': 4242,\n",
       " 'citi': 2332,\n",
       " 'balconi': 830,\n",
       " 'photograph': 9236,\n",
       " 'ban': 859,\n",
       " 'parliament': 9001,\n",
       " 'elect': 3872,\n",
       " 'result': 10234,\n",
       " 'delay': 3234,\n",
       " 'contempt': 2704,\n",
       " 'smith': 11332,\n",
       " 'student': 11845,\n",
       " 'uncov': 12833,\n",
       " 'soft': 11397,\n",
       " 'coral': 2770,\n",
       " 'speci': 11505,\n",
       " 'tamar': 12127,\n",
       " 'river': 10365,\n",
       " 'geoff': 4899,\n",
       " 'toovey': 12491,\n",
       " 'london': 7159,\n",
       " 'bridg': 1569,\n",
       " 'stab': 11600,\n",
       " 'suspect': 12007,\n",
       " 'wwii': 13686,\n",
       " 'pen': 9124,\n",
       " 'stori': 11771,\n",
       " 'futur': 4758,\n",
       " 'generat': 4884,\n",
       " 'masri': 7519,\n",
       " 'charg': 2182,\n",
       " 'domest': 3587,\n",
       " 'art': 573,\n",
       " 'quarter': 9776,\n",
       " 'downer': 3635,\n",
       " 'push': 9735,\n",
       " 'pacif': 8894,\n",
       " 'island': 6236,\n",
       " 'work': 13634,\n",
       " 'scheme': 10768,\n",
       " 'convict': 2729,\n",
       " 'babi': 766,\n",
       " 'keli': 6548,\n",
       " 'lane': 6816,\n",
       " 'refus': 10065,\n",
       " 'water': 13334,\n",
       " 'price': 9578,\n",
       " 'hard': 5407,\n",
       " 'swallow': 12022,\n",
       " 'cowra': 2863,\n",
       " 'minimum': 7898,\n",
       " 'size': 11224,\n",
       " 'entertain': 3989,\n",
       " 'abc': 12,\n",
       " 'david': 3125,\n",
       " 'split': 11553,\n",
       " 'schooli': 10785,\n",
       " 'dairi': 3051,\n",
       " 'farmer': 4270,\n",
       " 'halt': 5350,\n",
       " 'milk': 7862,\n",
       " 'altern': 314,\n",
       " 'wine': 13542,\n",
       " 'award': 752,\n",
       " 'state': 11664,\n",
       " 'origin': 8768,\n",
       " 'live': 7105,\n",
       " 'blog': 1302,\n",
       " 'cook': 2735,\n",
       " 'stand': 11633,\n",
       " 'aussi': 701,\n",
       " 'flash': 4475,\n",
       " 'north': 8530,\n",
       " 'brawl': 1519,\n",
       " 'street': 11810,\n",
       " 'perman': 9164,\n",
       " 'resid': 10210,\n",
       " 'promis': 9643,\n",
       " 'check': 2211,\n",
       " 'peter': 9192,\n",
       " 'ryan': 10590,\n",
       " 'extend': 4182,\n",
       " 'bell': 1087,\n",
       " 'hit': 5683,\n",
       " 'maiden': 7339,\n",
       " 'centuri': 2123,\n",
       " 'england': 3971,\n",
       " 'declar': 3179,\n",
       " 'grain': 5109,\n",
       " 'season': 10879,\n",
       " 'edg': 3832,\n",
       " 'tribun': 12642,\n",
       " 'controversi': 2721,\n",
       " 'cull': 2984,\n",
       " 'surviv': 12002,\n",
       " 'birthday': 1235,\n",
       " 'parti': 9017,\n",
       " 'give': 4978,\n",
       " 'touch': 12521,\n",
       " 'judg': 6434,\n",
       " 'dump': 3729,\n",
       " 'pass': 9029,\n",
       " 'gaff': 4771,\n",
       " 'surat': 11976,\n",
       " 'prepar': 9552,\n",
       " 'possibl': 9479,\n",
       " 'grog': 5210,\n",
       " 'restrict': 10232,\n",
       " 'hamper': 5363,\n",
       " 'recruit': 10015,\n",
       " 'drought': 3692,\n",
       " 'levi': 6999,\n",
       " 'slug': 11309,\n",
       " 'attract': 676,\n",
       " 'coal': 2438,\n",
       " 'worker': 13637,\n",
       " 'hunter': 5879,\n",
       " 'chamber': 2152,\n",
       " 'crime': 2913,\n",
       " 'concern': 2611,\n",
       " 'outlook': 8814,\n",
       " 'rabbit': 9820,\n",
       " 'plagu': 9319,\n",
       " 'leagu': 6905,\n",
       " 'januari': 6293,\n",
       " 'hurt': 5890,\n",
       " 'carol': 1994,\n",
       " 'candlelight': 1911,\n",
       " 'firework': 4435,\n",
       " 'labor': 6761,\n",
       " 'accommod': 61,\n",
       " 'cape': 1937,\n",
       " 'alumina': 317,\n",
       " 'drop': 3691,\n",
       " 'hill': 5649,\n",
       " 'project': 9636,\n",
       " 'invad': 6185,\n",
       " 'chief': 2248,\n",
       " 'apologis': 465,\n",
       " 'consult': 2694,\n",
       " 'slay': 11274,\n",
       " 'teen': 12216,\n",
       " 'rememb': 10143,\n",
       " 'peac': 9093,\n",
       " 'justic': 6464,\n",
       " 'luczak': 7224,\n",
       " 'open': 8726,\n",
       " 'mar': 7440,\n",
       " 'rover': 10524,\n",
       " 'travel': 12603,\n",
       " 'rocki': 10418,\n",
       " 'road': 10378,\n",
       " 'seek': 10905,\n",
       " 'beckham': 1046,\n",
       " 'notch': 8548,\n",
       " 'real': 9956,\n",
       " 'european': 4092,\n",
       " 'goal': 5026,\n",
       " 'wind': 13532,\n",
       " 'caus': 2076,\n",
       " 'chao': 2168,\n",
       " 'snowi': 11375,\n",
       " 'mountain': 8138,\n",
       " 'dribbl': 3676,\n",
       " 'nat': 8322,\n",
       " 'faster': 4284,\n",
       " 'premier': 9547,\n",
       " 'biotech': 1224,\n",
       " 'allianc': 296,\n",
       " 'pfizer': 9208,\n",
       " 'vaccin': 13013,\n",
       " 'move': 8146,\n",
       " 'closer': 2420,\n",
       " 'get': 4925,\n",
       " 'approv': 488,\n",
       " 'mccain': 7586,\n",
       " 'potato': 9493,\n",
       " 'cost': 2809,\n",
       " 'father': 4290,\n",
       " 'twin': 12766,\n",
       " 'toddler': 12452,\n",
       " 'grant': 5125,\n",
       " 'return': 10259,\n",
       " 'christchurch': 2295,\n",
       " 'quak': 9767,\n",
       " 'koala': 6682,\n",
       " 'sterilis': 11712,\n",
       " 'kangaroo': 6490,\n",
       " 'environ': 4004,\n",
       " 'band': 862,\n",
       " 'adel': 108,\n",
       " 'domin': 3588,\n",
       " 'chart': 2195,\n",
       " 'metro': 7803,\n",
       " 'confus': 2643,\n",
       " 'commut': 2571,\n",
       " 'commonwealth': 2566,\n",
       " 'ombudsman': 8697,\n",
       " 'condit': 2623,\n",
       " 'nauru': 8337,\n",
       " 'countri': 2835,\n",
       " 'wide': 13485,\n",
       " 'phil': 9219,\n",
       " 'saudi': 10720,\n",
       " 'arabia': 501,\n",
       " 'grace': 5095,\n",
       " 'period': 9159,\n",
       " 'vote': 13216,\n",
       " 'cervic': 2133,\n",
       " 'begin': 1069,\n",
       " 'boyfriend': 1481,\n",
       " 'sperm': 11528,\n",
       " 'brisban': 1593,\n",
       " 'doubl': 3624,\n",
       " 'shooter': 11100,\n",
       " 'agent': 170,\n",
       " 'highlight': 5638,\n",
       " 'rental': 10167,\n",
       " 'shortag': 11109,\n",
       " 'boom': 1402,\n",
       " 'sugar': 11904,\n",
       " 'product': 9624,\n",
       " 'challeng': 2150,\n",
       " 'talk': 12119,\n",
       " 'continu': 2712,\n",
       " 'courthous': 2843,\n",
       " 'act': 87,\n",
       " 'nurs': 8590,\n",
       " 'rape': 9908,\n",
       " 'grow': 5228,\n",
       " 'toxic': 12542,\n",
       " 'illeg': 5957,\n",
       " 'store': 11769,\n",
       " 'station': 11669,\n",
       " 'gympi': 5307,\n",
       " 'prison': 9602,\n",
       " 'exchang': 4134,\n",
       " 'enhanc': 3974,\n",
       " 'egypt': 3854,\n",
       " 'israel': 6242,\n",
       " 'tie': 12390,\n",
       " 'winton': 13559,\n",
       " 'reflect': 10054,\n",
       " 'latest': 6855,\n",
       " 'novel': 8558,\n",
       " 'minist': 7899,\n",
       " 'cyril': 3041,\n",
       " 'rioli': 10345,\n",
       " 'voic': 13202,\n",
       " 'insid': 6122,\n",
       " 'dale': 3059,\n",
       " 'fidel': 4370,\n",
       " 'castro': 2044,\n",
       " 'entir': 3993,\n",
       " 'bash': 961,\n",
       " 'ocean': 8638,\n",
       " 'pool': 9445,\n",
       " 'campaign': 1881,\n",
       " 'ramp': 9885,\n",
       " 'half': 5344,\n",
       " 'offer': 8658,\n",
       " 'sue': 11899,\n",
       " 'colleg': 2504,\n",
       " 'haze': 5485,\n",
       " 'see': 10902,\n",
       " 'tini': 12427,\n",
       " 'local': 7125,\n",
       " 'market': 7477,\n",
       " 'close': 2419,\n",
       " 'rain': 9867,\n",
       " 'aid': 193,\n",
       " 'bulldog': 1703,\n",
       " 'creek': 2902,\n",
       " 'cadel': 1826,\n",
       " 'evan': 4103,\n",
       " 'triumph': 12663,\n",
       " 'itali': 6245,\n",
       " 'kite': 6652,\n",
       " 'runner': 10567,\n",
       " 'censor': 2108,\n",
       " 'activ': 90,\n",
       " 'pick': 9249,\n",
       " 'sydney': 12059,\n",
       " 'search': 10876,\n",
       " 'sister': 11217,\n",
       " 'public': 9698,\n",
       " 'sector': 10896,\n",
       " 'union': 12894,\n",
       " 'influenc': 6076,\n",
       " 'secur': 10898,\n",
       " 'passag': 9030,\n",
       " 'hamilton': 5358,\n",
       " 'wast': 13329,\n",
       " 'boost': 1408,\n",
       " 'region': 10077,\n",
       " 'deep': 3195,\n",
       " 'purpl': 9729,\n",
       " 'member': 7726,\n",
       " 'aust': 702,\n",
       " 'staff': 11608,\n",
       " 'stop': 11764,\n",
       " 'debacl': 3154,\n",
       " 'undercov': 12839,\n",
       " 'oper': 8727,\n",
       " 'plane': 9322,\n",
       " 'crash': 2879,\n",
       " 'channel': 2166,\n",
       " 'ten': 12242,\n",
       " 'canadian': 1898,\n",
       " 'owner': 8878,\n",
       " 'mutant': 8238,\n",
       " 'loss': 7189,\n",
       " 'shark': 11027,\n",
       " 'bite': 1242,\n",
       " 'sink': 11209,\n",
       " 'cheetah': 2217,\n",
       " 'wildcat': 13504,\n",
       " 'final': 4400,\n",
       " 'aliv': 281,\n",
       " 'track': 12551,\n",
       " 'hottest': 5826,\n",
       " 'children': 2256,\n",
       " 'india': 6035,\n",
       " 'sweat': 12034,\n",
       " 'tendulkar': 12247,\n",
       " 'fit': 4451,\n",
       " 'swamp': 12023,\n",
       " 'cheap': 2203,\n",
       " 'knight': 6669,\n",
       " 'consid': 2675,\n",
       " 'reject': 10108,\n",
       " 'booz': 1414,\n",
       " 'cultur': 2992,\n",
       " 'serv': 10965,\n",
       " 'timer': 12416,\n",
       " 'villag': 13147,\n",
       " 'fete': 4358,\n",
       " 'pie': 9257,\n",
       " 'coast': 2441,\n",
       " 'wild': 13502,\n",
       " 'offic': 8659,\n",
       " 'breed': 1541,\n",
       " 'cattl': 2071,\n",
       " 'milit': 7859,\n",
       " 'philippin': 9224,\n",
       " 'bomb': 1376,\n",
       " 'iran': 6210,\n",
       " 'coach': 2436,\n",
       " 'blue': 1315,\n",
       " 'bring': 1589,\n",
       " 'idri': 5946,\n",
       " 'cover': 2850,\n",
       " 'chang': 2164,\n",
       " 'finalist': 4402,\n",
       " 'alcohol': 249,\n",
       " 'alic': 273,\n",
       " 'spring': 11575,\n",
       " 'casino': 2030,\n",
       " 'fin': 4398,\n",
       " 'eject': 3865,\n",
       " 'drunk': 3703,\n",
       " 'aim': 197,\n",
       " 'self': 10918,\n",
       " 'suffici': 11902,\n",
       " 'gile': 4947,\n",
       " 'launceston': 6862,\n",
       " 'respit': 10221,\n",
       " 'care': 1969,\n",
       " 'servic': 10967,\n",
       " 'hepburn': 5589,\n",
       " 'virginia': 13173,\n",
       " 'trioli': 12655,\n",
       " 'lockdown': 7130,\n",
       " 'melburnian': 7716,\n",
       " 'mayor': 7576,\n",
       " 'vietnam': 13133,\n",
       " 'trip': 12656,\n",
       " 'lend': 6963,\n",
       " 'slide': 11284,\n",
       " 'midland': 7836,\n",
       " 'emerg': 3924,\n",
       " 'violent': 13166,\n",
       " 'coff': 2475,\n",
       " 'robberi': 10391,\n",
       " 'super': 11947,\n",
       " 'iceland': 5932,\n",
       " 'iraqi': 6213,\n",
       " 'invas': 6187,\n",
       " 'passion': 9033,\n",
       " 'agricultur': 183,\n",
       " 'julia': 6446,\n",
       " 'hailstorm': 5328,\n",
       " 'toll': 12461,\n",
       " 'fruit': 4720,\n",
       " 'tree': 12619,\n",
       " 'hour': 5831,\n",
       " 'wastewat': 13330,\n",
       " 'spend': 11527,\n",
       " 'free': 4673,\n",
       " 'whoop': 13477,\n",
       " 'cough': 2821,\n",
       " 'nuttal': 8596,\n",
       " 'attempt': 670,\n",
       " 'overthrow': 8868,\n",
       " 'beatti': 1038,\n",
       " 'thwart': 12373,\n",
       " 'go': 5025,\n",
       " 'raid': 9861,\n",
       " 'wednesday': 13385,\n",
       " 'august': 686,\n",
       " 'issu': 6243,\n",
       " 'south': 11464,\n",
       " 'territori': 12272,\n",
       " 'primari': 9586,\n",
       " 'produc': 9623,\n",
       " 'farm': 4269,\n",
       " 'financ': 4403,\n",
       " 'loan': 7121,\n",
       " 'meander': 7671,\n",
       " 'option': 8743,\n",
       " 'manag': 7395,\n",
       " 'bonus': 1390,\n",
       " 'pilbara': 9268,\n",
       " 'look': 7171,\n",
       " 'perform': 9154,\n",
       " 'semi': 10927,\n",
       " 'secret': 10891,\n",
       " 'penguin': 9132,\n",
       " 'chase': 2197,\n",
       " 'danger': 3083,\n",
       " 'freeway': 4678,\n",
       " 'drive': 3685,\n",
       " 'kewel': 6589,\n",
       " 'liverpool': 7108,\n",
       " 'histor': 5680,\n",
       " 'juve': 6468,\n",
       " 'rematch': 10141,\n",
       " 'human': 5861,\n",
       " 'right': 10332,\n",
       " 'watch': 13331,\n",
       " 'palestinian': 8930,\n",
       " 'leigh': 6953,\n",
       " 'complet': 2587,\n",
       " 'hawaiian': 5473,\n",
       " 'ironman': 6221,\n",
       " 'kevin': 6588,\n",
       " 'mcdonald': 7606,\n",
       " 'ceas': 2093,\n",
       " 'effect': 3846,\n",
       " 'gaza': 4866,\n",
       " 'protea': 9668,\n",
       " 'lodg': 7142,\n",
       " 'hall': 5346,\n",
       " 'marijuana': 7465,\n",
       " 'possess': 9478,\n",
       " 'boss': 1432,\n",
       " 'andrew': 383,\n",
       " 'demetriou': 3264,\n",
       " 'wayn': 13360,\n",
       " 'bennett': 1120,\n",
       " 'golden': 5043,\n",
       " 'point': 9392,\n",
       " 'cowboy': 2859,\n",
       " 'wollongong': 13594,\n",
       " 'basketbal': 972,\n",
       " 'groom': 5213,\n",
       " 'offenc': 8655,\n",
       " 'activist': 91,\n",
       " 'joshua': 6416,\n",
       " 'wong': 13603,\n",
       " 'china': 2265,\n",
       " 'peta': 9189,\n",
       " 'robot': 10404,\n",
       " 'bolt': 1372,\n",
       " 'approach': 486,\n",
       " 'ultim': 12795,\n",
       " 'sprint': 11583,\n",
       " 'sach': 10600,\n",
       " 'shellfish': 11058,\n",
       " 'reef': 10043,\n",
       " 'timor': 12419,\n",
       " 'civilian': 2342,\n",
       " 'gosnel': 5076,\n",
       " 'deliber': 3241,\n",
       " 'scipion': 10803,\n",
       " 'honour': 5772,\n",
       " 'whistleblow': 13466,\n",
       " 'door': 3608,\n",
       " 'restaur': 10226,\n",
       " 'belinda': 1086,\n",
       " 'neal': 8348,\n",
       " 'marin': 7467,\n",
       " 'debt': 3162,\n",
       " 'ski': 11235,\n",
       " 'serbia': 10955,\n",
       " 'quit': 9810,\n",
       " 'fail': 4225,\n",
       " 'greater': 5151,\n",
       " 'lofti': 7145,\n",
       " 'rang': 9895,\n",
       " 'heritag': 5599,\n",
       " 'waff': 13233,\n",
       " 'agenc': 168,\n",
       " 'need': 8356,\n",
       " 'unhealthi': 12884,\n",
       " 'alert': 258,\n",
       " 'canteen': 1929,\n",
       " 'cemeteri': 2106,\n",
       " 'hell': 5562,\n",
       " 'ignor': 5950,\n",
       " 'hater': 5461,\n",
       " 'enni': 3977,\n",
       " 'near': 8350,\n",
       " 'baghdad': 807,\n",
       " 'better': 1157,\n",
       " 'white': 13468,\n",
       " 'kingston': 6637,\n",
       " 'larri': 6845,\n",
       " 'lobster': 7124,\n",
       " 'stay': 11675,\n",
       " 'driver': 3686,\n",
       " 'fieri': 4376,\n",
       " 'highway': 5640,\n",
       " 'coupl': 2838,\n",
       " 'young': 13753,\n",
       " 'marri': 7491,\n",
       " 'man': 7393,\n",
       " 'rooster': 10477,\n",
       " 'wooli': 13623,\n",
       " 'threaten': 12351,\n",
       " 'walk': 13257,\n",
       " 'away': 754,\n",
       " 'morrison': 8089,\n",
       " 'asset': 625,\n",
       " 'test': 12280,\n",
       " 'pension': 9142,\n",
       " 'whitlam': 13472,\n",
       " 'engin': 3970,\n",
       " 'skill': 11238,\n",
       " 'industri': 6052,\n",
       " 'matur': 7554,\n",
       " 'howard': 5841,\n",
       " 'armi': 550,\n",
       " 'murder': 8214,\n",
       " 'villawood': 13150,\n",
       " 'cheaper': 2204,\n",
       " 'anderson': 378,\n",
       " 'label': 6760,\n",
       " 'green': 5158,\n",
       " 'brown': 1644,\n",
       " 'accord': 64,\n",
       " 'ipart': 6205,\n",
       " 'madrid': 7309,\n",
       " 'thump': 12364,\n",
       " 'avoid': 744,\n",
       " 'upset': 12973,\n",
       " 'toni': 12479,\n",
       " 'abbott': 11,\n",
       " 'turkish': 12744,\n",
       " 'prevent': 9574,\n",
       " 'foreign': 4588,\n",
       " 'fighter': 4385,\n",
       " 'cloth': 2424,\n",
       " 'nappi': 8303,\n",
       " 'salin': 10644,\n",
       " 'unseason': 12936,\n",
       " 'summer': 11923,\n",
       " 'govern': 5087,\n",
       " 'announc': 419,\n",
       " 'buyout': 1805,\n",
       " 'lawyer': 6890,\n",
       " 'want': 13285,\n",
       " 'forest': 4593,\n",
       " 'protest': 9671,\n",
       " 'pygmi': 9744,\n",
       " 'peopl': 9144,\n",
       " 'flore': 4519,\n",
       " 'ancestor': 372,\n",
       " 'hobbit': 5697,\n",
       " 'suspici': 12010,\n",
       " 'agre': 179,\n",
       " 'ramsi': 9890,\n",
       " 'review': 10275,\n",
       " 'rundl': 10566,\n",
       " 'mall': 7377,\n",
       " 'garden': 4820,\n",
       " 'feel': 4318,\n",
       " 'economist': 3824,\n",
       " 'haigh': 5326,\n",
       " 'chocol': 2277,\n",
       " 'recal': 9982,\n",
       " 'hazelnut': 5487,\n",
       " 'luke': 7233,\n",
       " 'odonnel': 8647,\n",
       " 'million': 7872,\n",
       " 'patel': 9045,\n",
       " 'medic': 7689,\n",
       " 'suspens': 12009,\n",
       " 'nitschk': 8484,\n",
       " 'licenc': 7026,\n",
       " 'fight': 4383,\n",
       " 'knowl': 6678,\n",
       " 'focus': 4545,\n",
       " 'samaritan': 10661,\n",
       " 'xmas': 13697,\n",
       " 'keen': 6541,\n",
       " 'victorian': 13124,\n",
       " 'team': 12202,\n",
       " 'head': 5491,\n",
       " 'stricken': 11816,\n",
       " 'sell': 10922,\n",
       " 'tassi': 12178,\n",
       " 'devil': 3370,\n",
       " 'diseas': 3483,\n",
       " 'dual': 3706,\n",
       " 'citizen': 2335,\n",
       " 'child': 2251,\n",
       " 'porn': 9459,\n",
       " 'webber': 13377,\n",
       " 'pole': 9406,\n",
       " 'solomon': 11419,\n",
       " 'warlord': 13296,\n",
       " 'hostag': 5816,\n",
       " 'bundaberg': 1724,\n",
       " 'revamp': 10265,\n",
       " 'singer': 11205,\n",
       " 'toad': 12443,\n",
       " 'awar': 751,\n",
       " 'reach': 9947,\n",
       " 'deal': 3148,\n",
       " ...}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostramos el vocabulario:\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "108845c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981006, 13808)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "731d4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num√©ro de traces:  981006\n",
      "N√∫mero de tokens:  13808\n"
     ]
    }
   ],
   "source": [
    "print('Num√©ro de traces: ',info_train_vec.shape[0])\n",
    "print('N√∫mero de tokens: ',info_train_vec.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efbf8e",
   "metadata": {},
   "source": [
    "**3) Implementaci√≥n del modelo:** Ya contamos con el elemento principal para implementar nuestro modelo [LDA-SKTL](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Acerca de los par√°metros que vamos a utilizar:\n",
    "- n_components: n√∫mero de t√≥picos:\n",
    "- max_iter: the maximum number of passes over the training data.\n",
    "- learning_method: method used to update_component. Only used in fit method. In general, if the data size is large, the online update will be much faster than the batch update.\n",
    "- learning_offset: A (positive) parameter that downweights early iterations in online learning\n",
    "- random_state: Pass an int for reproducible results across multiple function calls.\n",
    "\n",
    "Existen m√°s par√°metros para configurar el modelo pero no aondaremos en ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e56f9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 10\n",
    "#Implementamos el modelo\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, max_iter=5, learning_method='online',learning_offset=50., random_state=0)\n",
    "\n",
    "lda.fit(info_train_vec)\n",
    "\n",
    "# making LDA TOP MATRIX USING CORPUS TF\n",
    "lda_topic_modelling = lda.fit_transform(info_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc0a9c",
   "metadata": {},
   "source": [
    "Nota: Las siguientes funciones no tienen otro objetivo m√°s que trabajar con la informaci√≥n que arroj√≥ el modelo, no son fundamentales para efectos del desarrollo del modelo como tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f645769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return an integer list of predicted topic catergories for a given topic matrix\n",
    "def get_keys(topic_matrix):\n",
    "  # print(topic_matrix.argmax(axis = 1)) # axis = 1, will return maximum index in that array \n",
    "  keys = topic_matrix.argmax(axis = 1).tolist()\n",
    "  print(\"length of the keys is: \",len(keys))\n",
    "  return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f38d4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a tuple of topic categories and their accompanying magnitude for a given list of keys\n",
    "from collections import Counter\n",
    "def key_to_count(keys):\n",
    "  count_pairs = Counter(keys).items()\n",
    "  # print(\"Count_pairs\",count_pairs)\n",
    "  categories = [pair[0] for pair in count_pairs]\n",
    "  # print(\"categories\",categories)\n",
    "  counts = [pair[1] for pair in count_pairs]\n",
    "  # print(\"Counts: \",counts)\n",
    "  return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e4650f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the keys is:  981006\n"
     ]
    }
   ],
   "source": [
    "lda_keys = get_keys(lda_topic_modelling)\n",
    "# print(\"keys: \",lda_keys)\n",
    "# key_to_count(lda_keys)\n",
    "\n",
    "lda_categories, lda_count = key_to_count(lda_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9083d4",
   "metadata": {},
   "source": [
    "**4) Visualizaci√≥n de la informaci√≥n:** Con el fin de mostrar de una manera ordenada los resultados arrojados por el modelo mostramos lo siguiente:\n",
    "- 4.1) El t√≥pico predominante para cada documento perteneciente a la informaci√≥n de entrenamiento\n",
    "- 4.2) la contribuci√≥n de cada t√≥pico para cada documento\n",
    "- 4.3) N√∫mero de documentos asociados a cada t√≥pico\n",
    "- 4.4) Tabla que contenga la contribuci√≥n de cada palabra a su respectivo t√≥pico \n",
    "- 4.5) Las palabras m√°s importantes asociadas a cada t√≥pico\n",
    "- 4.6) [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/index.html): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb782d8",
   "metadata": {},
   "source": [
    "**4.1) T√≥pico predominante para cada documento:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5ef8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #0 - topic: 0\n",
      "\n",
      "Document #1 - topic: 5\n",
      "\n",
      "Document #2 - topic: 3\n",
      "\n",
      "Document #3 - topic: 6\n",
      "\n",
      "Document #4 - topic: 5\n",
      "\n",
      "Document #5 - topic: 9\n",
      "\n",
      "Document #6 - topic: 5\n",
      "\n",
      "Document #7 - topic: 6\n",
      "\n",
      "Document #8 - topic: 1\n",
      "\n",
      "Document #9 - topic: 8\n",
      "\n",
      "Document #10 - topic: 6\n",
      "\n",
      "Document #11 - topic: 5\n",
      "\n",
      "Document #12 - topic: 3\n",
      "\n",
      "Document #13 - topic: 2\n",
      "\n",
      "Document #14 - topic: 4\n",
      "\n",
      "Document #15 - topic: 3\n",
      "\n",
      "Document #16 - topic: 0\n",
      "\n",
      "Document #17 - topic: 1\n",
      "\n",
      "Document #18 - topic: 3\n",
      "\n",
      "Document #19 - topic: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostramos el t√≥pico predominante para cada trace_traindata\n",
    "doc_topic = lda.transform(info_train_vec)\n",
    "\n",
    "# check for 20 documents\n",
    "for n in range(20):\n",
    "  # print(doc_topic[n])\n",
    "  topic_most_pr = doc_topic[n].argmax()\n",
    "  # print(topic_most_pr)\n",
    "  print(\"Document #{} - topic: {}\\n\".format(n,topic_most_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df3fc6",
   "metadata": {},
   "source": [
    "**4.2) Tabla t√≥pico-documento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e585f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226253</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226254</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226255</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.516666</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226256</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226257</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226258 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.020000  0.020000  0.020000  0.020000  0.020000  0.820000  0.020000   \n",
       "1        0.025000  0.025000  0.025000  0.025000  0.025000  0.025000  0.025000   \n",
       "2        0.220000  0.420000  0.020000  0.220000  0.020000  0.020000  0.020000   \n",
       "3        0.220000  0.020000  0.020000  0.420000  0.020000  0.020000  0.020000   \n",
       "4        0.020000  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1226253  0.020000  0.020000  0.020000  0.220000  0.020000  0.220000  0.020000   \n",
       "1226254  0.020000  0.020000  0.020000  0.020000  0.420000  0.020000  0.020000   \n",
       "1226255  0.016667  0.016667  0.016667  0.016667  0.016667  0.183333  0.016667   \n",
       "1226256  0.014286  0.300000  0.157143  0.014286  0.157143  0.014286  0.014286   \n",
       "1226257  0.014286  0.014286  0.014286  0.157143  0.014286  0.157143  0.300000   \n",
       "\n",
       "                7         8         9  \n",
       "0        0.020000  0.020000  0.020000  \n",
       "1        0.025000  0.775000  0.025000  \n",
       "2        0.020000  0.020000  0.020000  \n",
       "3        0.220000  0.020000  0.020000  \n",
       "4        0.820000  0.020000  0.020000  \n",
       "...           ...       ...       ...  \n",
       "1226253  0.220000  0.020000  0.220000  \n",
       "1226254  0.420000  0.020000  0.020000  \n",
       "1226255  0.516666  0.016667  0.183333  \n",
       "1226256  0.157143  0.014286  0.157143  \n",
       "1226257  0.157143  0.014286  0.157143  \n",
       "\n",
       "[1226258 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a dataframe from the document-topic matrix\n",
    "doc_topic_df = pd.DataFrame(data=doc_topic)\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe91aae",
   "metadata": {},
   "source": [
    "**4.3) N√∫mero de documentos asociados a cada t√≥pico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5edd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = lda.transform(processed_docs_vec)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(processed_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be9c39d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc0          5\n",
       "Doc1          8\n",
       "Doc2          1\n",
       "Doc3          3\n",
       "Doc4          7\n",
       "             ..\n",
       "Doc1226253    3\n",
       "Doc1226254    4\n",
       "Doc1226255    7\n",
       "Doc1226256    1\n",
       "Doc1226257    6\n",
       "Name: dominant_topic, Length: 1226258, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic['dominant_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d4c20ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Num</th>\n",
       "      <th>Num Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>212590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>201546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>155160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>124670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>101651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>97856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>91251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>88591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>78178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>74765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Num  Num Documents\n",
       "0          0         212590\n",
       "1          1         201546\n",
       "2          2         155160\n",
       "3          3         124670\n",
       "4          6         101651\n",
       "5          4          97856\n",
       "6          5          91251\n",
       "7          8          88591\n",
       "8          9          78178\n",
       "9          7          74765"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88066b0b",
   "metadata": {},
   "source": [
    "**4.4) Topic-Keyword Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "370231a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaco</th>\n",
       "      <th>aacta</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abalon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abar</th>\n",
       "      <th>abat</th>\n",
       "      <th>abattoir</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>zygier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic0</th>\n",
       "      <td>132.121386</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>629.153581</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>78.339516</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>398.294495</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100023</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>15.614606</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>709.410616</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>183.855465</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100023</td>\n",
       "      <td>0.100040</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>23.598954</td>\n",
       "      <td>0.100024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>0.100021</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>45.519442</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>44.207086</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>32.720552</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 13808 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              aaco     aacta     aaron      abalon     abandon      abar  \\\n",
       "Topic0  132.121386  0.100002  0.100005    0.100011    0.100016  0.100009   \n",
       "Topic1    0.100008  0.100007  0.100004  398.294495    0.100010  0.100013   \n",
       "Topic2    0.100008  0.100004  0.100018    0.100006    0.100012  0.100006   \n",
       "Topic3    0.100006  0.100020  0.100007    0.100014  709.410616  0.100014   \n",
       "Topic4    0.100021  0.100019  0.100004    0.100008    0.100010  0.100002   \n",
       "\n",
       "             abat    abattoir        abba     abbey  ...      zone        zoo  \\\n",
       "Topic0   0.100014  629.153581    0.100014  0.100001  ...  0.100012   0.100012   \n",
       "Topic1   0.100016    0.100009    0.100008  0.100012  ...  0.100011   0.100010   \n",
       "Topic2  15.614606    0.100006    0.100008  0.100011  ...  0.100005   0.100010   \n",
       "Topic3   0.100006    0.100008  183.855465  0.100002  ...  0.100009   0.100012   \n",
       "Topic4   0.100016    0.100009    0.100011  0.100005  ...  0.100011  45.519442   \n",
       "\n",
       "         zookeep      zoom  zuckerberg       zuma     zurich    zverev  \\\n",
       "Topic0  0.100004  0.100024    0.100011  78.339516   0.100007  0.100003   \n",
       "Topic1  0.100002  0.100029    0.100009   0.100023   0.100002  0.100008   \n",
       "Topic2  0.100003  0.100011    0.100005   0.100017   0.100004  0.100020   \n",
       "Topic3  0.100027  0.100007    0.100007   0.100023   0.100040  0.100010   \n",
       "Topic4  0.100013  0.100011   44.207086   0.100010  32.720552  0.100002   \n",
       "\n",
       "        zvonareva    zygier  \n",
       "Topic0   0.100002  0.100002  \n",
       "Topic1   0.100002  0.100009  \n",
       "Topic2   0.100006  0.100001  \n",
       "Topic3  23.598954  0.100024  \n",
       "Topic4   0.100004  0.100004  \n",
       "\n",
       "[5 rows x 13808 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(lda.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd571f",
   "metadata": {},
   "source": [
    "**4.5) Tabla Word-Topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "012f2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c29ec1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>plan</td>\n",
       "      <td>council</td>\n",
       "      <td>water</td>\n",
       "      <td>call</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>warn</td>\n",
       "      <td>year</td>\n",
       "      <td>health</td>\n",
       "      <td>rise</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>polic</td>\n",
       "      <td>interview</td>\n",
       "      <td>crash</td>\n",
       "      <td>investig</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>charg</td>\n",
       "      <td>court</td>\n",
       "      <td>face</td>\n",
       "      <td>murder</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>hous</td>\n",
       "      <td>elect</td>\n",
       "      <td>govern</td>\n",
       "      <td>help</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>chang</td>\n",
       "      <td>claim</td>\n",
       "      <td>world</td>\n",
       "      <td>trial</td>\n",
       "      <td>record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>death</td>\n",
       "      <td>sydney</td>\n",
       "      <td>say</td>\n",
       "      <td>open</td>\n",
       "      <td>polic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>australia</td>\n",
       "      <td>farmer</td>\n",
       "      <td>servic</td>\n",
       "      <td>talk</td>\n",
       "      <td>protest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>kill</td>\n",
       "      <td>report</td>\n",
       "      <td>australian</td>\n",
       "      <td>urg</td>\n",
       "      <td>fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>govt</td>\n",
       "      <td>market</td>\n",
       "      <td>coast</td>\n",
       "      <td>case</td>\n",
       "      <td>boost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word 0     Word 1      Word 2    Word 3   Word 4\n",
       "index                                                       \n",
       "Topic 0       plan    council       water      call     fear\n",
       "Topic 1       warn       year      health      rise     test\n",
       "Topic 2      polic  interview       crash  investig    fight\n",
       "Topic 3      charg      court        face    murder    woman\n",
       "Topic 4       hous      elect      govern      help      win\n",
       "Topic 5      chang      claim       world     trial   record\n",
       "Topic 6      death     sydney         say      open    polic\n",
       "Topic 7  australia     farmer      servic      talk  protest\n",
       "Topic 8       kill     report  australian       urg     fund\n",
       "Topic 9       govt     market       coast      case    boost"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic - Keywords Dataframe\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda, n_words=5) \n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords.index.name='index'\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b459e2",
   "metadata": {},
   "source": [
    "**4.6) pyLDAvis:** The pyLDAvis offers the best visualization to view the topics-keywords distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7bb9c9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2156423697090985925207548594\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2156423697090985925207548594_data = {\"mdsDat\": {\"x\": [32.42095184326172, 0.648955225944519, 71.01968383789062, 3.7859814167022705, -42.666500091552734, -3.6216893196105957, -3.632401704788208, -36.58440017700195, 44.391380310058594, 40.235809326171875], \"y\": [8.115229606628418, 71.70709991455078, 9.971006393432617, 33.18513488769531, -9.22206974029541, -43.52651596069336, -4.171707630157471, 35.804412841796875, 50.61538314819336, -31.591665267944336], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.70025115512123, 10.44723316270002, 10.405568307082472, 10.243107131087761, 10.110136577664957, 10.016527358145904, 9.859816325406628, 9.539459055447317, 9.415395548142767, 9.262505379200956]}, \"tinfo\": {\"Term\": [\"polic\", \"charg\", \"australia\", \"plan\", \"govt\", \"interview\", \"kill\", \"report\", \"australian\", \"death\", \"crash\", \"council\", \"face\", \"court\", \"warn\", \"urg\", \"water\", \"fund\", \"sydney\", \"say\", \"murder\", \"year\", \"chang\", \"hous\", \"open\", \"health\", \"elect\", \"claim\", \"world\", \"jail\", \"govt\", \"market\", \"coast\", \"case\", \"boost\", \"worker\", \"minist\", \"road\", \"concern\", \"gold\", \"famili\", \"close\", \"countri\", \"region\", \"farm\", \"strike\", \"reject\", \"offer\", \"offic\", \"storm\", \"local\", \"vote\", \"cut\", \"bomb\", \"develop\", \"law\", \"give\", \"hour\", \"parti\", \"award\", \"plan\", \"warn\", \"year\", \"health\", \"rise\", \"test\", \"south\", \"melbourn\", \"north\", \"continu\", \"power\", \"rural\", \"support\", \"high\", \"probe\", \"public\", \"union\", \"secur\", \"canberra\", \"brisban\", \"hear\", \"perth\", \"question\", \"women\", \"trade\", \"begin\", \"station\", \"issu\", \"remain\", \"wont\", \"airport\", \"fatal\", \"nation\", \"seek\", \"say\", \"death\", \"sydney\", \"open\", \"jail\", \"miss\", \"hospit\", \"drug\", \"die\", \"price\", \"arrest\", \"busi\", \"child\", \"tell\", \"adelaid\", \"search\", \"fall\", \"iraq\", \"guilti\", \"bushfir\", \"look\", \"speak\", \"second\", \"children\", \"tasmanian\", \"media\", \"citi\", \"risk\", \"club\", \"week\", \"name\", \"say\", \"polic\", \"kill\", \"report\", \"australian\", \"urg\", \"fund\", \"green\", \"time\", \"forc\", \"budget\", \"land\", \"cost\", \"bodi\", \"rain\", \"target\", \"campaign\", \"near\", \"hit\", \"sign\", \"game\", \"save\", \"premier\", \"blaze\", \"england\", \"million\", \"battl\", \"rate\", \"boat\", \"despit\", \"cancer\", \"right\", \"attack\", \"leader\", \"council\", \"charg\", \"face\", \"murder\", \"woman\", \"group\", \"final\", \"work\", \"want\", \"push\", \"alleg\", \"assault\", \"defend\", \"appeal\", \"mayor\", \"teen\", \"aussi\", \"island\", \"action\", \"doctor\", \"play\", \"drive\", \"launch\", \"ahead\", \"beat\", \"compani\", \"reveal\", \"anti\", \"east\", \"stop\", \"stand\", \"court\", \"water\", \"call\", \"fear\", \"coronavirus\", \"indigen\", \"get\", \"student\", \"trump\", \"victoria\", \"job\", \"threat\", \"weather\", \"find\", \"river\", \"liber\", \"rail\", \"link\", \"critic\", \"debat\", \"demand\", \"chines\", \"rais\", \"sport\", \"girl\", \"move\", \"approv\", \"truck\", \"good\", \"ralli\", \"bush\", \"plan\", \"council\", \"school\", \"home\", \"queensland\", \"west\", \"mine\", \"say\", \"studi\", \"hous\", \"elect\", \"govern\", \"help\", \"win\", \"china\", \"industri\", \"lead\", \"centr\", \"train\", \"communiti\", \"park\", \"share\", \"need\", \"back\", \"delay\", \"futur\", \"welcom\", \"announc\", \"prison\", \"start\", \"injur\", \"decis\", \"expect\", \"darwin\", \"threaten\", \"peopl\", \"join\", \"team\", \"increas\", \"flood\", \"polic\", \"chang\", \"claim\", \"world\", \"trial\", \"record\", \"labor\", \"take\", \"break\", \"state\", \"news\", \"live\", \"head\", \"inquiri\", \"port\", \"bank\", \"sale\", \"tasmania\", \"life\", \"make\", \"hill\", \"star\", \"plead\", \"challeng\", \"tourism\", \"food\", \"senat\", \"turn\", \"titl\", \"stay\", \"covid\", \"accus\", \"meet\", \"deni\", \"interview\", \"crash\", \"investig\", \"fight\", \"lose\", \"victim\", \"feder\", \"resid\", \"releas\", \"review\", \"sentenc\", \"race\", \"research\", \"rescu\", \"rat\", \"unit\", \"presid\", \"black\", \"clash\", \"go\", \"light\", \"promis\", \"show\", \"john\", \"number\", \"victori\", \"plane\", \"indonesia\", \"order\", \"christma\", \"polic\", \"leav\", \"driver\", \"court\", \"australia\", \"farmer\", \"servic\", \"talk\", \"protest\", \"dead\", \"shoot\", \"deal\", \"return\", \"consid\", \"opposit\", \"abus\", \"chief\", \"drought\", \"season\", \"free\", \"spark\", \"stab\", \"build\", \"india\", \"project\", \"fail\", \"care\", \"suspect\", \"drop\", \"violenc\", \"teacher\", \"export\", \"confid\", \"cattl\"], \"Freq\": [31887.0, 16666.0, 15942.0, 18642.0, 14154.0, 12988.0, 12789.0, 12371.0, 12285.0, 12258.0, 11715.0, 15788.0, 11458.0, 14553.0, 11309.0, 11175.0, 10663.0, 10553.0, 10504.0, 18832.0, 10147.0, 10247.0, 9671.0, 9336.0, 9389.0, 9181.0, 8909.0, 8773.0, 8624.0, 8767.0, 14153.915569821454, 8714.442261546732, 7593.649262581598, 6990.485770328782, 6922.588898026754, 6826.618509326829, 6743.92484960184, 6704.6817728372, 6356.43058140871, 5630.61758278248, 5627.125123072479, 5148.529761900321, 4727.8404179831905, 4720.152679715207, 4711.123258292816, 4594.1579070617645, 4463.775979717268, 4452.208621727015, 4384.635345499472, 4282.813146025861, 4191.712966282362, 4041.323945388614, 4012.4729325322096, 3910.432313825382, 3826.6421795971382, 3719.611958786286, 3695.876459993423, 3670.17248502882, 3591.530397103846, 3353.470009356809, 4697.2422159564085, 11308.36661474395, 10246.211878830987, 9181.043018635452, 7934.700313819051, 7730.26927323437, 7585.775219542234, 6801.762223028719, 6763.424516424601, 6614.597073584908, 6496.410484904188, 6440.517814450049, 5750.88443095563, 5662.7546251731155, 5573.031474317118, 5381.132062895774, 5319.776133564551, 4933.200249089519, 4907.285897210111, 4638.4901302276285, 4588.175081818763, 4567.967443941982, 4391.300741991752, 4287.62228203697, 4154.018205588551, 4065.3989617028155, 3541.9327085402583, 3128.617274552319, 2847.111494813473, 2792.4886574084776, 2765.151431715426, 3903.7058028145207, 6199.158557382559, 5263.724587718437, 4184.924281550313, 12258.07865090431, 10503.669084536796, 9388.631437139431, 8766.102799943028, 8602.776799366402, 8344.679725372784, 7591.488160957902, 6985.9896195267975, 6631.115173464298, 6499.122432043116, 6491.056779862105, 5935.451377846886, 5249.981218564819, 5228.878484780454, 5175.052501624061, 5165.3051922923805, 4669.212275754116, 4599.2115994502465, 4470.14262087203, 4330.026904440535, 3848.0293451594207, 3840.775946479342, 3492.3869720751977, 3306.4272481844414, 3194.0696606310926, 3122.496516208165, 3061.1252329014437, 3034.3682051426536, 2815.586473029691, 2747.4726126978785, 9565.422053311344, 9316.929537230422, 12788.62557339703, 12370.711379744793, 12284.527356968996, 11174.862908621042, 10552.9531539738, 6128.198240788746, 5528.917048482375, 5327.249102751331, 4989.3970220761885, 4419.160099734757, 4158.363729848802, 4113.588890776753, 4090.6511179291124, 4063.9023505256687, 3839.546702453239, 3835.643597494953, 3723.3898790830162, 3593.937440689403, 3489.280655132766, 3395.714086128725, 3348.33566898217, 3298.800312949354, 3138.6700791416547, 3116.5917392147608, 3004.04422395883, 2864.9844885508924, 2830.7074692612873, 2774.9310619320336, 2716.516108172091, 2693.3893253992087, 7939.335396705808, 3266.840747470193, 4319.842780135133, 16665.827883955448, 11457.294725643247, 10146.42878209415, 8097.471812753048, 6830.406965112772, 6821.281446830002, 6736.559835578152, 5848.604112157976, 5739.103760560558, 5524.156672651465, 5191.933898784039, 4809.387795840578, 4603.580550841031, 4478.223836068907, 4437.980739304507, 4372.7249717617515, 4061.441013789795, 4051.914466185975, 3878.4415438680408, 3836.326942952457, 3653.5411926436295, 3534.110610460536, 3532.044282087064, 3530.021154510518, 3384.359423421619, 3238.427256231314, 3226.721390283326, 3210.4542630858186, 3202.8063400925917, 3079.509022961046, 11523.395076264742, 10662.54536076783, 8538.288348021471, 7053.539459294798, 6638.178689201341, 5811.472027728794, 5275.743578552164, 4531.46638280258, 4435.752626581479, 3904.318906662728, 3897.5041618409273, 3616.3285283777327, 3609.53676431196, 3536.431049664432, 3527.5619783013117, 3351.123921569478, 3153.871103773439, 3129.049731204426, 2876.065556362792, 2873.604600890352, 2866.7049684620215, 2825.9336010185134, 2778.7882806482567, 2609.9484100196796, 2573.673660131445, 2558.785001373715, 2502.789042009642, 2459.906861767922, 2415.1601586912243, 2379.1880678210628, 2368.2919985897747, 13944.064656139873, 11467.99310093168, 6333.890121668092, 6073.970814876792, 4486.50544562192, 3716.0039303440426, 3283.984212089571, 5081.698317105624, 2822.6496925838633, 9335.307363438187, 8908.60238932309, 8488.302084748717, 8317.42292620246, 7307.486398167261, 7155.91658714523, 6399.606267534849, 6291.8881998060715, 5929.277811121434, 5218.538052769869, 5208.541707215338, 5064.79941018935, 4799.897948961161, 4735.644674967074, 4653.80262244856, 4253.766677349077, 4054.102836994151, 4045.670930210479, 3954.3854309560434, 3931.8282964045848, 3830.207033909956, 3804.2594192842403, 3583.638201655222, 3537.5315589320394, 3466.2391060273912, 3274.9734912390936, 3242.0166192160855, 3144.951383991301, 3046.061315627986, 3006.1923003114384, 4382.655302250746, 4959.524590825783, 9671.074383455034, 8772.894721196713, 8623.828102080815, 6595.0765590437895, 6460.691252033944, 6268.171745660027, 5919.987948866705, 5496.255647519841, 5380.7070636968265, 5176.932399734039, 4763.473973404325, 4716.307566685377, 4663.80068968712, 4383.683908433525, 4143.203165063992, 4065.0776172671017, 3838.3527502818024, 3602.2841376423135, 3598.344116522069, 3377.6917664839452, 3143.4560587352917, 3075.3287468787476, 3062.05807023901, 2982.386870570788, 2974.1245033517957, 2969.6619349266457, 2888.544571149777, 2839.311246284314, 2786.301077302124, 2661.583738874801, 6106.968723286079, 4277.238543817293, 3323.4337670273735, 12987.915458452164, 11714.222746013378, 6954.293052248755, 6478.817133290385, 5565.960781396179, 5353.379693082583, 5216.003391505065, 5191.324328064076, 4679.640619752056, 4603.699986577971, 4129.490867750625, 3987.6943151338137, 3900.011465901311, 3688.845222501198, 3503.0146583510827, 3267.1586231080837, 3093.1356389252333, 2977.116348245758, 2911.372364931764, 2901.437808879441, 2900.2336317278505, 2873.230137259527, 2751.341833130554, 2632.5331731063116, 2621.9114210797075, 2611.4884027917565, 2502.33230983349, 2412.4731164756404, 2391.270968491662, 2388.86455491311, 17610.799321654944, 3821.868501567802, 3718.2590648645837, 3029.3436625337267, 15942.065641632926, 7377.107080265316, 7125.274983215448, 7037.585997011014, 7002.238399010305, 6713.740839229963, 6334.94806809841, 6254.359576034788, 6204.489625785637, 5034.0566845555495, 4408.448484939447, 4149.430496581216, 3941.6416867541843, 3891.5310474828957, 3726.890107127519, 3700.0160727605826, 3626.446200786192, 3547.083318957657, 3504.8964292246064, 3361.997129199009, 3266.6169146335874, 3264.462971174047, 3200.1616621030694, 3174.9006649842404, 3143.4294862025063, 3099.814724358203, 2875.398339693083, 2859.0577101940407, 2846.3742113089033, 2767.3851573387215], \"Total\": [31887.0, 16666.0, 15942.0, 18642.0, 14154.0, 12988.0, 12789.0, 12371.0, 12285.0, 12258.0, 11715.0, 15788.0, 11458.0, 14553.0, 11309.0, 11175.0, 10663.0, 10553.0, 10504.0, 18832.0, 10147.0, 10247.0, 9671.0, 9336.0, 9389.0, 9181.0, 8909.0, 8773.0, 8624.0, 8767.0, 14154.81475664772, 8715.34143001926, 7594.548422972642, 6991.384972033013, 6923.4880822036375, 6827.517692448681, 6744.824034176667, 6705.580966245468, 6357.329766708882, 5631.516741270864, 5628.0243151028735, 5149.42894102655, 4728.7395585393315, 4721.051863794303, 4712.022427555587, 4595.057080454809, 4464.675165952868, 4453.107803104626, 4385.534549530135, 4283.712326417461, 4192.6121412751845, 4042.2231355129675, 4013.3721241523153, 3911.3315142482625, 3827.5413595094506, 3720.511151250726, 3696.775659961506, 3671.071615871787, 3592.429598764122, 3354.369200759752, 18642.106106495663, 11309.265193889192, 10247.110459509788, 9181.94158744136, 7935.598895842968, 7731.167859308837, 7586.6738128375955, 6802.660815493608, 6764.323105839518, 6615.495651035359, 6497.309059373371, 6441.416360343984, 5751.783013824905, 5663.653216814798, 5573.930065570824, 5382.030635860259, 5320.674717856559, 4934.098833774804, 4908.184481572364, 4639.388717968543, 4589.073668541727, 4568.866030561281, 4392.19932826002, 4288.520870379298, 4154.916802081637, 4066.2975373891527, 3542.831286445149, 3129.515843291594, 2848.010072438676, 2793.3872482492743, 2766.050005789259, 3976.0534738816614, 8364.513060348496, 7343.841133974853, 18832.744759808058, 12258.977623940631, 10504.568058849169, 9389.53040889876, 8767.001762572221, 8603.675752338908, 8345.57869919756, 7592.387128405435, 6986.888579639053, 6632.014150341808, 6500.021386867668, 6491.955757229542, 5936.35035696328, 5250.880213883563, 5229.777454949595, 5175.951441237558, 5166.204163257376, 4670.111245250006, 4600.1105777532, 4471.041604789534, 4330.925883919877, 3848.9283139234826, 3841.6749295382447, 3493.2859529955504, 3307.3262406819463, 3194.9686224155776, 3123.395503268178, 3062.0242151755915, 3035.2671817890564, 2816.4854388771064, 2748.3715879220317, 18832.744759808058, 31887.95208466731, 12789.523913812201, 12371.609737927027, 12285.425720578221, 11175.761267245489, 10553.851506527864, 6129.096596277678, 5529.815402817272, 5328.147460444527, 4990.295362192562, 4420.058454190202, 4159.262083015329, 4114.487250145613, 4091.549473333867, 4064.8007015335656, 3840.4450560922046, 3836.541951518241, 3724.288234724938, 3594.8357927614616, 3490.1790075580716, 3396.6124409671, 3349.2340307461855, 3299.6986498216784, 3139.568430194226, 3117.4900811220705, 3004.9425636218016, 2865.882844740217, 2831.605821699792, 2775.8294389303837, 2717.4144489741393, 2694.2876668684517, 10426.786702873964, 4576.84339362458, 15788.634290780448, 16666.725747324595, 11458.192604951993, 10147.32665695455, 8098.3697024857, 6831.304874435966, 6822.179341202692, 6737.457733377833, 5849.502014230885, 5740.001661142343, 5525.054552693884, 5192.831764928171, 4810.285694684087, 4604.478431407638, 4479.121735113767, 4438.878628672518, 4373.622862479715, 4062.3389161529317, 4052.8123549728352, 3879.3394416838205, 3837.2248360059557, 3654.4390779078685, 3535.008502631506, 3532.942188274428, 3530.919049465634, 3385.2573261421953, 3239.325158963414, 3227.619303319295, 3211.3521812429967, 3203.704238444533, 3080.4069106697066, 14553.536332412963, 10663.44293269843, 8539.185936855356, 7054.437042161914, 6639.076269389538, 5812.369607383665, 5276.641165981551, 4532.363958229047, 4436.650183370805, 3905.2164843373994, 3898.401732415304, 3617.226113829801, 3610.4343296774355, 3537.328641558905, 3528.459556834007, 3352.0215157859707, 3154.768673652537, 3129.9473208064833, 2876.9631376090374, 2874.5021745547506, 2867.6025563649255, 2826.831194677945, 2779.6858653771837, 2610.8459745914847, 2574.571254451623, 2559.682573526104, 2503.6866095826063, 2460.804456874183, 2416.0577383251, 2380.085649801559, 2369.189580873866, 18642.106106495663, 15788.634290780448, 8278.143322503925, 8544.779180126172, 6437.361647328921, 5164.694970724031, 4238.709941068391, 18832.744759808058, 3768.3758680299475, 9336.204835655484, 8909.499863883462, 8489.199570818215, 8318.320411825951, 7308.383870967865, 7156.81406527754, 6400.50375119667, 6292.785673218372, 5930.175279813457, 5219.435526830749, 5209.439183391683, 5065.696878472977, 4800.79543383199, 4736.542158985414, 4654.700109228875, 4254.664154057074, 4055.0003039649896, 4046.568404255165, 3955.2828977752424, 3932.7257777868845, 3831.1045079721694, 3805.156880976184, 3584.5356769802156, 3538.4290361813587, 3467.1365874078274, 3275.870969806525, 3242.9140962516108, 3145.8488650938475, 3046.9587953254654, 3007.089781498661, 6856.911363616533, 31887.95208466731, 9671.971151220956, 8773.791487164019, 8624.724854419503, 6595.973328754682, 6461.588004628844, 6269.0685115500355, 5920.884707468818, 5497.152390830047, 5381.603826884463, 5177.8291385967605, 4764.370728190505, 4717.20433048268, 4664.69745490723, 4384.580659865394, 4144.099930008216, 4065.9743737069543, 3839.24951418917, 3603.180905141191, 3599.2408821861072, 3378.5885088957775, 3144.352820134328, 3076.2255263648062, 3062.954829877491, 2983.283635519334, 2975.021261966181, 2970.558689270036, 2889.441330778875, 2840.2080071315686, 2787.197844611738, 2662.480513017147, 8155.251011968378, 5750.293148597325, 4677.878458334667, 12988.812652423287, 11715.119968800567, 6955.190272905839, 6479.714372555791, 5566.858027536842, 5354.276937065238, 5216.90064919691, 5192.2215765728315, 4680.537863072161, 4604.597233141996, 4130.3881064452, 3988.591559614966, 3900.9087109467, 3689.7424569838595, 3503.911901837378, 3268.0558675108696, 3094.032879288225, 2978.013574927791, 2912.2695981282395, 2902.335046298426, 2901.130872902171, 2874.1273842738146, 2752.23907084867, 2633.4304018527364, 2622.8086658505285, 2612.38563888692, 2503.229518021276, 2413.3703630897126, 2392.168208604585, 2389.7618005621835, 31887.95208466731, 5337.700353688755, 5915.640047553728, 14553.536332412963, 15942.96156270709, 7378.002990522395, 7126.170904075117, 7038.481904176988, 7003.134308909817, 6714.63674421261, 6335.843976679697, 6255.2554915433375, 6205.385532173038, 5034.95259957038, 4409.344397738357, 4150.326411648914, 3942.5376064919005, 3892.4269499755032, 3727.786020425709, 3700.911989550418, 3627.342117554706, 3547.9792310770595, 3505.7923442899787, 3362.893031508754, 3267.5128374026945, 3265.358888156858, 3201.057561270421, 3175.796583100949, 3144.3254066181034, 3100.710646121895, 2876.2942444087344, 2859.953619211418, 2847.2701133314895, 2768.2810657895284], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5138, -3.9988, -4.1365, -4.2192, -4.229, -4.2429, -4.2551, -4.261, -4.3143, -4.4356, -4.4362, -4.5251, -4.6103, -4.6119, -4.6138, -4.639, -4.6678, -4.6704, -4.6857, -4.7092, -4.7307, -4.7672, -4.7744, -4.8001, -4.8218, -4.8502, -4.8566, -4.8635, -4.8852, -4.9538, -4.6168, -3.7143, -3.8129, -3.9227, -4.0686, -4.0947, -4.1136, -4.2227, -4.2283, -4.2506, -4.2686, -4.2772, -4.3905, -4.4059, -4.4219, -4.4569, -4.4684, -4.5439, -4.5491, -4.6055, -4.6164, -4.6208, -4.6602, -4.6841, -4.7158, -4.7373, -4.8752, -4.9993, -5.0935, -5.1129, -5.1228, -4.7779, -4.3154, -4.479, -4.7084, -3.6297, -3.7841, -3.8964, -3.965, -3.9838, -4.0142, -4.1088, -4.1919, -4.2441, -4.2642, -4.2654, -4.3549, -4.4776, -4.4817, -4.492, -4.4939, -4.5949, -4.61, -4.6384, -4.6703, -4.7883, -4.7902, -4.8853, -4.94, -4.9746, -4.9972, -5.0171, -5.0258, -5.1007, -5.1252, -3.8777, -3.904, -3.5716, -3.6048, -3.6118, -3.7064, -3.7637, -4.3072, -4.4101, -4.4473, -4.5128, -4.6342, -4.695, -4.7058, -4.7114, -4.718, -4.7748, -4.7758, -4.8055, -4.8409, -4.8704, -4.8976, -4.9117, -4.9266, -4.9763, -4.9834, -5.0202, -5.0676, -5.0796, -5.0995, -5.1208, -5.1293, -4.0483, -4.9363, -4.6569, -3.2937, -3.6684, -3.7899, -4.0155, -4.1857, -4.187, -4.1995, -4.3408, -4.3597, -4.3979, -4.4599, -4.5365, -4.5802, -4.6078, -4.6168, -4.6317, -4.7055, -4.7079, -4.7516, -4.7625, -4.8114, -4.8446, -4.8452, -4.8457, -4.8879, -4.932, -4.9356, -4.9406, -4.943, -4.9823, -3.6627, -3.731, -3.9532, -4.1442, -4.2049, -4.3379, -4.4346, -4.5867, -4.608, -4.7357, -4.7374, -4.8123, -4.8142, -4.8346, -4.8371, -4.8884, -4.9491, -4.957, -5.0413, -5.0422, -5.0446, -5.0589, -5.0757, -5.1384, -5.1524, -5.1582, -5.1803, -5.1976, -5.216, -5.231, -5.2356, -3.4627, -3.6582, -4.2518, -4.2937, -4.5967, -4.7851, -4.9087, -4.4721, -5.0601, -3.8482, -3.895, -3.9433, -3.9636, -4.0931, -4.114, -4.2257, -4.2427, -4.3021, -4.4298, -4.4317, -4.4597, -4.5134, -4.5269, -4.5443, -4.6342, -4.6822, -4.6843, -4.7072, -4.7129, -4.7391, -4.7459, -4.8056, -4.8185, -4.8389, -4.8957, -4.9058, -4.9362, -4.9681, -4.9813, -4.6043, -4.4807, -3.7798, -3.8773, -3.8944, -4.1626, -4.1832, -4.2135, -4.2706, -4.3449, -4.3661, -4.4047, -4.488, -4.4979, -4.5091, -4.5711, -4.6275, -4.6465, -4.7039, -4.7674, -4.7685, -4.8318, -4.9036, -4.9255, -4.9299, -4.9562, -4.959, -4.9605, -4.9882, -5.0054, -5.0242, -5.07, -4.2395, -4.5956, -4.8479, -3.4718, -3.5751, -4.0965, -4.1673, -4.3192, -4.3581, -4.3841, -4.3889, -4.4926, -4.509, -4.6177, -4.6526, -4.6749, -4.7305, -4.7822, -4.8519, -4.9067, -4.9449, -4.9672, -4.9706, -4.9711, -4.9804, -5.0238, -5.0679, -5.072, -5.0759, -5.1186, -5.1552, -5.164, -5.165, -3.1673, -4.6951, -4.7226, -4.9275, -3.2505, -4.0211, -4.0558, -4.0682, -4.0733, -4.1153, -4.1734, -4.1862, -4.1942, -4.4033, -4.536, -4.5965, -4.6479, -4.6607, -4.7039, -4.7111, -4.7312, -4.7534, -4.7653, -4.8069, -4.8357, -4.8364, -4.8563, -4.8642, -4.8742, -4.8881, -4.9633, -4.969, -4.9734, -5.0016], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2348, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2347, 2.2346, 0.8565, 2.2588, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2587, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2586, 2.2585, 2.2585, 2.2585, 2.2585, 2.2405, 1.9592, 1.9258, 0.7547, 2.2628, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2627, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2626, 2.2625, 2.2625, 2.2625, 2.2625, 2.2625, 2.2625, 1.5854, 1.0324, 2.2785, 2.2785, 2.2785, 2.2785, 2.2785, 2.2784, 2.2784, 2.2784, 2.2784, 2.2784, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2783, 2.2782, 2.2782, 2.2782, 2.2782, 2.006, 1.9414, 0.9825, 2.2916, 2.2916, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2915, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2914, 2.2913, 2.0582, 2.3008, 2.3008, 2.3008, 2.3008, 2.3008, 2.3008, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3007, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.3006, 2.0106, 1.9812, 2.0332, 1.9596, 1.9399, 1.9717, 2.0457, 0.991, 2.012, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3166, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3165, 2.3164, 2.3164, 2.3164, 2.3164, 2.3164, 2.3164, 2.3164, 1.8691, 0.4558, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3496, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3495, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.3494, 2.0605, 2.0538, 2.0079, 2.3628, 2.3627, 2.3627, 2.3627, 2.3627, 2.3627, 2.3627, 2.3627, 2.3626, 2.3626, 2.3626, 2.3626, 2.3626, 2.3626, 2.3626, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3625, 2.3624, 2.3624, 1.7691, 2.0288, 1.8985, 0.7933, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.3791, 2.379, 2.379, 2.379, 2.379, 2.379, 2.379, 2.379, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789, 2.3789]}, \"token.table\": {\"Topic\": [10, 8, 10, 5, 3, 5, 2, 5, 7, 5, 5, 6, 3, 5, 4, 7, 5, 10, 4, 1, 7, 8, 4, 5, 2, 9, 4, 4, 4, 1, 1, 8, 2, 4, 10, 6, 3, 3, 6, 4, 2, 4, 10, 1, 10, 7, 8, 8, 5, 10, 3, 3, 7, 6, 9, 3, 8, 9, 1, 3, 1, 7, 5, 1, 10, 10, 2, 6, 4, 4, 6, 1, 5, 9, 8, 9, 6, 1, 7, 10, 10, 3, 6, 7, 5, 7, 6, 1, 8, 4, 1, 3, 5, 5, 3, 9, 10, 10, 3, 5, 7, 4, 7, 10, 5, 10, 3, 1, 1, 10, 2, 9, 6, 9, 9, 5, 6, 2, 7, 8, 4, 10, 4, 7, 4, 6, 6, 1, 9, 1, 6, 7, 1, 4, 5, 3, 8, 2, 2, 7, 2, 8, 4, 1, 6, 3, 1, 7, 7, 10, 6, 9, 7, 7, 8, 9, 9, 3, 5, 2, 3, 6, 9, 7, 4, 8, 4, 5, 1, 7, 4, 10, 9, 10, 6, 8, 9, 6, 8, 1, 3, 9, 8, 1, 5, 3, 8, 10, 2, 4, 6, 8, 1, 3, 6, 5, 3, 2, 7, 4, 7, 8, 2, 9, 1, 1, 3, 10, 9, 7, 1, 7, 2, 1, 6, 9, 5, 8, 3, 7, 9, 8, 2, 4, 9, 3, 7, 2, 10, 9, 10, 2, 5, 2, 6, 2, 9, 6, 4, 6, 6, 9, 4, 8, 1, 1, 9, 2, 4, 9, 9, 9, 10, 5, 9, 4, 2, 3, 6, 1, 2, 8, 4, 2, 3, 6, 6, 10, 3, 10, 3, 2, 1, 2, 8, 9, 10, 7, 10, 9, 4, 2, 10, 3, 6, 10, 5, 8, 7, 8, 2, 8, 5, 1, 1, 6, 2, 6, 2, 10, 3, 8, 10, 4, 8, 3, 10, 7, 5, 3, 2, 6, 7, 4, 8, 8, 2, 7, 8, 6, 6, 8, 2, 9, 4, 9, 9, 6, 10, 1, 5, 2, 6, 6, 3, 7, 6, 7, 7, 5, 2, 2, 5, 1, 8, 2], \"Freq\": [0.999680407872212, 0.7488426770724246, 0.25100392336126626, 0.9997995577140801, 0.999851340720271, 0.9997333134186132, 0.9996203952252991, 0.9998091326187232, 0.9996756495531675, 0.9998081238023772, 0.9998960943319064, 0.9997257605724381, 0.9998428640758426, 0.9998398244029802, 0.7614042778693988, 0.23852027195631625, 0.9998575866051327, 0.9999396873220004, 0.9999653475111157, 0.9995918157251618, 0.9998495909054406, 0.9997345792749226, 0.9996863289058459, 0.9997397138102689, 0.9996809044647564, 0.9996596473111055, 0.9997882685978866, 0.9997860501291707, 0.999881576946047, 0.9996595752000534, 0.9999295034240194, 0.9997903658570628, 0.999700667899811, 0.9997404237427757, 0.9997739899537207, 0.99949789544768, 0.9997670330805202, 0.9998527782281207, 0.9998611182770669, 0.9998841134072473, 0.9997586721573299, 0.9998474840765288, 0.9996696212891588, 0.9998019030508901, 0.9995372342045178, 0.9998018136466459, 0.9996882651130936, 0.9998995911789053, 0.9999564553148832, 0.9998636394765099, 0.9997725274146435, 0.9996318786916234, 0.9998862531190394, 0.9997059623936831, 0.9996812232240032, 0.9995532095545641, 0.9999097896086114, 0.9995640519926261, 0.9999167012436792, 0.9995825139227745, 0.9999277872834437, 0.9999156946887712, 0.9996285877199096, 0.9997908293642647, 0.9995539189184958, 0.9998108026736019, 0.9999250772638206, 0.9998378886842285, 0.9996965608345569, 0.2736145457826332, 0.726345280332231, 0.9998436034528491, 0.7917663265344319, 0.20812810926605868, 0.9998195243064512, 0.9999043997156196, 0.9996652242093592, 0.9996581119044362, 0.9996721826847101, 0.9999051707133436, 0.9997992901257136, 0.9999202524084291, 0.9998253003392393, 0.9998505588928419, 0.9997327196832597, 0.9998438997690475, 0.9997898745195397, 0.28944745188656024, 0.7103647582119939, 0.9997011924008187, 0.9998585620745533, 0.9998728218392314, 0.9996547242890302, 0.9998798508065101, 0.37138838440795885, 0.6285034197673149, 0.9995784766375281, 0.999890312655577, 0.9998173000952171, 0.9995789371060282, 0.9999438954047816, 0.999818946391243, 0.9998787495306612, 0.9996665613018995, 0.9998959168349572, 0.9995838472267822, 0.9997669152787378, 0.9998179973920645, 0.9997830172561133, 0.9998640566392175, 0.9818781426469805, 0.018108408368489395, 0.9999380471950771, 0.9998273593350778, 0.9998897524621122, 0.9998271313104349, 0.9996243940856115, 0.3606580089575007, 0.6392090793613933, 0.9996567211202027, 0.9997846417628181, 0.9997535770769493, 0.9999193179355103, 0.999753315933414, 0.9996621928114523, 0.9998784897510778, 0.9997781166667516, 0.999790179325755, 0.9995400095863748, 0.9999082411906766, 0.9995622048644278, 0.9998586944731116, 0.99994243961071, 0.9998210835380953, 0.9998089860634315, 0.9997585758571607, 0.9997446940182562, 0.999897452250987, 0.9997660380679685, 0.9998412646110537, 0.9998846651110527, 0.9998258122010929, 0.999654099080483, 0.2890653986407087, 0.7108434134994593, 0.999930657990486, 0.9997080918097174, 0.9998709501690791, 0.9996375959556093, 0.9997344454609806, 0.9997643633360952, 0.9994321787029994, 0.9999212950704738, 0.9996959702287262, 0.9998504822844413, 0.9999374344333826, 0.9998288655149412, 0.9997620516532372, 0.9996704075704743, 0.9998351683399527, 0.9998857348727249, 0.9998969494570137, 0.9998365622830079, 0.9997301634216232, 0.999959035706432, 0.9998295581635347, 0.9997605338931211, 0.9997147099842179, 0.9998626126276885, 0.9998751469922589, 0.7138107466274339, 0.2860049792884331, 0.7160386958325056, 0.28383009528682523, 0.9996952538099293, 0.9996722603798477, 0.9996101958333786, 0.999697336501421, 0.999712296068315, 0.9998539952529455, 0.9997862157089054, 0.9998458686151869, 0.9996552378052137, 0.9998460840541898, 0.9997495636019507, 0.9996968288174156, 0.7437881668768996, 0.2559869491799851, 0.9999028592617607, 0.9998427962529735, 0.7747640309570815, 0.22506847915135678, 0.9998778271794058, 0.9999214577166365, 0.9997333366515193, 0.9998692604466772, 0.9995009452404255, 0.7411070979595945, 0.2588315642978742, 0.9998587395823922, 0.999885536966163, 0.9998398675246775, 0.9998043993731797, 0.9996916794347, 0.9997512292193209, 0.9998781107470257, 0.9999435106042942, 0.9996951025782775, 0.999511652817564, 0.9998624318648164, 0.9998804155370866, 0.9997181250491133, 0.9998104495611191, 0.2519565103410379, 0.7479841558857637, 0.9995088272919346, 0.9996808016057692, 0.9996016136156785, 0.2921793150987546, 0.15554463914240882, 0.5522775483743875, 0.9998675677538084, 0.9997985228405469, 0.9996315483675202, 0.9996661705519876, 0.9998470826028386, 0.9998154517177414, 0.9998331400717478, 0.9998430496135091, 0.9996077472835814, 0.9998380283941757, 0.9998085042746891, 0.999825494625006, 0.30291913159937517, 0.6970246889673828, 0.9997269412951496, 0.9998516870915147, 0.9997563454782099, 0.9998657053183768, 0.9997532579541715, 0.9995438610364087, 0.9997397474985316, 0.9996919466747088, 0.9999089999813633, 0.9997771971533781, 0.9998487760189102, 0.9998850852000569, 0.9996453409879231, 0.9999507147461048, 0.9997987781010422, 0.999767051470815, 0.9997647294987673, 0.9997767210166307, 0.9995909151139868, 0.9998702963339122, 0.9995220752096051, 0.9999245304795733, 0.9996655104259087, 0.99986975709184, 0.9999133607888128, 0.999935361988623, 0.9997603591126262, 0.9998196906542196, 0.22221933411062983, 0.5078919786781778, 0.2698491412067433, 0.7651474193230238, 0.23471446727891304, 0.9998161804166132, 0.999789145508513, 0.9998243137301766, 0.9997772979804778, 0.2830943592150858, 0.7167911048139546, 0.9998119245137104, 0.9996639283260007, 0.9998356895882967, 0.9998343120753732, 0.9998667933296963, 0.9995497953423472, 0.9997675018249389, 0.9999111846832724, 0.9996299997322527, 0.9997588123633988, 0.9996759768290747, 0.9997240031541103, 0.999867903597964, 0.9995697619791057, 0.9997116998583905, 0.9998877979680617, 0.9997653609844958, 0.9995702333746943, 0.9997801799441777, 0.9998337128258898, 0.9997699527043298, 0.9996990625109506, 0.2507711632528929, 0.7491290940348325, 0.9998638658963624, 0.9997491706159053, 0.9999459226837328, 0.9998505785009287, 0.9999315329379902, 0.999803015795273, 0.9996745420727275, 0.9995989991354246, 0.9995500305953571, 0.9996853271114344, 0.9998020606675653, 0.9998323683177469, 0.9998489414109111, 0.9996610347843302, 0.9997341257288359, 0.9998525442970743, 0.9995746765277278, 0.9995697239430905, 0.9997793452612149, 0.9999165567179611, 0.9998524359171619, 0.999673091914339, 0.9998534517386017, 0.9998472608617542, 0.9998731894181964, 0.999676912649699, 0.9999318822917487, 0.9997615108295206, 0.9994695886907761, 0.9996884975923157, 0.999770811854765, 0.9996974101943503, 0.9999141782959192, 0.9998881276663425, 0.9999584625058505, 0.9998797015434223, 0.9998276437469175, 0.9998595342526355, 0.7195003811578555, 0.2803650570281418, 0.9998106461028461, 0.9998308668860499, 0.9998785431166033, 0.999503381333847, 0.9999320614101124, 0.9999241755976328, 0.9999159562268086, 0.9998916319372007], \"Term\": [\"abus\", \"accus\", \"accus\", \"action\", \"adelaid\", \"ahead\", \"airport\", \"alleg\", \"announc\", \"anti\", \"appeal\", \"approv\", \"arrest\", \"assault\", \"attack\", \"attack\", \"aussi\", \"australia\", \"australian\", \"award\", \"back\", \"bank\", \"battl\", \"beat\", \"begin\", \"black\", \"blaze\", \"boat\", \"bodi\", \"bomb\", \"boost\", \"break\", \"brisban\", \"budget\", \"build\", \"bush\", \"bushfir\", \"busi\", \"call\", \"campaign\", \"canberra\", \"cancer\", \"care\", \"case\", \"cattl\", \"centr\", \"challeng\", \"chang\", \"charg\", \"chief\", \"child\", \"children\", \"china\", \"chines\", \"christma\", \"citi\", \"claim\", \"clash\", \"close\", \"club\", \"coast\", \"communiti\", \"compani\", \"concern\", \"confid\", \"consid\", \"continu\", \"coronavirus\", \"cost\", \"council\", \"council\", \"countri\", \"court\", \"court\", \"covid\", \"crash\", \"critic\", \"cut\", \"darwin\", \"dead\", \"deal\", \"death\", \"debat\", \"decis\", \"defend\", \"delay\", \"demand\", \"deni\", \"deni\", \"despit\", \"develop\", \"die\", \"doctor\", \"drive\", \"driver\", \"driver\", \"drop\", \"drought\", \"drug\", \"east\", \"elect\", \"england\", \"expect\", \"export\", \"face\", \"fail\", \"fall\", \"famili\", \"farm\", \"farmer\", \"fatal\", \"fatal\", \"fear\", \"feder\", \"fight\", \"final\", \"find\", \"flood\", \"flood\", \"food\", \"forc\", \"free\", \"fund\", \"futur\", \"game\", \"get\", \"girl\", \"give\", \"go\", \"gold\", \"good\", \"govern\", \"govt\", \"green\", \"group\", \"guilti\", \"head\", \"health\", \"hear\", \"help\", \"high\", \"hill\", \"hit\", \"home\", \"home\", \"hospit\", \"hour\", \"hous\", \"increas\", \"india\", \"indigen\", \"indonesia\", \"industri\", \"injur\", \"inquiri\", \"interview\", \"investig\", \"iraq\", \"island\", \"issu\", \"jail\", \"job\", \"john\", \"join\", \"kill\", \"labor\", \"land\", \"launch\", \"law\", \"lead\", \"leader\", \"leader\", \"leav\", \"leav\", \"liber\", \"life\", \"light\", \"link\", \"live\", \"local\", \"look\", \"lose\", \"make\", \"market\", \"mayor\", \"media\", \"meet\", \"meet\", \"melbourn\", \"million\", \"mine\", \"mine\", \"minist\", \"miss\", \"move\", \"murder\", \"name\", \"nation\", \"nation\", \"near\", \"need\", \"news\", \"north\", \"number\", \"offer\", \"offic\", \"open\", \"opposit\", \"order\", \"park\", \"parti\", \"peopl\", \"perth\", \"plan\", \"plan\", \"plane\", \"play\", \"plead\", \"polic\", \"polic\", \"polic\", \"port\", \"power\", \"premier\", \"presid\", \"price\", \"prison\", \"probe\", \"project\", \"promis\", \"protest\", \"public\", \"push\", \"queensland\", \"queensland\", \"question\", \"race\", \"rail\", \"rain\", \"rais\", \"ralli\", \"rat\", \"rate\", \"record\", \"region\", \"reject\", \"releas\", \"remain\", \"report\", \"rescu\", \"research\", \"resid\", \"return\", \"reveal\", \"review\", \"right\", \"rise\", \"risk\", \"river\", \"road\", \"rural\", \"sale\", \"save\", \"say\", \"say\", \"say\", \"school\", \"school\", \"search\", \"season\", \"second\", \"secur\", \"seek\", \"seek\", \"senat\", \"sentenc\", \"servic\", \"share\", \"shoot\", \"show\", \"sign\", \"south\", \"spark\", \"speak\", \"sport\", \"stab\", \"stand\", \"star\", \"start\", \"state\", \"station\", \"stay\", \"stop\", \"storm\", \"strike\", \"student\", \"studi\", \"studi\", \"support\", \"suspect\", \"sydney\", \"take\", \"talk\", \"target\", \"tasmania\", \"tasmanian\", \"teacher\", \"team\", \"teen\", \"tell\", \"test\", \"threat\", \"threaten\", \"time\", \"titl\", \"tourism\", \"trade\", \"train\", \"trial\", \"truck\", \"trump\", \"turn\", \"union\", \"unit\", \"urg\", \"victim\", \"victori\", \"victoria\", \"violenc\", \"vote\", \"want\", \"warn\", \"water\", \"weather\", \"week\", \"welcom\", \"west\", \"west\", \"win\", \"woman\", \"women\", \"wont\", \"work\", \"worker\", \"world\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 2, 7, 9, 4, 1, 5, 6, 3, 8]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2156423697090985925207548594\", ldavis_el2156423697090985925207548594_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2156423697090985925207548594\", ldavis_el2156423697090985925207548594_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2156423697090985925207548594\", ldavis_el2156423697090985925207548594_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=               x          y  topics  cluster       Freq\n",
       "topic                                                  \n",
       "9      32.420952   8.115230       1        1  10.700251\n",
       "1       0.648955  71.707100       2        1  10.447233\n",
       "6      71.019684   9.971006       3        1  10.405568\n",
       "8       3.785981  33.185135       4        1  10.243107\n",
       "3     -42.666500  -9.222070       5        1  10.110137\n",
       "0      -3.621689 -43.526516       6        1  10.016527\n",
       "4      -3.632402  -4.171708       7        1   9.859816\n",
       "5     -36.584400  35.804413       8        1   9.539459\n",
       "2      44.391380  50.615383       9        1   9.415396\n",
       "7      40.235809 -31.591665      10        1   9.262505, topic_info=            Term          Freq         Total Category  logprob  loglift\n",
       "9407       polic  31887.000000  31887.000000  Default  30.0000  30.0000\n",
       "2182       charg  16666.000000  16666.000000  Default  29.0000  29.0000\n",
       "711    australia  15942.000000  15942.000000  Default  28.0000  28.0000\n",
       "9321        plan  18642.000000  18642.000000  Default  27.0000  27.0000\n",
       "5089        govt  14154.000000  14154.000000  Default  26.0000  26.0000\n",
       "...          ...           ...           ...      ...      ...      ...\n",
       "13165    violenc   3099.814724   3100.710646  Topic10  -4.8881   2.3789\n",
       "12200    teacher   2875.398340   2876.294244  Topic10  -4.9633   2.3789\n",
       "4176      export   2859.057710   2859.953619  Topic10  -4.9690   2.3789\n",
       "2636      confid   2846.374211   2847.270113  Topic10  -4.9734   2.3789\n",
       "2071       cattl   2767.385157   2768.281066  Topic10  -5.0016   2.3789\n",
       "\n",
       "[359 rows x 6 columns], token_table=       Topic      Freq     Term\n",
       "term                           \n",
       "43        10  0.999680     abus\n",
       "69         8  0.748843    accus\n",
       "69        10  0.251004    accus\n",
       "89         5  0.999800   action\n",
       "109        3  0.999851  adelaid\n",
       "...      ...       ...      ...\n",
       "13604      2  0.999503     wont\n",
       "13634      5  0.999932     work\n",
       "13637      1  0.999924   worker\n",
       "13644      8  0.999916    world\n",
       "13730      2  0.999892     year\n",
       "\n",
       "[346 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[10, 2, 7, 9, 4, 1, 5, 6, 3, 8])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, info_train_vec, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83be02",
   "metadata": {},
   "source": [
    "Finalmente corresponder√≠a testear nuestro modelo, para ello utilizaremos la informaci√≥n ya prepocesada guardada en info_test. Aplicamos los mismos pasos que antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "545478fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dejamos todo como str:\n",
    "info_test=info_test.apply(str)\n",
    "\n",
    "#Aplicamos el CV\n",
    "#la clave ac√° est√° en aplicar el .transform() ya que contamos con nuestro vocabulario guardado en CountVectorizer()\n",
    "#y queremos crear la matrix-tokens de la informaci√≥n externa asociada a ese vocabulario.\n",
    "info_test_vec = vectorizer.transform(info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb14a44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299999</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.014287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502660</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.197340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245247</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245248</th>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245249</th>\n",
       "      <td>0.366666</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245250</th>\n",
       "      <td>0.014292</td>\n",
       "      <td>0.299994</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245251</th>\n",
       "      <td>0.219997</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245252 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.299999  0.014286  0.157143  0.014286  0.014286  0.014286  0.157143   \n",
       "1       0.502660  0.016667  0.016667  0.016667  0.183333  0.016667  0.016667   \n",
       "2       0.016667  0.016667  0.016667  0.016667  0.350000  0.016667  0.350000   \n",
       "3       0.350000  0.016667  0.016667  0.016667  0.016667  0.016667  0.350000   \n",
       "4       0.025000  0.025000  0.275000  0.275000  0.025000  0.025000  0.025000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "245247  0.016667  0.016667  0.183333  0.350000  0.016667  0.016667  0.016667   \n",
       "245248  0.157143  0.157143  0.014286  0.014286  0.300000  0.014286  0.014286   \n",
       "245249  0.366666  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n",
       "245250  0.014292  0.299994  0.014286  0.014286  0.014286  0.300000  0.014286   \n",
       "245251  0.219997  0.220000  0.020000  0.020000  0.020000  0.220000  0.020000   \n",
       "\n",
       "               7         8         9  \n",
       "0       0.014286  0.300000  0.014287  \n",
       "1       0.016667  0.016667  0.197340  \n",
       "2       0.016667  0.016667  0.183333  \n",
       "3       0.183333  0.016667  0.016667  \n",
       "4       0.025000  0.025000  0.275000  \n",
       "...          ...       ...       ...  \n",
       "245247  0.016667  0.016667  0.350000  \n",
       "245248  0.157143  0.157143  0.014286  \n",
       "245249  0.366667  0.033333  0.033333  \n",
       "245250  0.157143  0.014286  0.157143  \n",
       "245251  0.020000  0.220000  0.020003  \n",
       "\n",
       "[245252 rows x 10 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se lo pasamos a nuestro LDA\n",
    "doc_topic_test = lda.transform(info_test_vec)\n",
    "# making a dataframe from the document-topic matrix for test info\n",
    "doc_topic_df_test = pd.DataFrame(data=doc_topic_test)\n",
    "doc_topic_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b44dab",
   "metadata": {},
   "source": [
    "Debido a que train_test_split() nos hace la partici√≥n de manera aleatoria, corresponde hacer coincidir los √≠ndices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5cb02e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos in dataset</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>519246</td>\n",
       "      <td>['billion', 'dollar', 'hole', 'liber', 'hospit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>534672</td>\n",
       "      <td>['plan', 'bauxit', 'bring', 'river', 'worri']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>755151</td>\n",
       "      <td>['hong', 'kong', 'social', 'worker', 'shan', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>831855</td>\n",
       "      <td>['teenag', 'girl', 'shoot', 'western', 'sydney']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581991</td>\n",
       "      <td>['unit', 'overpow', 'arsenal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245247</th>\n",
       "      <td>453023</td>\n",
       "      <td>['east', 'timor', 'prepar', 'possibl', 'cyclon']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245248</th>\n",
       "      <td>748199</td>\n",
       "      <td>['respons', 'cattl', 'diseas', 'win', 'industr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245249</th>\n",
       "      <td>663514</td>\n",
       "      <td>['observatori', 'gingin']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245250</th>\n",
       "      <td>984877</td>\n",
       "      <td>['giant', 'hors', 'defi', 'drought', 'north', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245251</th>\n",
       "      <td>933593</td>\n",
       "      <td>['senat', 'leyonhjelm', 'secur', 'plan']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245252 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pos in dataset                                      headline_text\n",
       "0               519246  ['billion', 'dollar', 'hole', 'liber', 'hospit...\n",
       "1               534672      ['plan', 'bauxit', 'bring', 'river', 'worri']\n",
       "2               755151  ['hong', 'kong', 'social', 'worker', 'shan', '...\n",
       "3               831855   ['teenag', 'girl', 'shoot', 'western', 'sydney']\n",
       "4               581991                     ['unit', 'overpow', 'arsenal']\n",
       "...                ...                                                ...\n",
       "245247          453023   ['east', 'timor', 'prepar', 'possibl', 'cyclon']\n",
       "245248          748199  ['respons', 'cattl', 'diseas', 'win', 'industr...\n",
       "245249          663514                          ['observatori', 'gingin']\n",
       "245250          984877  ['giant', 'hors', 'defi', 'drought', 'north', ...\n",
       "245251          933593           ['senat', 'leyonhjelm', 'secur', 'plan']\n",
       "\n",
       "[245252 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_test_a=pd.DataFrame(info_test)\n",
    "info_test_a.index.name='Pos in dataset'\n",
    "info_test_a.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168a396",
   "metadata": {},
   "source": [
    "Y ahora podemos ver c√≥mo, por ejemplo, para la noticia n√∫mero 534672:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c3da1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'planned bauxite mine brings river worries'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[documents['index'] == 534672].values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f4e3e",
   "metadata": {},
   "source": [
    "nos dice que su t√≥pico principal es el t√≥pico 0 con un score de 0.502660:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c30e20f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word 0       plan\n",
       "Word 1    council\n",
       "Word 2      water\n",
       "Word 3       call\n",
       "Word 4       fear\n",
       "Name: Topic 0, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_keywords.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d9a4a",
   "metadata": {},
   "source": [
    "Tiene sentido. Ya para terminar agregamos a nuestro dataset original la contribuci√≥n de cada uno de sus t√≥picos arrojados por nuestro modelo LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b5be7737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['decid', 'communiti', 'broadcast', 'licenc']</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['wit', 'awar', 'defam']</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['call', 'infrastructur', 'protect', 'summit']</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['staff', 'aust', 'strike', 'rise']</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['strike', 'affect', 'australian', 'travel']</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226253</th>\n",
       "      <td>['reader', 'learn', 'look', 'year']</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226254</th>\n",
       "      <td>['south', 'african', 'variant', 'covid']</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226255</th>\n",
       "      <td>['victoria', 'coronavirus', 'restrict', 'mean'...</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226256</th>\n",
       "      <td>['what', 'life', 'like', 'american', 'doctor',...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.157143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226257</th>\n",
       "      <td>['women', 'shed', 'canberra', 'reskil', 'unemp...</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226258 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline_text         0  \\\n",
       "0            ['decid', 'communiti', 'broadcast', 'licenc']  0.020000   \n",
       "1                                 ['wit', 'awar', 'defam']  0.025000   \n",
       "2           ['call', 'infrastructur', 'protect', 'summit']  0.220000   \n",
       "3                      ['staff', 'aust', 'strike', 'rise']  0.020000   \n",
       "4             ['strike', 'affect', 'australian', 'travel']  0.220000   \n",
       "...                                                    ...       ...   \n",
       "1226253                ['reader', 'learn', 'look', 'year']  0.020000   \n",
       "1226254           ['south', 'african', 'variant', 'covid']  0.025000   \n",
       "1226255  ['victoria', 'coronavirus', 'restrict', 'mean'...  0.516667   \n",
       "1226256  ['what', 'life', 'like', 'american', 'doctor',...  0.014286   \n",
       "1226257  ['women', 'shed', 'canberra', 'reskil', 'unemp...  0.016667   \n",
       "\n",
       "                1         2         3         4         5         6         7  \\\n",
       "0        0.020000  0.020000  0.220000  0.220000  0.220000  0.020000  0.020000   \n",
       "1        0.025000  0.275000  0.275000  0.275000  0.025000  0.025000  0.025000   \n",
       "2        0.020000  0.020000  0.220000  0.020000  0.020000  0.220000  0.020000   \n",
       "3        0.220000  0.020000  0.220000  0.020000  0.020000  0.220000  0.020000   \n",
       "4        0.020000  0.020000  0.020000  0.020000  0.020000  0.020000  0.020000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1226253  0.220000  0.020000  0.020000  0.020000  0.020000  0.220000  0.020000   \n",
       "1226254  0.525000  0.025000  0.025000  0.025000  0.275000  0.025000  0.025000   \n",
       "1226255  0.183333  0.016667  0.016667  0.016667  0.183333  0.016667  0.016667   \n",
       "1226256  0.157143  0.014286  0.157143  0.014286  0.300000  0.157143  0.014286   \n",
       "1226257  0.350000  0.183333  0.016667  0.016667  0.183333  0.016667  0.016667   \n",
       "\n",
       "                8         9  \n",
       "0        0.220000  0.020000  \n",
       "1        0.025000  0.025000  \n",
       "2        0.220000  0.020000  \n",
       "3        0.020000  0.220000  \n",
       "4        0.220000  0.420000  \n",
       "...           ...       ...  \n",
       "1226253  0.420000  0.020000  \n",
       "1226254  0.025000  0.025000  \n",
       "1226255  0.016667  0.016667  \n",
       "1226256  0.014286  0.157143  \n",
       "1226257  0.016667  0.183333  \n",
       "\n",
       "[1226258 rows x 11 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dejamos todo como str:\n",
    "processed_docs=processed_docs.apply(str)\n",
    "\n",
    "#Aplicamos el CV\n",
    "processed_docs_vec = vectorizer.transform(processed_docs)\n",
    "\n",
    "#se lo pasamos a nuestro LDA\n",
    "doc_topic_dataset = lda.transform(processed_docs_vec)\n",
    "\n",
    "# making a dataframe from the document-topic matrix for test info\n",
    "doc_topic_df_dataset = pd.DataFrame(data=doc_topic_dataset)\n",
    "\n",
    "#concat\n",
    "final_data=pd.concat([processed_docs, doc_topic_df_dataset], axis=1,)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c0a99",
   "metadata": {},
   "source": [
    "Como comentario, el n√∫mero √≥ptimo de t√≥picos no tiene una forma expl√≠cita de calcularse, por lo que se recomienda probar con distintos k hasta obtener los mejores resultados. El siguiente esquema muestra potecialmente la construcci√≥n del modelo LDA m√°s √≥ptimo:\n",
    "\n",
    "![Figura 3: esquema soluci√≥n LDA (Hammoe,L.,2018)](gr√°ficoLDA.png) \n",
    "\n",
    "#### ¬øPor qu√© funciona el m√©todo? ####\n",
    "\n",
    "Para entender un poco mejor el fundamento de c√≥mo esta t√©cnica logra agrupar grupos de palabras en un t√≥pico contamos con \"El principio de la caja de Dirichlet\", √©ste establece que si contamos con $n$ elementos para distribuir en $m$ lugares distintos (con $n > m $) existir√° al menos un lugar de $m$ con m√°s de un elemento, con esto podemos entender intuitivamente porqu√© si tenemos un texto con cinco palabras y trabajamos con cuatro t√≥picos, el algoritmo devolver√° al menos un grupo de palabras con dos palabras pertenecientes de un mismo tipo.\n",
    "\n",
    "Adem√°s, un hecho muy simple pero no por eso menos importante que revela el LDA es que las palabras agrupadas en un mismo t√≥pico *coocurren* y no as√≠ las palabras pertencientes a t√≥picos distintos.\n",
    "\n",
    "Una de las mayores ventajas de este modelo es que asume intuitivamente que una palabra pertenece a un t√≥pico y que cada documento pertenece al menos a un t√≥pico (como lo comentamos al inicio del texto). Durante √©ste proceso intuitivo lo que algoritmo va a decidir es a d√≥nde ir√° a parar cada palabra teniendo en cuenta lo siguiente (Chand√≠a, B., 2016):\n",
    "- Una palabra pertenece a un t√≥pico, por lo que, en estricto rigor, si en el corpus se tiene un diccionario de mil palabras; potencialmente, existir√°n mil t√≥picos\n",
    "- Un documento pertenece a un t√≥pico, por lo que, en estricto rigor, las palabras de x documento, pertenecen al t√≥pico de dicho documento. \n",
    "\n",
    "Por ejemplo, de acuerdo a estas reglas, si contamos con un corpus de 5 documentos con un vocabulario total de 500 palabras estaremos trabajando potencialmente con un corpus con 500 t√≥picos y a su vez se tiene que en el corpus hay 5 t√≥picos (equivalente a la cantidad de documentos) y en estricto rigor cada documento habla de un t√≥pico.\n",
    "\n",
    "Pero esta intuici√≥n estricta intuitivamente (valga la redundancia) no siempre ser√° cierta (basta con mirar el ejemplo anterior)\n",
    "y es por esto que el LDA contempla la posibilidad de que un documento pueda estar hablando de m√°s de un tema a trav√©s de la proporci√≥n $\\theta_{d}$ fijada con el par√°metro Dirichlet $\\alpha$. De esta manera, este factor va a establecer en c√≥mo va a influir la proporci√≥n de t√≥picos $\\theta_{d}$ de cada documento en la asignaci√≥n de t√≥picos de cada palabra (Chand√≠a B., 2016).\n",
    "\n",
    "#### Biliograf√≠a ####\n",
    "- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n",
    "\n",
    "- Chand√≠a Sep√∫lveda, B. (2016). Aplicaci√≥n y evaluacion LDA para asignaci√≤n de t√≥picos en datos de Twitter.\n",
    "\n",
    "- Hammoe, L. (2018). Detecci√≥n de t√≥picos: utilizando el modelo LDA.\n",
    "\n",
    "- Nicolai Manaut, F. I. (2019). Sistema de an√°lisis de t√≥picos para interacciones cliente-call center.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
